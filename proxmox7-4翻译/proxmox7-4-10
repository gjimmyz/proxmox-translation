proxmox7-4-10

第10章
QEMU/KVM 虚拟机

QEMU（Quick Emulator 的简称）是一个开源的虚拟机管理程序，可以模拟物理计算机。从运行 QEMU 的宿主系统的角度来看，QEMU 
是一个用户程序，可以访问诸如分区、文件、网络卡等一系列本地资源，然后将这些资源传递给模拟的计算机，使其看起来像是真实的设备。
运行在模拟计算机中的客户操作系统访问这些设备，并像在真实硬件上运行一样运行。
例如，您可以将 ISO 镜像作为参数传递给 QEMU，而在模拟计算机中运行的操作系统将看到一个真正的 CD-ROM 插入到 CD 驱动器中。
QEMU 可以模拟从 ARM 到 Sparc 的各种硬件，但 Proxmox VE 只关心 32 位和 64 位的 PC 
克隆仿真，因为它代表了绝大多数服务器硬件。由于处理器扩展的可用性，使得当模拟架构与主机架构相同时，QEMU 可以大大加速 PC 克隆的仿真。
注意
您有时可能会遇到术语 KVM（基于内核的虚拟机）。这意味着 QEMU 正在运行在虚拟化处理器扩展的支持下，通过 Linux KVM 模块。
在 Proxmox VE 的上下文中，QEMU 和 KVM 可以互换使用，因为 Proxmox VE 中的 QEMU 总是会尝试加载 KVM 模块。
Proxmox VE 内部的 QEMU 以 root 进程运行，因为这是访问块和 PCI 设备所必需的。

10.1 模拟设备和半虚拟化设备
QEMU 模拟的 PC 硬件包括主板、网络控制器、SCSI、IDE 和 SATA 控制器、串行端口（完整列表可以在 kvm(1) 手册页中查看），所有这些都是通过软件模拟的。
所有这些设备都是现有硬件设备的精确软件等价物，如果客户机中运行的操作系统具有适当的驱动程序，它将像在真实硬件上运行一样使用这些设备。
这使得 QEMU 可以运行未经修改的操作系统。然而，这会带来性能成本，因为在软件中运行本来在硬件中运行的东西会为宿主 CPU 带来很多额外工作。
为了缓解这种情况，QEMU 可以向客户操作系统提供半虚拟化设备，其中客户操作系统意识到它正在 QEMU 内运行并与虚拟机管理程序合作。
QEMU 依赖于 virtio 虚拟化标准，因此可以提供半虚拟化的 virtio 设备，包括半虚拟化的通用磁盘控制器、半虚拟化的网络卡、半虚拟化的串行端口、半虚拟化的 SCSI 控制器等。
提示
强烈建议您在可以使用 virtio 设备的情况下使用它们，因为它们可以大大提高性能并且通常维护得更好。使用 virtio 通用磁盘控制器与模拟 IDE 
控制器相比，可以将顺序写入吞吐量提高一倍，如通过 bonnie++(8) 测量。使用 virtio 网络接口可以提供高达三倍于模拟的 Intel E1000 网络卡的吞吐量，
如通过 iperf(1) 测量。aa请参阅 KVM wiki 上的此基准测试 https://www.linux-kvm.org/page/Using_VirtIO_NIC

10.2 虚拟机设置
一般来说，Proxmox VE 会为虚拟机（VM）选择合理的默认设置。确保您了解所更改设置的含义，因为它可能导致性能降低或使您的数据处于风险之中。

10.2.1 通用设置
虚拟机的通用设置包括：
• 节点：运行 VM 的物理服务器
• VM ID：用于标识您的 VM 的此 Proxmox VE 安装中的唯一编号
• 名称：一个自由形式的文本字符串，您可以用来描述 VM
• 资源池：VM 的逻辑分组

10.2.2 操作系统设置
在创建虚拟机（VM）时，设置适当的操作系统（OS）允许 Proxmox VE 优化一些底层参数。
例如，Windows 操作系统希望 BIOS 时钟使用本地时间，而基于 Unix 的操作系统希望 BIOS 时钟具有 UTC 时间。

10.2.3 系统设置
在创建 VM 时，您可以更改新 VM 的一些基本系统组件。您可以指定要使用的显示类型（第 10.2.8 节）。
此外，还可以更改 SCSI 控制器（第 10.2.4 节）。
如果您打算安装 QEMU Guest Agent，或者您选择的 ISO 映像已经包含并自动安装它，您可能希望勾选 QEMU Agent 框，让 Proxmox VE 
知道它可以使用其功能来显示更多信息，并更智能地完成某些操作（例如，关闭或快照）。
Proxmox VE 允许使用不同的固件和机器类型引导 VM，即 SeaBIOS 和 OVMF（第 10.2.10 节）。
在大多数情况下，只有当您计划使用 PCIe 透传（第 10.9 节）时，才需要从默认的 SeaBIOS 切换到 OVMF。VM 的机器类型定义了 VM 虚拟主板的硬件布局。
您可以在默认的 Intel 440FX 和 Q35 芯片组之间进行选择，Q35 芯片组还提供了一个虚拟 PCIe 总线，因此，如果要通过 PCIe 硬件，可能是个理想的选择。

10.2.4 硬盘
总线/控制器
QEMU 可以模拟许多存储控制器：
提示
出于性能原因和更好的维护，强烈建议使用 VirtIO SCSI 或 VirtIO Block 控制器。
• IDE 控制器，其设计可以追溯到 1984 年的 PC/AT 磁盘控制器。尽管此控制器已被最新设计所取代，但您可以想象的每一个操作系统都支持它，使其成为运行 2003 
年之前发布的操作系统的绝佳选择。您可以在此控制器上连接多达 4 个设备。
• SATA（Serial ATA）控制器，始于 2003 年，具有更现代的设计，允许更高的吞吐量以及连接更多设备。您可以在此控制器上连接多达 6 个设备。
• SCSI 控制器，设计于 1985 年，通常在服务器级硬件上找到，可以连接多达 14 个存储设备。Proxmox VE 默认模拟 LSI 53C895A 控制器。
如果追求性能，建议使用 VirtIO SCSI single 类型的 SCSI 控制器并为附加磁盘启用 IO Thread（第 10.2.4 节）设置。
这是自 Proxmox VE 7.3 以来新创建的 Linux VM 的默认设置。每个磁盘将拥有自己的 VirtIO SCSI 控制器，QEMU 将在专用线程中处理磁盘 IO。
Linux 发行版自 2012 年以来支持此控制器，FreeBSD 自 2014 年以来支持。对于 
Windows 操作系统，您需要在安装过程中提供一个包含驱动程序的额外 ISO。
• VirtIO Block 控制器，通常称为 VirtIO 或 virtio-blk，是一种较旧类型的半虚拟化控制器。在功能上，它已被 VirtIO SCSI 控制器所取代。
映像格式
在每个控制器上，您可以附加一些模拟硬盘，这些硬盘由位于配置存储中的文件或块设备支持。存储类型的选择将决定硬盘映像的格式。
呈现块设备的存储（LVM、ZFS、Ceph）将需要原始磁盘映像格式，而基于文件的存储（Ext4、NFS、CIFS、GlusterFS）将允许您选择原始磁盘映像格式或 QEMU 映像格式。
• QEMU 映像格式是一种写时复制格式，允许对磁盘映像进行快照和薄配置。
• 原始磁盘映像是硬盘的位对位映像，类似于在 Linux 中对块设备执行 dd 命令时所得到的内容。
此格式本身不支持薄配置或快照，需要存储层进行这些任务的协作。但是，它可能比 QEMU 映像格式快 10%。1
• VMware 映像格式仅在您打算将磁盘映像导入/导出到其他虚拟机管理程序时才有意义。
缓存模式
设置硬盘的缓存模式将影响主机系统如何通知客户机系统块写入完成。无缓存默认表示在每个块到达物理存储写入队列时，将通知客户机系统写入已完成，忽略主机页面缓存。
这在安全性和速度之间提供了良好的平衡。
如果您希望 Proxmox VE 备份管理器在对 VM 进行备份时跳过某个磁盘，可以在该磁盘上设置“无备份”选项。
如果您希望 Proxmox VE 存储复制机制在启动复制作业时跳过一个磁盘，您可以在该磁盘上设置“跳过复制”选项。
从 Proxmox VE 5.0 开始，复制要求将磁盘映像放在 zfspool 类型的存储上，因此当 VM 配置了复制时，将磁盘映像添加到其他存储需要为此磁盘映像跳过复制。
Trim/Discard
如果您的存储支持薄配置（请参阅 Proxmox VE 指南中的存储章节），您可以在驱动器上激活 Discard 选项。
设置了 Discard 并且启用了 TRIM 的客户机操作系统2时，当 VM 
的文件系统在删除文件后将块标记为未使用时，控制器会将此信息传递给存储，存储会相应地缩小磁盘映像。要使客户机能够发出 TRIM 命令，必须在驱动器上启用 Discard 
选项。某些客户机操作系统可能还需要设置 SSD 仿真标志。请注意，仅在使用 Linux 内核 5.0 或更高版本的客户机上支持 VirtIO Block 驱动器上的 Discard。
如果您希望将驱动器显示为固态驱动器而不是旋转硬盘，则可以在该驱动器上设置 SSD 仿真选项。无需底层存储实际由 SSD 支持；此功能可与任何类型的物理介质一起使用。
请注意，SSD 仿真不支持 VirtIO Block 驱动器。
IO 线程
IO 线程选项仅在使用 VirtIO 控制器的磁盘或在模拟控制器类型为 VirtIO SCSI single 的 SCSI 控制器时可用。
启用 IO 线程后，QEMU 为每个存储控制器创建一个 I/O 线程，而不是在主事件循环或 vCPU 线程中处理所有 I/O。一个好处是更好地分配工作和利用底层存储。
另一个好处是对于非常 I/O 密集型的主机工作负载，降低了客户端的延迟（挂起），因为既不是主线程也不是 vCPU 线程会被磁盘 I/O 阻塞。
1有关详细信息，请参阅此基准测试 https://events.static.linuxfound.org/sites/events/files/slides/-
CloudOpen2013_Khoa_Huynh_v3.pdf
2TRIM、UNMAP 和丢弃 https://en.wikipedia.org/wiki/Trim_%28computing%29

10.2.5 CPU
CPU 插槽是PC主板上的一个物理插槽，可以插入CPU。然后，这个CPU可以包含一个或多个内核，它们是独立的处理单元。
从性能角度来看，拥有一个单一的CPU插槽，具有4个核心，或者两个CPU插槽，每个插槽具有两个核心，这两者之间的差别并不大。
然而，某些软件许可证取决于一台计算机拥有的插槽数量， 在这种情况下，将插槽数量设置为许可证允许的数量是有意义的。
增加虚拟 CPU（核心和插槽）的数量通常会提高性能，但这在很大程度上取决于虚拟机的使用情况。
多线程应用程序将从大量虚拟 CPU 中受益，因为对于您添加的每个虚拟 CPU，QEMU 都会在主机系统上创建一个新的执行线程。
如果您不确定虚拟机的工作负载，通常将总核心数量设置为 2 是一个安全的选择。
请注意
如果您所有虚拟机的总核心数量大于服务器上的核心数量（例如，一台只有 8 个核心的计算机上有 4 个虚拟机，每个虚拟机有 4 个核心（=总计 16 
个核心）），这是完全安全的。在这种情况下，主机系统将在您的服务器核心之间平衡 QEMU 执行线程，就像您运行标准多线程应用程序一样。
然而，Proxmox VE 将阻止您启动具有多于物理可用的虚拟 CPU 核心的虚拟机，因为这只会由于上下文切换的成本而降低性能。
资源限制
除了虚拟核心数量外，您还可以配置 VM 可以获得的资源，以便与主机 CPU 时间以及与其他 VM 的关系。使用 cpulimit（“主机 CPU 时间”）选项，您可以限制整个 VM 
可以在主机上使用多少 CPU 时间。这是一个表示 CPU 时间百分比的浮点值，因此 1.0 等于 100%，2.5 等于 250% 以此类推。
如果一个单独的进程完全使用一个单独的核心，它将占用 100% 的 CPU 时间。如果一个具有四个核心的虚拟机充分利用其所有核心，理论上它将使用 400%。
实际上，由于 QEMU 可以为 VM 外围设备（除了 vCPU 核心线程）创建额外线程，因此使用量可能会更高。
如果 VM 应该具有多个 vCPUs，因为它并行运行了几个进程，但整个 VM 不应该能够同时运行所有 vCPUs 100%，则此设置可能很有用。
以一个特定的示例：假设我们有一个 VM，它可以从拥有 8 个 vCPUs 中受益，但是在任何时候，这 8 个核心都不应该满负荷运行 - 
因为这会使服务器过载，以至于其他 VM 和 CT 获得更少的 CPU。因此，我们将 cpulimit 限制设置为 4.0（= 400%）。
如果所有核心都执行相同的繁重工作，它们都将获得 50% 的真实主机核心 CPU 时间。但是，如果只有 4 个核心工作，它们仍然可以获得接近每个核心 100% 的实际核心。
请注意
根据其配置，VM 可以使用额外的线程，例如用于网络或 IO 操作，还有实时迁移。因此，VM 可以显示使用的 CPU 时间超过其虚拟 CPU 可以使用的时间。
要确保 VM 从不使用比分配的虚拟 CPU 更多的 CPU 时间，请将 cpulimit 设置为与总核心计数相同的值。
第二个 CPU 资源限制设置是 cpuunits（现在通常称为 CPU 共享或 CPU 权重），它控制 VM 与其他正在运行的 VM 相比获得多少 CPU 时间。
默认值为 100（如果主机使用旧的 cgroup v1，则为 1024）。如果为 VM 增加此值，调度程序将根据其他权重较低的 VM 对其进行优先级排序。
例如，如果 VM 100 设置了默认值 100，VM 200 更改为 200，后者 VM 200 将获得比第一个 VM 100 多两倍的 CPU 带宽。
有关更多信息，请参阅 man systemd.resource-control，其中 CPUQuota 对应于 cpulimit，CPUWeight 对应于我们的 cpuunits 
设置，请访问其“注意事项”部分以获取参考和实现细节。第三个 CPU 资源限制设置，affinity，控制虚拟机将被允许在哪些主机核心上执行。
例如，如果提供了亲和值 0-3,8-11，虚拟机将受限于使用主机核心 0,1,2,3,8,9,10 和 11。
有效的亲和值以 cpuset 列表格式编写。列表格式是一个以 ASCII 十进制表示的 CPU 数字和数字范围的逗号分隔列表。
请注意
CPU 亲和性使用 taskset 命令将虚拟机限制在给定的一组内核上。此限制对于可能为 IO 创建的某些类型的进程不会生效。CPU 亲和性不是一项安全功能。
有关亲和性的更多信息，请参阅 man cpuset。在这里，列表格式对应于有效的亲和值。访问其“格式”部分以获取更多示例。
CPU 类型
QEMU 可以模拟从 486 到最新 Xeon 处理器的不同 CPU 类型。每个新处理器代都会添加新功能，如硬件辅助的 3d 渲染，随机数生成，内存保护等。
通常，您应该为 VM 选择一个与主机系统 CPU 类型非常接近的处理器类型，因为这意味着主机 CPU 功能（也称为 CPU 标志）将在您的 VM 中可用。
如果您想要完全匹配，可以将 CPU 类型设置为主机，这样 VM 将具有与主机系统完全相同的 CPU 标志。
虽然如此，这也存在一个缺点。如果您想在不同的主机之间进行 VM 的实时迁移，您的 VM 可能会在具有不同 CPU 类型的新系统上运行。
如果传递给客户机的 CPU 标志丢失，qemu 进程将停止。为了解决这个问题，QEMU 还有自己的 CPU 类型 kvm64，Proxmox VE 默认使用它。
kvm64 是一个类似 Pentium 4 的 CPU 类型，具有较少的 CPU 标志集，但可以确保在任何地方都能正常工作。
简而言之，如果您关心实时迁移和在节点之间移动 VM，请保留 kvm64 默认值。
如果您不关心实时迁移或者拥有一个同构集群，其中所有节点都具有相同的 CPU，请将 CPU 类型设置为 host，因为理论上这将为您的客户机提供最大性能。
自定义 CPU 类型
您可以指定具有可配置功能集的自定义 CPU 类型。这些由管理员在配置文件 /etc/pve/virtual-guest/cpu-models.conf 中维护。
有关格式详细信息，请参阅 man cpu-models.conf。
指定的自定义类型可以由在 /nodes 上具有 Sys.Audit 权限的任何用户选择。通过 CLI 或 API 为 VM 配置自定义 CPU 类型时，名称需要以 custom- 为前缀。
与熔断/幽灵相关的 CPU 标志
有几个与熔断和幽灵漏洞相关的 CPU 标志，除非您选择的 VM 的 CPU 类型默认启用它们，否则需要手动设置。
要使用这些 CPU 标志，需要满足以下两个要求：
    主机 CPU 必须支持该功能并将其传播到客户机的虚拟 CPU
    客户操作系统必须更新到能够缓解攻击并能够利用 CPU 功能的版本
否则，您需要设置虚拟 CPU 的所需 CPU 标志，可以通过编辑 WebUI 中的 CPU 选项，或者设置 VM 配置文件中的 cpu 选项的 flags 属性来实现。
对于 Spectre v1, v2, v4 修复，您的 CPU 或系统供应商还需要为您的 CPU 提供所谓的“微码更新”4。
要检查 Proxmox VE 主机是否存在漏洞，请以 root 用户身份执行以下命令：
for f in /sys/devices/system/cpu/vulnerabilities/; do echo "${f##/} -" $( -
cat "$f"); done
社区脚本也可用于检测主机是否仍然存在漏洞。5
英特尔处理器
    pcid
    这减小了熔断（CVE-2017-5754）缓解措施内核页表隔离（KPTI）对性能的影响，有效地将内核内存从用户空间隐藏起来。没有 PCID，KPTI 是一种相当昂贵的机制6。
    要检查 Proxmox VE 主机是否支持 PCID，请以 root 用户身份执行以下命令：
    3Meltdown Attack https://meltdownattack.com/
    4如果您的供应商不提供此类更新，可以使用 Debian non-free 的 'intel-microcode' / 'amd-microcode'。
    请注意，并非所有受影响的 CPU 都可以更新以支持 spec-ctrl。
    5spectre-meltdown-checker https://meltdown.ovh/
    6PCID 现在是 x86 上的关键性能/安全特性 https://groups.google.com/forum/m/#!topic/mechanicalsympathy/L9mHTbeQLNU
    Proxmox VE Administration Guide 189 / 534
    ＃grep ' pcid ' /proc/cpuinfo
    如果返回不为空，则您的主机 CPU 支持 pcid。
    spec-ctrl
    在 retpolines 不足以应对的情况下，启用 Spectre v1（CVE-2017-5753）和 Spectre v2（CVE-2017-5715）修复所需。
    默认包含在带有 -IBRS 后缀的 Intel CPU 型号中。必须明确地为没有 -IBRS 后缀的 Intel CPU 型号打开。
    需要更新的主机 CPU 微码（intel-microcode >= 20180425）。
    ssbd
    启用 Spectre V4（CVE-2018-3639）修复所需。默认情况下，没有任何 Intel CPU 型号包括它。必须明确地为所有 Intel CPU 型号打开。
    需要更新的主机 CPU 微码（intel-microcode >= 20180703）。
AMD 处理器
    ibpb
    在 retpolines 不足以应对的情况下，启用 Spectre v1（CVE-2017-5753）和 Spectre v2（CVE-2017-5715）修复所需。
    默认包含在带有 -IBPB 后缀的 AMD CPU 型号中。必须明确地为没有 -IBPB 后缀的 AMD CPU 型号打开。
    要在客户端 CPU 上使用此功能，需要主机 CPU 微码支持此功能。
    virt-ssbd
    启用 Spectre v4（CVE-2018-3639）修复所需。默认情况下，没有任何 AMD CPU 型号包括它。必须明确地为所有 AMD CPU 型号打开。即使提供了 
    amd-ssbd，也应为最大客户端兼容性提供给客户端。请注意，当使用 "host" cpu 模型时，必须明确启用此功能，因为这是物理 CPU 中不存在的虚拟功能。
    amd-ssbd
    启用 Spectre v4（CVE-2018-3639）修复所需。默认情况下，没有任何 AMD CPU 型号包括它。必须明确地为所有 AMD CPU 型号打开。
    这比 virt-ssbd 提供更高的性能，因此支持此功能的主机应尽可能将此功能暴露给客户端。尽管如此，还应暴露 virt-ssbd 以获得最大的客户端兼容性，
    因为一些内核只知道 virt-ssbd。
    amd-no-ssb
    建议表示主机不易受 Spectre V4（CVE-2018-3639）攻击。默认情况下，没有任何 AMD CPU 型号包括它。
    未来的 CPU 硬件代不容易受到 CVE-2018-3639 攻击，因此应通过暴露 amd-no-ssb 告知客户端不要启用其缓解措施。这与 virt-ssbd 和 amd-ssbd 互斥。
NUMA
您还可以选择在 VM 中模拟 NUMA 7 架构。NUMA 架构的基本原理是，不是让所有核心共享一个全局内存池，而是将内存分散到靠近每个插槽的本地存储区域。
这样可以提高速度，因为内存总线不再成为瓶颈。
7https://en.wikipedia.org/wiki/Non-uniform_memory_access
Proxmox VE Administration Guide 190 / 534
如果您的系统具有 NUMA 架构 8，我们建议激活该选项，因为这将允许在主机系统上正确分配 VM 资源。此选项还需要在 VM 中热插拔内核或 RAM。
如果使用 NUMA 选项，建议将套接字数设置为主机系统节点数。
vCPU 热插拔
现代操作系统引入了在运行系统中热插拔和在一定程度上热拔出 CPU 的功能。虚拟化使我们能够避免实际硬件在这种场景下可能导致的许多（物理）问题。
然而，这仍然是一个相对较新且复杂的功能，因此应将其使用限制在绝对需要的情况下。
大部分功能可以通过其他经过充分测试且不那么复杂的功能实现，参见资源限制第 10.2.5 节。
在 Proxmox VE 中，插入的 CPU 最大数量始终为 cores * sockets。
要使用少于此总核心数的 CPU 启动 VM，可以使用 vpus 设置，它表示在 VM 启动时应插入多少个 vCPU。
目前只有 Linux 支持此功能，需要 3.10 以上的内核，推荐使用 4.7 以上的内核。
您可以使用以下 udev 规则在客户端中自动将新 CPU 设置为在线状态：
SUBSYSTEM=="cpu", ACTION=="add", TEST=="online", ATTR{online}=="0", ATTR{online}="1"
将此内容保存在 /etc/udev/rules.d/ 目录下，文件以 .rules 结尾。
注意：CPU 热拔除依赖于机器并需要客户端协作。
删除命令不能保证 CPU 实际被移除，通常它是一个请求，通过目标相关的机制（如 x86/amd64 上的 ACPI）转发给客户端操作系统。

10.2.6 内存
对于每个 VM，您可以选择设置固定大小的内存，或者要求 Proxmox VE 根据主机当前 RAM 使用情况动态分配内存。
8 如果命令 numactl --hardware | grep available 返回多个节点，则您的主机系统具有 NUMA 架构。
固定内存分配
当将内存和最小内存设置为相同数量时，Proxmox VE 将简单地为您的 VM 分配您指定的内存。
即使使用固定内存大小，气球设备也会添加到 VM 中，因为它提供了有用的信息，例如客户机实际使用了多少内存。通常情况下，您应该保持气球设备启用，
但如果您想禁用它（例如出于调试目的），只需取消选中“气球设备”或在配置中设置：
balloon: 0
自动内存分配
当将最小内存设置得低于内存时，Proxmox VE 将确保为 VM 始终提供您指定的最小内存，如果主机上的 RAM 使用率低于 80%，
则会动态向客户机添加内存，直到达到指定的最大内存。 
当主机 RAM 不足时，VM 会将一些内存释放回主机，如果需要，交换运行中的进程，并在最后一招中启动 oom 杀手。 
主机和客户机之间的内存传递是通过在客户机内运行的特殊气球内核驱动程序完成的，该驱动程序将从主机获取或释放内存页面。9
当多个 VM 使用自动分配功能时，可以设置一个 Shares 系数，表示每个 VM 应占用的空闲主机内存的相对数量。 假设您有四个 VM，其中三个运行 HTTP 
服务器，最后一个是数据库服务器。 为了在数据库服务器 RAM 中缓存更多的数据库块，您希望在有空闲 RAM 时优先考虑数据库 VM。 
为此，您为数据库 VM 分配一个 Shares 属性值 3000，将其他 VM 的 Shares 默认设置为 1000。 
主机服务器有 32GB 的 RAM，目前使用了 16GB，剩余 32 * 80/100 - 16 = 9GB RAM 可分配给 VM。 
数据库 VM 将获得额外的 9 * 3000 / (3000 + 1000 + 1000 + 1000) = 4.5 GB RAM，每个 HTTP 服务器将获得 1.5 GB。
在 2010 年之后发布的所有 Linux 发行版都包含了气球内核驱动程序。 对于 Windows 
操作系统，需要手动添加气球驱动程序，这可能会导致客户机速度变慢，因此我们不建议在关键系统上使用它。
在为 VM 分配 RAM 时，一个经验法则是始终为主机留出 1GB 的 RAM。

10.2.7 网络设备
每个 VM 可以有四种不同类型的多个网络接口控制器（NIC）：
    Intel E1000 是默认值，模拟 Intel 千兆网络卡。
    如果您追求最大性能，应使用 VirtIO 抛出虚拟化的 NIC。和所有 VirtIO 设备一样，客户机操作系统应安装适当的驱动程序。
    Realtek 8139 模拟较旧的 100 MB/s 网络卡，仅应在模拟较旧操作系统时使用（2002 年之前发布）
    vmxnet3 是另一种抛出虚拟化设备，只有在从另一个虚拟机管理程序导入 VM 时才应使用。
Proxmox VE 将为每个 NIC 生成一个随机 MAC 地址，以便您的 VM 可以在以太网网络上寻址。
您添加到 VM 的 NIC 可以遵循以下两种不同的模型之一：
    在默认的桥接模式下，每个虚拟 NIC 都由主机上的一个 tap 设备支持（模拟以太网 NIC 的软件环回设备）。
    此 tap 设备将添加到一个桥中，在 Proxmox VE 中默认为 vmbr0。在此模式下，VM 可以直接访问主机所在的以太网局域网。
    在另一种 NAT 模式下，每个虚拟 NIC 只会与 QEMU 用户网络堆栈通信，其中内置的路由器和 DHCP 服务器可以提供网络访问。
    这个内置的 DHCP 将在私有 10.0.2.0/24 范围内提供地址。 NAT 模式比桥接模式慢得多，只应用于测试。此模式仅通过 CLI 或 API 提供，但不通过 WebUI 提供。
在创建 VM 时，您还可以通过选择“无网络设备”来跳过添加网络设备。
您可以为每个 VM 网络设备覆盖 MTU 设置。选项 mtu=1 表示一个特殊情况，在这种情况下，MTU 值将从底层桥继承。此选项仅适用于 VirtIO 网络设备。
如果您使用 VirtIO 驱动程序，您可以选择性地激活 Multiqueue 选项。此选项允许客户机操作系统使用多个虚拟 CPU 来处理网络数据包，从而增加传输的数据包总数。
在使用 Proxmox VE 的 VirtIO 驱动程序时，每个 NIC 网络队列都会传递给主机内核，队列将由 vhost 驱动程序生成的内核线程处理。
通过激活此选项，可以为每个 NIC 将多个网络队列传递给主机内核。
使用 Multiqueue 时，建议将其设置为等于客户机总核心数的值。您还需要使用 ethtool 命令在 VM 中为每个 VirtIO NIC 设置多用途通道的数量：
ethtool -L ens1 combined X
其中 X 是 VM 的 vcpus 数量。
请注意，将 Multiqueue 参数设置为大于 1 的值将随着流量的增加增加主机和客户机系统上的 CPU 负载。
我们建议仅在 VM 需要处理大量传入连接时设置此选项，例如当 VM 作为路由器、反向代理或执行长轮询的繁忙 HTTP 服务器运行时。

10.2.8 显示
QEMU 可以虚拟几种类型的 VGA 硬件。一些例子是：
    std，默认值，模拟带有 Bochs VBE 扩展的卡。
    cirrus，曾经是默认值，它模拟了一个非常旧的硬件模块及其所有问题。此显示类型应仅在确实需要时使用 10，例如，如果使用 Windows XP 或更早版本
    vmware，是一个兼容 VMWare SVGA-II 的适配器。
    qxl，是 QXL 虚拟化图形卡。选择此选项还会为 VM 启用 SPICE（一种远程查看器协议）。
    virtio-gl，通常称为 VirGL，是供 VM 内部使用的虚拟 3D GPU，可以将工作负载卸载到主机 GPU，无需特殊（昂贵）型号和驱动程序，
    也无需完全绑定主机 GPU，允许在多个客户机和/或主机之间重用。
10 https://www.kraxel.org/blog/2014/10/qemu-using-cirrus-considered-harmful/ qemu: 使用 cirrus 被认为是有害的
注意
VirGL 支持需要一些额外的库，这些库由于相对较大且对于所有 GPU 型号/供应商并非全部开源，因此默认情况下没有安装。对于大多数设置，您只需执行以下操作：
apt install libgl1 libegl1
您可以通过设置 memory 选项来编辑分配给虚拟 GPU 的内存量。这可以在 VM 内启用更高的分辨率，尤其是在使用 SPICE/QXL 时。
由于内存由显示设备预留，为 SPICE 选择多监视器模式（例如 qxl2 用于双监视器）有一些影响：
    Windows 需要每个监视器一个设备，所以如果您的 ostype 是 Windows 的某个版本，Proxmox VE 会为 VM 提供每个监视器一个额外的设备。
    每个设备都会得到指定的内存量。Linux VM 可以始终启用更多虚拟监视器，但选择多监视器模式会使设备的内存乘以监视器的数量。
选择 serialX 作为显示类型将禁用 VGA 输出，并将 Web 控制台重定向到所选串行端口。在这种情况下，配置的显示内存设置将被忽略。

10.2.9 USB 透传
有两种不同类型的 USB 透传设备：
    主机 USB 透传
    SPICE USB 透传
主机 USB 透传是通过为 VM 提供主机的 USB 设备来实现的。这可以通过供应商和产品 ID 来完成，也可以通过主机总线和端口来完成。
供应商/产品 ID 的格式如下：0123:abcd，其中 0123 是供应商的 ID，abcd 是产品的 ID，这意味着相同的 usb 设备有相同的 ID。
总线/端口的格式如下：1-2.3.4，其中 1 是总线，2.3.4 是端口路径。这表示您主机的物理端口（取决于 usb 控制器的内部顺序）。
如果在 VM 配置中存在一个设备，当 VM 启动时，设备不在主机中，则 VM 可以正常启动。只要设备/端口在主机中可用，就会通过它。
警告
使用这种类型的 USB 透传意味着您无法将 VM 在线移动到另一个主机，因为硬件仅在当前 VM 所在的主机上可用。
第二种类型的透传是 SPICE USB 透传。如果您使用支持它的 SPICE 客户端，这会很有用。
如果您为 VM 添加一个 SPICE USB 端口，您可以将 USB 设备从 SPICE 客户端所在的位置直接透传到 VM（例如输入设备或硬件加密狗）。

10.2.10 BIOS 和 UEFI
为了正确模拟计算机，QEMU 需要使用固件。在常见的 PC 上通常被称为 BIOS 或 (U)EFI，在启动 VM 
时作为第一步之一执行。它负责进行基本硬件初始化，并为操作系统提供固件和硬件接口。默认情况下，QEMU 使用 SeaBIOS，这是一个开源的 x86 BIOS 
实现。对于大多数标准设置，SeaBIOS 是一个不错的选择。
某些操作系统（如 Windows 11）可能需要使用兼容 UEFI 的实现。在这种情况下，您必须使用 OVMF，这是一个开源的 UEFI 实现。11
还有其他情况，其中 SeaBIOS 可能不是从其启动的理想固件，例如，如果您想进行 VGA 透传。12
如果您想使用 OVMF，有几件事情需要考虑：
为了保存诸如启动顺序之类的事物，需要有一个 EFI 磁盘。此磁盘将包含在备份和快照中，且只能有一个。
您可以使用以下命令创建这样一个磁盘：
# qm set <vmid> -efidisk0 <storage>:1,format=<format>,efitype=4m,pre-  -
enrolled-keys=1
其中 <storage> 是您想要存放磁盘的存储，<format> 是存储支持的格式。或者，您可以通过在 VM 的硬件部分添加 Add ! EFI Disk 通过 Web 界面创建这样一个磁盘。
efitype 选项指定应使用哪个版本的 OVMF 固件。对于新 VM，这应始终是 4m，因为它支持 Secure Boot 并为未来的开发分配了更多空间（这是 GUI 中的默认值）。
pre-enroll-keys 指定 efidisk 是否应预先加载特定于发行版的和 Microsoft Standard Secure Boot 密钥。
它还默认启用 Secure Boot（尽管仍可以在 VM 内的 OVMF 菜单中禁用）。
注意
如果您想在现有 VM（仍然使用 2m efidisk）中开始使用 Secure Boot，您需要重新创建 efidisk。
为此，请删除旧的 efidisk（qm set <vmid> -delete efidisk0）并按照上述方法添加新的 efidisk。这将重置您在 OVMF 菜单中所做的任何自定义配置！
在使用虚拟显示器的 OVMF（无 VGA 透传）时，您需要在 OVMF 菜单中设置客户端分辨率（在启动过程中按 ESC 键即可进入），或者选择 SPICE 作为显示类型。

10.2.11 可信平台模块 (TPM)
可信平台模块是一种安全存储机密数据（如加密密钥）并为验证系统启动提供防篡改功能的设备。
11请参阅 OVMF 项目 https://github.com/tianocore/tianocore.github.io/wiki/OVMF
12AlexWilliamson 有一篇关于此的很好的博客文章 https://vfio.blogspot.co.at/2014/08/primary-graphics-assignment-withoutvga.html
某些操作系统（如 Windows 11）要求在机器（物理或虚拟）上安装这样的设备。
通过指定 tpmstate 卷添加 TPM。这类似于 efidisk，一旦创建就不能更改（只能删除）。您可以通过以下命令添加一个：
# qm set <vmid> -tpmstate0 <storage>:1,version=<version>
其中 <storage> 是您想要将状态放在的存储，<version> 是 v1.2 或 v2.0。您还可以通过 Web 界面添加一个，在 VM 的硬件部分选择 Add ! TPM State。
v2.0 TPM 规范较新，支持性更好，因此除非您有需要 v1.2 TPM 的特定实现，否则应首选 v2.0 TPM。
注意
与物理 TPM 相比，模拟的 TPM 并没有提供任何实质性的安全优势。TPM 的重点是其中的数据除了通过 TPM 
规范指定的命令之外，不能轻易修改。由于在模拟设备中，数据存储在普通卷上，所以任何有权访问该卷的人都有可能编辑它。

10.2.12 虚拟机间共享内存
您可以添加一个虚拟机间共享内存设备 (ivshmem)，它允许在主机和客户机之间或多个客户机之间共享内存。要添加这样的设备，您可以使用 qm：
# qm set <vmid> -ivshmem size=32,name=foo
大小以 MiB 为单位。文件将位于 /dev/shm/pve-shm-$name 下（默认名称为 vmid）。
注意
目前，只要使用它的任何虚拟机关闭或停止，设备将被删除。打开的连接仍将持续，但无法再建立与同一设备的新连接。
这种设备的一个用例是 Looking Glass 13 项目，它可以在主机和客户机之间实现高性能、低延迟的显示镜像。

10.2.13 音频设备
要添加音频设备，请运行以下命令：
qm set <vmid> -audio0 device=<device>
支持的音频设备有：
    ich9-intel-hda：Intel HD Audio 控制器，模拟 ICH9
    intel-hda：Intel HD Audio 控制器，模拟 ICH6
    AC97：音频编解码器 '97，适用于较旧的操作系统，如 Windows XP
有两个可用的后端：
    spice
    none
spice 后端可与 SPICE Section 10.2.8 结合使用，而 none 后端可在需要在虚拟机中使用音频设备以使某些软件工作时使用。
要使用主机的物理音频设备，请使用设备直通（参见 PCI 直通第 10.9 节和 USB 直通第 10.2.9 节）。
像 Microsoft 的 RDP 这样的远程协议具有播放声音的选项。

10.2.14 VirtIO RNG
RNG（随机数生成器）是向系统提供熵（随机性）的设备。虚拟硬件-RNG 
可用于将来自主机系统的熵提供给客户机虚拟机。这有助于避免客户机中的熵饥饿问题（熵不足的情况，可能导致系统速度减慢或出现问题），特别是在客户机启动过程中。
要添加基于 VirtIO 的仿真 RNG，请运行以下命令：
qm set <vmid> -rng0 source=<source>[,max_bytes=X,period=Y]
source 指定在主机上从哪里读取熵，必须是以下之一：
    /dev/urandom：非阻塞内核熵池（首选）
    /dev/random：阻塞内核池（不推荐，可能导致主机系统上的熵饥饿）
    /dev/hwrng：传递连接到主机的硬件 RNG（如果有多个可用，则将使用在 /sys/devices/virtual/misc/hw_random/rng_current 中选择的一个）
通过 max_bytes 和 period 参数可以指定限制，它们以每 period 毫秒为单位读取 max_bytes。
然而，它并不代表线性关系：1024B/1000ms 意味着在 1 秒的计时器上，最多有 1 KiB 的数据可用，而不是在 1 秒的过程中将 1 KiB 
数据流传输到客户机。因此，减少周期可以用于以更快的速度向客户机注入熵。
默认情况下，限制设置为每 1000 毫秒 1024 字节（1 KiB/s）。建议始终使用限制器以避免客户端使用过多的主机资源。
如果需要，可以使用 max_bytes 的值 0 来禁用所有限制。

10.2.15 设备启动顺序
QEMU 可以告诉客户机应该从哪些设备启动，以及启动顺序。这可以在配置中通过 boot 属性指定，例如：
vbnet
boot: order=scsi0;net0;hostpci0
这样，客户机首先尝试从磁盘 scsi0 启动，如果失败，它将继续尝试从 net0 网络启动，如果还是失败，最后尝试从传递的 PCIe 设备启动（如果是 
NVMe，则显示为磁盘，否则尝试启动到选项 ROM）。
在 GUI 上，您可以使用拖放编辑器指定启动顺序，并使用复选框启用或禁用某些设备的启动。
注意
如果您的客户机使用多个磁盘启动操作系统或加载引导程序，则所有这些磁盘都必须标记为可启动（即它们必须启用复选框或出现在配置列表中），以便客户机能够启动。
这是因为最近的 SeaBIOS 和 OVMF 版本只有在磁盘被标记为可启动时才会初始化磁盘。
在任何情况下，即使设备未出现在列表中或禁用了复选标记，一旦客户机的操作系统启动并初始化它们，它们仍将对客户机可用。可启动标志仅影响客户机 BIOS 和引导加载程序。

10.2.16 虚拟机的自动启动和关闭
创建 VM 后，您可能希望它们在主机系统启动时自动启动。为此，您需要从 Web 界面中的 VM 的“选项”选项卡中选择“启动时启动”选项，或使用以下命令设置：
# qm set <vmid> -onboot 1
启动和关机顺序
在某些情况下，您可能希望能够微调 VM 的启动顺序，例如，如果您的一个 VM 为其他客户机系统提供防火墙或 DHCP。对此，您可以使用以下参数：
    启动/关机顺序：定义启动顺序优先级。例如，如果您希望 VM 首先启动，请将其设置为 1。（我们为关闭使用相反的启动顺序，因此启动顺序为 1 
    的计算机将是最后一个被关闭的计算机）。如果一个主机上定义了多个具有相同顺序的 VM，它们将按 VMID 升序排序。
    启动延迟：定义此 VM 启动和后续 VM 启动之间的时间间隔。例如，如果您希望在启动其他 VM 之前等待 240 秒，请将其设置为 240。
    关机超时：定义 Proxmox VE 在发出关机命令后应等待 VM 离线的时间（以秒为单位）。
    默认情况下，此值设置为 180，这意味着 Proxmox VE 将发出关机请求并等待 180 秒以使计算机离线。如果计算机在超时后仍在线，它将被强制停止。
注意
由 HA 堆栈管理的 VM 当前不遵循启动顺序和启动顺序选项。启动和关机算法将跳过这些 VM，因为 HA 管理器本身确保 VM 被启动和停止。
请注意，没有设置启动/关机顺序参数的计算机将始终在设置参数的计算机之后启动。此外，此参数只能在同一主机上运行的虚拟机之间强制执行，而不是在整个集群范围内。
如果您需要在主机启动和第一个 VM 启动之间有延迟，请参阅有关 Proxmox VE 节点管理的部分 3.10.4。

10.2.17 QEMU 客户机代理
QEMU 客户机代理是在 VM 内部运行的服务，它在主机和客户机之间提供了通信通道。它用于交换信息并允许主机向客户机发出命令。
例如，VM 概要面板中的 IP 地址是通过客户机代理获取的。
或者，在开始备份时，通过客户机代理告诉客户机通过 fs-freeze 和 fs-thaw 命令同步未完成的写操作。
为使客户机代理正常工作，必须采取以下步骤：
    在客户机中安装代理并确保其正在运行
    在 Proxmox VE 中启用通过代理的通信
安装客户机代理
对于大多数 Linux 发行版，客户机代理是可用的。软件包通常名为 qemu-guest-agent。
对于 Windows，可以从 Fedora VirtIO 驱动程序 ISO 安装。
启用客户机代理通信
可以在 VM 的选项面板中启用 Proxmox VE 与客户机代理的通信。需要重新启动 VM 以使更改生效。
自动使用 QGA 进行 TRIM
可以启用“运行 guest-trim”选项。启用此功能后，Proxmox VE 将在以下可能将零写入存储的操作之后向客户机发出 trim 命令：
    将磁盘移动到另一个存储
    将 VM 实时迁移到具有本地存储的另一个节点
    在支持瘦分配的存储上，这有助于释放未使用的空间。
    注意
    在 Linux 上的 ext4 存在一个问题，因为它使用内存优化以避免发出重复的 TRIM 请求。由于客户机不知道底层存储的更改，因此只有第一个 guest-trim 
    将按预期运行。此后的请求，直到下次重启，将仅考虑从那时起发生更改的文件系统的部分。
备份时的文件系统冻结和解冻
默认情况下，在执行备份时，客户机文件系统通过 fs-freeze QEMU 客户机代理命令进行同步，以提供一致性。
在 Windows 客户机上，某些应用程序可能通过挂钩到 Windows VSS（卷影复制服务）层来处理一致性备份，然后 fs-freeze 可能会干扰它。
例如，已观察到，使用某些 SQL 服务器调用 fs-freeze 会触发 VSS 调用 SQL Writer VSS 模块，从而中断差异备份的 SQL 服务器备份链。
对于此类设置，您可以通过将 freeze-fs-on-backup QGA 选项设置为 0 来配置 Proxmox VE 在备份时不发出冻结和解冻周期。此选项可以通过 CLI 或 API 设置。
重要
禁用此选项可能会导致文件系统不一致的备份，因此只有在了解操作原理的情况下才应禁用。
故障排除
虚拟机无法关闭
确保安装并运行客户机代理。
启用客户机代理后，Proxmox VE 将通过客户机代理发送关闭等电源命令。如果客户机代理未运行，命令将无法正常执行，并且关闭命令将遇到超时。

10.2.18 SPICE 增强功能
SPICE 增强功能是可选的特性，可以改善远程查看器的体验。
要通过 GUI 启用它们，请转到虚拟机的选项面板。运行以下命令通过 CLI 启用它们：
qm set <vmid> -spice_enhancements foldersharing=1,videostreaming=all
注意
要使用这些功能，虚拟机的显示必须设置为 SPICE (qxl)。
文件夹共享
与客户机共享本地文件夹。客户机中需要安装 spice-webdavd 守护程序。它通过位于 http://localhost:9843 的本地 WebDAV 服务器提供共享文件夹。
对于 Windows 客户机，可以从官方 SPICE 网站下载 Spice WebDAV 守护程序的安装程序。
大多数 Linux 发行版都有一个名为 spice-webdavd 的软件包可以安装。
要在 Virt-Viewer（远程查看器）中共享文件夹，请转到文件 ! 首选项。选择要共享的文件夹，然后启用复选框。
注意
文件夹共享目前仅在 Virt-Viewer 的 Linux 版本中工作。
警告
实验性！目前此功能无法可靠地工作。
视频流
快速刷新区域被编码为视频流。有两个选项：
    all：任何快速刷新区域都将编码为视频流。
    filter：使用其他过滤器来决定是否应使用视频流（目前仅跳过小窗口表面）。
    关于是否应启用视频流以及应选择哪个选项，无法给出一般性建议。在特定情况下，您的实际效果可能会有所不同。
故障排除
共享文件夹未显示
确保 WebDAV 服务在客户机中已启用并正在运行。在 Windows 上，它被称为 Spice webdav 代理。
在 Linux 中，名称为 spice-webdavd，但根据发行版的不同可能会有所不同。
如果服务正在运行，请在客户机中的浏览器中打开 http://localhost:9843 以检查 WebDAV 服务器。重新启动 SPICE 会话可能会有所帮助。

10.3 迁移
如果您有一个集群，可以使用以下命令将 VM 迁移到另一个主机：
qm migrate <vmid> <target>
通常有两种机制：
    在线迁移（又称为实时迁移）
    离线迁移

10.3.1 在线迁移
如果您的 VM 正在运行并且没有配置本地绑定的资源（例如通过的设备），则可以在 qm 迁移命令调用中使用 --online 标志启动实时迁移。
当 VM 正在运行时，Web 界面默认为实时迁移。
如何工作
在线迁移首先在目标主机上使用 incoming 标志启动一个新的 QEMU 进程，该进程仅执行基本初始化（客户机 vCPU 
仍处于暂停状态），然后等待源虚拟机的客户机内存和设备状态数据流。所有其他资源（如磁盘）要么是共享的，要么在 VM 
运行时状态迁移开始之前已经发送；因此只剩下内存内容和设备状态需要传输。
一旦建立了此连接，源开始异步将内存内容发送到目标。如果源上的客户机内存发生变化，这些部分将被标记为脏，然后再次发送客户机内存数据。
这个循环重复进行，直到运行中的源 VM 和传入的目标 VM 之间的数据差异足够小，可以在几毫秒内发送，因为这时源 VM 
可以完全暂停，而用户或程序都不会注意到暂停，以便将剩余数据发送到目标，然后取消目标 VM 的 CPU 暂停，使其在不到一秒的时间内成为新的运行 VM。
要求
要使实时迁移正常工作，需要满足以下条件：
    VM 没有无法迁移的本地资源。例如，当前阻止实时迁移的 PCI 或 USB 设备。另一方面，本地磁盘可以通过将它们发送到目标来进行迁移。
    主机位于同一个 Proxmox VE 集群中。
    主机之间具有正常且可靠的网络连接。
    目标主机必须具有相同或更高版本的 Proxmox VE 软件包。尽管有时候反过来也可以工作，但不能保证。
    主机具有来自相同供应商且具有类似功能的 CPU。
    不同供应商可能会根据实际型号和 VM 配置的 CPU 类型而有所不同，但不能保证 - 因此请在将此类设置部署到生产环境之前进行测试。

10.3.2 离线迁移
如果您有本地资源，只要所有磁盘都存储在两个主机上定义的存储上，您仍然可以离线迁移您的 
VM。然后，迁移会像在线迁移一样将磁盘通过网络复制到目标主机。请注意，任何硬件直通配置可能需要根据目标主机上的设备位置进行调整。

10.4 副本和克隆
VM 的安装通常是使用操作系统供应商的安装介质（CD-ROM）完成的。根据操作系统，这可能是一个耗时的任务，您可能希望避免。
部署相同类型的多个 VM 的简单方法是复制现有的 VM。我们使用术语克隆来表示这样的副本，并区分链接克隆和完全克隆。
完全克隆
这种复制的结果是一个独立的 VM。新的 VM 不与原始 VM 共享任何存储资源。
可以选择目标存储，因此可以将 VM 迁移到完全不同的存储。如果存储驱动程序支持多种格式，您还可以更改磁盘映像格式。
注意
完全克隆需要读取和复制所有 VM 镜像数据。这通常比创建链接克隆慢得多。
某些存储类型允许复制特定快照，这默认为当前 VM 数据。这也意味着最终的副本永远不会包含来自原始 VM 的任何其他快照。
链接克隆
现代存储驱动程序支持一种生成快速链接克隆的方法。这样的克隆是一个可写副本，其初始内容与原始数据相同。
创建链接克隆几乎是瞬时的，最初不会占用额外的空间。
它们被称为链接克隆，因为新映像仍然引用原始映像。未修改的数据块从原始映像中读取，但修改过的数据块从新位置写入（然后从新位置读取）。
这种技术称为写时复制。
这要求原始卷是只读的。使用 Proxmox VE，可以将任何 VM 转换为只读模板。以后可以使用这些模板高效地创建链接克隆。
注意
在链接克隆存在时，您不能删除原始模板。
对于链接克隆，无法更改目标存储，因为这是存储内部功能。
目标节点选项允许您在不同的节点上创建新的 VM。唯一的限制是 VM 位于共享存储上，并且该存储也在目标节点上可用。
为避免资源冲突，所有网络接口 MAC 地址会被随机化，我们为 VM BIOS（smbios1）设置生成新的 UUID。

10.5 虚拟机模板
可以将 VM 转换为模板。这些模板是只读的，您可以使用它们创建链接克隆。
注意
模板无法启动，因为这会修改磁盘映像。如果要更改模板，请创建链接克隆并修改它。

10.6 VM 生成 ID
Proxmox VE 支持虚拟机生成 ID（vmgenid）14用于虚拟机。客户操作系统可以使用此功能检测导致时间偏移事件的任何事件，例如恢复备份或快照回滚。
14官方 vmgenid 规范 https://docs.microsoft.com/en-us/windows/desktop/hyperv_v2/virtual-machine-generation-identifier
在创建新 VM 时，将自动生成 vmgenid 并将其保存在其配置文件中。
要为已经存在的 VM 创建并添加 vmgenid，可以将特殊值“1”传递给 Proxmox VE 以自动生成，也可以手动设置 UUID 15 作为值，例如：
qm set VMID -vmgenid 1
qm set VMID -vmgenid 00000000-0000-0000-0000-000000000000
注意
将 vmgenid 设备添加到现有 VM 的初始操作可能导致快照回滚、备份还原等更改的相同效果，因为 VM 可能将其解释为生成更改。
在极少数情况下，如果不希望使用 vmgenid 机制，可以在创建 VM 时为其值传递“0”，或使用以下命令在配置中回溯删除属性：
qm set VMID -delete vmgenid
vmgenid 的最显著用例是较新的 Microsoft Windows 操作系统，它们使用它来避免在快照回滚、
备份还原或整个 VM 克隆操作中遇到时间敏感或复制服务（如数据库或域控制器 16）的问题。

10.7 导入虚拟机和磁盘映像
从外部虚拟机管理器导出的 VM 通常采用一个或多个磁盘映像的形式，配置文件描述了 VM 的设置（RAM，内核数量）。
如果磁盘来自 VMware 或 VirtualBox，磁盘映像可以采用 vmdk 格式；如果磁盘来自 KVM 虚拟机管理器，则可以采用 qcow2 格式。VM 导出的最流行配置格式是 OVF 
标准，但实际上互操作性受限，因为许多设置未在标准本身中实现，虚拟机管理器将补充信息导出为非标准扩展。
除了格式问题外，如果从一个虚拟机管理器更改为另一个虚拟机管理器时，模拟硬件的变化过大，从其他虚拟机管理器导入磁盘映像可能会失败。Windows VM 
尤其受此影响，因为操作系统对任何硬件更改非常挑剔。在导出前安装 MergeIDE.zip 实用程序并在启动导入的 Windows VM 之前选择 IDE 硬盘类型可以解决这个问题。
最后，还有一个问题是关于准虚拟化驱动程序，这些驱动程序可以提高模拟系统的速度，并且特定于虚拟机管理程序。GNU/Linux 和其他免费的 Unix 
操作系统默认已经安装了所有必要的驱动程序，您可以在导入虚拟机后立即切换到准虚拟化驱动程序。对于 Windows 虚拟机，您需要自己安装 Windows 准虚拟化驱动程序。
通常，GNU/Linux 和其他免费的 Unix 虚拟机可以轻松导入。请注意，由于上述问题，我们无法在所有情况下保证成功导入/导出 Windows 虚拟机。

10.7.1 逐步演示 Windows OVF 导入示例
Microsoft 提供虚拟机下载以开始 Windows 开发。我们将使用其中一个来演示 OVF 导入功能。
15在线 GUID 生成器 http://guid.one/
16https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/get-started/virtual-dc/virtualized-domain-controllerarchitecture
下载虚拟机 zip 文件
在了解用户协议后，选择 VMware 平台的 Windows 10 Enterprise (Evaluation - Build)，并下载 zip 文件。
从 zip 文件中提取磁盘映像
使用 unzip 实用程序或您选择的任何压缩工具解压缩 zip 文件，然后通过 ssh/scp 将 ovf 和 vmdk 文件复制到您的 Proxmox VE 主机。
导入虚拟机
这将创建一个新的虚拟机，使用从 OVF 清单中读取的核心、内存和 VM 名称，并将磁盘导入到 local-lvm 存储中。您需要手动配置网络。
qm importovf 999 WinDev1709Eval.ovf local-lvm
现在，虚拟机已准备好启动。

10.7.2 向虚拟机添加外部磁盘映像
您还可以将现有磁盘映像添加到 VM，无论是来自外部虚拟机管理程序还是您自己创建的映像。
假设您使用 vmdebootstrap 工具创建了一个 Debian/Ubuntu 磁盘映像：
vmdebootstrap --verbose \
--size 10GiB --serial-console \
--grub --no-extlinux \
--package openssh-server \
--package avahi-daemon \
--package qemu-guest-agent \
--hostname vm600 --enable-dhcp \
--customize=./copy_pub_ssh.sh \
--sparse --image vm600.raw
现在，您可以创建一个新的目标 VM，将映像导入到存储 pvedir 并将其连接到 VM 的 SCSI 控制器：
# qm create 600 --net0 virtio,bridge=vmbr0 --name vm600 --serial0 socket \
--boot order=scsi0 --scsihw virtio-scsi-pci --ostype l26 \
--scsi0 pvedir:0,import-from=/path/to/dir/vm600.raw
VM已准备好启动。

10.8 Cloud-Init 支持
Cloud-Init 是一个事实上的多发行版软件包，用于处理虚拟机实例的早期初始化。使用 Cloud-Init，可以在虚拟机管理程序端配置网络设备和 ssh 密钥。
当 VM 第一次启动时，VM 内的 Cloud-Init 软件将应用这些设置。
许多 Linux 发行版提供现成的 Cloud-Init 映像，主要为 OpenStack 设计。这些映像也适用于 Proxmox 
VE。虽然使用这些现成的映像似乎很方便，但我们通常建议您自己准备映像。这样的优点是您将确切知道已安装了什么，这将有助于您以后轻松地根据需要定制映像。
创建了这样一个 Cloud-Init 映像后，我们建议将其转换为 VM 模板。从 VM 模板中，您可以快速创建链接克隆，这是快速部署新 VM 实例的方法。在启动新 VM 
之前，您只需配置网络（可能还有 ssh 密钥）。
我们建议使用基于 SSH 密钥的身份验证来登录由 Cloud-Init 部署的 VM。也可以设置密码，但这不如使用基于 SSH 密钥的身份验证安全，
因为 Proxmox VE 需要将该密码的加密版本存储在 Cloud-Init 数据中。
Proxmox VE 生成一个 ISO 映像，将 Cloud-Init 数据传递给 VM。为此，所有 Cloud-Init VM 都需要分配一个 CD-ROM 驱动器。通常，应添加一个串行控制台并用作显示器。
许多 Cloud-Init 映像依赖于此，这是 OpenStack 的要求。然而，其他映像可能会在此配置下出现问题。如果使用串行控制台不起作用，请切换回默认显示配置。

10.8.1 准备 Cloud-Init 模板
第一步是准备您的 VM。基本上，您可以使用任何 VM。只需在要准备的 VM 内安装 Cloud-Init 软件包。在基于 Debian/Ubuntu 的系统上，这非常简单：
apt-get install cloud-init
警告
此命令仅在 VM 内执行，而不是在 Proxmox VE 主机上执行。
许多发行版已经提供现成的 Cloud-Init 映像（以 .qcow2 文件形式提供），因此您可以简单地下载并导入这些映像。
在以下示例中，我们将使用 Ubuntu 提供的云映像，网址为 https://cloud-images.ubuntu.com。
# 下载映像
wget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img
# 使用 VirtIO SCSI 控制器创建一个新的 VM
qm create 9000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci
# 将下载的磁盘导入到 local-lvm 存储中，作为 SCSI 驱动器进行连接
qm set 9000 --scsi0 local-lvm:0,import-from=/path/to/bionic-server-cloudimg-amd64.img
注意
Ubuntu Cloud-Init 映像要求 SCSI 驱动器使用 virtio-scsi-pci 控制器类型。
添加 Cloud-Init CD-ROM 驱动器
下一步是配置一个 CD-ROM 驱动器，该驱动器将用于将 Cloud-Init 数据传递给 VM。
qm set 9000 --ide2 local-lvm:cloudinit
为了能够直接从 Cloud-Init 映像引导，将启动参数设置为 order=scsi0，以限制 BIOS 仅从此磁盘启动。这将加快启动速度，因为 VM BIOS 跳过了对可引导 CD-ROM 的测试。
qm set 9000 --boot order=scsi0
对于许多 Cloud-Init 映像，需要配置一个串行控制台并将其用作显示器。但是，如果给定映像的配置不起作用，请切换回默认显示。
qm set 9000 --serial0 socket --vga serial0
最后一步，将 VM 转换为模板会很有帮助。然后，您可以从此模板快速创建链接克隆。从 VM 模板部署比创建完整克隆（复制）要快得多。
qm template 9000
完成这些步骤后，您的 VM 就已转换为模板。从此模板，您可以快速创建并部署新的 VM 实例。

10.8.2 部署 Cloud-Init 模板
您可以通过克隆轻松部署此类模板：
qm clone 9000 123 --name ubuntu2
然后配置用于身份验证的 SSH 公钥，并配置 IP 设置：
qm set 123 --sshkey ~/.ssh/id_rsa.pub
qm set 123 --ipconfig0 ip=10.0.10.123/24,gw=10.0.10.1
您还可以使用单个命令配置所有 Cloud-Init 选项。我们只是将上述示例拆分为几个命令以减少行长度。同时确保根据您的特定环境采用 IP 设置。

10.8.3 自定义 Cloud-Init 配置
Cloud-Init 集成还允许使用自定义配置文件而不是自动生成的配置。这是通过命令行上的 cicustom 选项完成的：
qm set 9000 --cicustom "user=<volume>,network=<volume>,meta=<volume>"
自定义配置文件必须位于支持片段的存储上，并且必须在 VM 要迁移到的所有节点上可用。否则，VM 将无法启动。例如：
qm set 9000 --cicustom "user=local:snippets/userconfig.yaml"
Cloud-Init 有三种配置。第一种是用户配置，如上例所示。第二种是网络配置，第三种是元配置。它们都可以一起指定或根据需要混合搭配。
对于没有指定自定义配置文件的任何配置，将使用自动生成的配置。
可以将生成的配置转储为自定义配置的基础：
qm cloudinit dump 9000 user
同样的命令也适用于网络和元配置。

10.8.4 Cloud-Init 特定选项
cicustom: [meta=<volume>] [,network=<volume>] [,user=<volume>]
[,vendor=<volume>]
指定在启动时替换自动生成的文件。
meta=<volume>
指定一个包含所有通过 cloud-init 传递给 VM 的元数据的自定义文件。这是特定于提供商的，意味着 configdrive2 和 nocloud 不同。
network=<volume>
传递包含所有网络数据的自定义文件，通过 cloud-init 传递给 VM。
user=<volume>
传递包含所有用户数据的自定义文件，通过 cloud-init 传递给 VM。
vendor=<volume>
传递包含所有供应商数据的自定义文件，通过 cloud-init 传递给 VM。
cipassword: <string>
分配给用户的密码。通常不建议使用此选项。请改用 ssh 密钥。还要注意，较旧的 cloud-init 版本不支持散列密码。
citype: <configdrive2 | nocloud | opennebula>
指定 cloud-init 配置格式。默认值取决于配置的操作系统类型（ostype）。我们对于 Linux 使用 nocloud 格式，对于 Windows 使用 configdrive2。
ciuser: <string>
要更改 ssh 密钥和密码的用户名，而不是镜像配置的默认用户。
ipconfig[n]: [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>]
[,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]
指定相应接口的 IP 地址和网关。
IP 地址使用 CIDR 表示法，网关是可选的，但需要指定相同类型的 IP。特殊字符串 dhcp 可用于使用 DHCP 的 IP 地址，在这种情况下不应提供显式网关。
对于 IPv6，特殊字符串 auto 可用于使用无状态自动配置。这需要 cloud-init 19.4 或更新版本。
如果启用了 cloud-init 并且未指定 IPv4 或 IPv6 地址，则默认在 IPv4 上使用 dhcp。
gw=<GatewayIPv4>
IPv4 流量的默认网关。
注意
需要选项：ip
gw6=<GatewayIPv6>
IPv6 流量的默认网关。
注意
需要选项：ip6
ip=<IPv4Format/CIDR> (默认 = dhcp)
CIDR 格式的 IPv4 地址。
ip6=<IPv6Format/CIDR> (默认 = dhcp)
CIDR 格式的 IPv6 地址。
nameserver: <string>
为容器设置 DNS 服务器 IP 地址。如果未设置 searchdomain 或 nameserver，创建将自动使用主机上的设置。
searchdomain: <string>
为容器设置 DNS 搜索域。如果未设置 searchdomain 或 nameserver，创建将自动使用主机上的设置。
sshkeys: <string>
设置公共 SSH 密钥（每行一个密钥，OpenSSH 格式）。

10.9 PCI(e) 透传
PCI(e) 透传是一种将主机上的 PCI 设备控制权交给虚拟机的机制。与使用虚拟化硬件相比，这可能具有一些优势，例如更低的延迟、更高的性能或更多的功能（例如，卸载）。
但是，如果将设备透传给虚拟机，则不能再在主机或任何其他虚拟机中使用该设备。

10.9.1 通用要求
由于透传是一项需要硬件支持的功能，因此需要检查一些要求并进行一些准备工作以使其正常工作。
硬件
您的硬件需要支持 IOMMU（I/O 内存管理单元）中断重新映射，包括 CPU 和主板。
通常，支持 VT-d 的英特尔系统和支持 AMD-Vi 的 AMD 系统支持此功能。但不能保证一切都能开箱即用，因为硬件实现不佳以及缺少或质量较低的驱动程序。
此外，服务器级硬件往往比消费级硬件支持得更好，但即便如此，许多现代系统也可以支持这一功能。
请咨询您的硬件供应商，以检查他们是否支持您特定设置下的 Linux 透传功能。
配置
确保硬件支持透传后，您需要进行一些配置以启用 PCI(e) 透传。
IOMMU
首先，您需要在 BIOS/UEFI 中启用 IOMMU 支持。通常，相应的设置称为 IOMMU 或 VT-d，但您应在主板手册中找到确切的选项名称。
对于英特尔 CPU，您可能还需要通过添加以下内容在内核命令行（第 3.12.6 节）上启用 IOMMU，以便支持较旧的（5.15 之前的）内核：
intel_iommu=on
对于 AMD CPU，应自动启用。
IOMMU 透传模式
如果您的硬件支持 IOMMU 透传模式，启用此模式可能会提高性能。这是因为虚拟机绕过了超级管理程序通常执行的（默认）DMA 转换，
而是将 DMA 请求直接传递给硬件 
IOMMU。要启用这些选项，请将以下内容添加到内核命令行（第 3.12.6 节）：
iommu=pt
内核模块
您需要确保加载了以下模块。可以通过将它们添加到 /etc/modules 来实现：
vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd
在更改任何模块相关内容后，您需要刷新 initramfs。在 Proxmox VE 上，可以通过执行以下命令完成：
update-initramfs -u -k all
完成配置
最后，重新启动以使更改生效，并检查是否确实已启用。
dmesg | grep -e DMAR -e IOMMU -e AMD-Vi
应显示已启用 IOMMU、Directed I/O 或中断重新映射，具体取决于硬件和内核，准确的消息可能有所不同。
同样重要的是，您要传递的设备必须位于单独的 IOMMU 组中。这可以通过以下命令进行检查：
# find /sys/kernel/iommu_groups/ -type l
设备与其功能（例如，带有 HDMI 音频设备的 GPU）或其根端口或 PCI(e) 桥一起位于 IOMMU 组中也是可以的。
PCI(e) 插槽
某些平台处理其物理 PCI(e) 插槽的方式不同。因此，如果您没有获得所需的 IOMMU 组分离，有时将卡放入另一个 PCI(e) 插槽可能会有所帮助。
不安全中断
对于某些平台，可能需要允许不安全中断。为此，请在 /etc/modprobe.d/ 中以 .conf 结尾的文件中添加以下行：
options vfio_iommu_type1 allow_unsafe_interrupts=1
请注意，此选项可能会使您的系统不稳定。
GPU 透传注意事项
无法通过 Proxmox VE 网络界面上的 NoVNC 或 SPICE 显示 GPU 的帧缓冲区。
在透传整个 GPU 或 vGPU 时，如需图形输出，必须将监视器物理连接到显卡，或者在客户机中配置远程桌面软件（例如，VNC 或 RDP）。
如果您想将 GPU 用作硬件加速器，例如，用于使用 OpenCL 或 CUDA 的程序，这是不需要的。

10.9.2 主机设备透传
最常用的 PCI(e) 透传变体是透传整个 PCI(e) 卡，例如 GPU 或网络卡。
主机配置
在这种情况下，主机不能使用该卡。有两种方法可以实现这一点：
• 通过在 /etc/modprobe.d/ 中的 .conf 文件添加以下内容，将设备 ID 传递给 vfio-pci 模块的选项：
options vfio-pci ids=1234:5678,4321:8765
其中 1234:5678 和 4321:8765 是通过以下命令获得的供应商和设备 ID：
lspci -nn
• 在主机上完全屏蔽驱动程序，确保它可以自由绑定以进行透传，方法是在 /etc/modprobe.d/ 中的以 .conf 结尾的文件中添加：
blacklist <driver_name>
将 <driver_name> 替换为要屏蔽的驱动程序名称。
blacklist DRIVERNAME
对于这两种方法，您都需要再次更新 initramfs（第 10.9.1 节），然后重新启动。
验证配置
要检查您的更改是否成功，您可以使用以下命令：
# lspci -nnk
然后检查设备条目。如果它显示：
Kernel driver in use: vfio-pci
或者完全缺少 in use 行，那么该设备已准备好用于透传。
虚拟机配置
要透传设备，您需要在虚拟机配置中设置 hostpciX 选项，例如通过执行以下命令：
# qm set VMID -hostpci0 00:02.0
如果您的设备具有多个功能（例如，'00:02.0' 和 '00:02.1'），您可以使用简化的语法 00:02` 一起传递它们。这相当于在 web 界面中勾选 All Functions` 复选框。
根据设备和客户操作系统，可能需要一些选项：
• x-vga=on|off 将 PCI(e) 设备标记为 VM 的主 GPU。启用此功能时，将忽略 vga 配置选项。
• pcie=on|off 告诉 Proxmox VE 使用 PCIe 或 PCI 端口。某些客户机/设备组合需要 PCIe 而不是 PCI。PCIe 仅适用于 q35 机器类型。
• rombar=on|off 使固件 ROM 对客户机可见。默认值为 on。某些 PCI(e) 设备需要禁用此功能。
• romfile=<path>，是设备要使用的 ROM 文件的可选路径。这是位于 /usr/share/kvm/ 下的相对路径。
示例
使用 GPU 设置为主设备的 PCIe 透传的示例：
# qm set VMID -hostpci0 02:00,pcie=on,x-vga=on
PCI ID 覆盖
您可以覆盖客户机将看到的 PCI 厂商 ID、设备 ID 和子系统 ID。如果您的设备是一个变种，具有客户端驱动程序无法识别的 
ID，但您仍希望强制加载这些驱动程序（例如，如果您知道您的设备使用与受支持变种相同的芯片组），则此功能非常有用。
可用选项包括 vendor-id、device-id、sub-vendor-id 和 sub-device-id。您可以设置这些选项中的任何一个或全部，以覆盖设备的默认 ID。
例如：
# qm set VMID -hostpci0 02:00,device-id=0x10f6,sub-vendor-id=0x0000
其他注意事项
当进行 GPU 透传时，使用 q35 作为机器类型、OVMF（用于 VM 的 EFI）而非 SeaBIOS 以及 PCIe 而非 PCI 可以获得最佳兼容性。
请注意，如果您想要为 GPU 透传使用 OVMF，GPU 需要具备 EFI 兼容 ROM，否则请使用 SeaBIOS。

10.9.3 SR-IOV
另一种透传 PCI(e) 设备的方法是使用您的设备（如果有）的硬件虚拟化功能。
SR-IOV（单根输入/输出虚拟化）使单个设备能够为系统提供多个 VF（虚拟功能）。这些 VF 中的每一个都可以在不同的 VM 
中使用，具有完整的硬件功能，并且比软件虚拟化设备具有更好的性能和更低的延迟。
目前，最常见的用例是具有 SR-IOV 支持的 NIC（网络接口卡），每个物理端口可以提供多个 VF。这允许在 VM 中使用诸如校验和卸载等功能，从而减少（主机）CPU 开销。
主机配置
通常，有两种方法可以在设备上启用虚拟功能。
    有时驱动程序模块有一个选项，例如对于一些 Intel 驱动程序：
max_vfs=4
这可以放在 /etc/modprobe.d/ 下以 .conf 结尾的文件中。（之后不要忘记更新 initramfs）
请参阅驱动程序模块文档以获取确切的参数和选项。
第二种，更通用的方法是使用 sysfs。如果设备和驱动程序支持此功能，您可以动态更改 VF 数量。例如，要在设备 0000:01:00.0 上设置 4 个 VF，请执行：
# echo 4 > /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs
要使此更改持久生效，您可以使用 sysfsutils Debian 软件。
VM 配置
创建 VF 后，使用 lspci 输出时，您应该将它们视为单独的 PCI(e) 设备。获取它们的 ID，并像正常的 PCI(e) 设备一样通过它们（第 10.9.2 节）。
其他注意事项
对于此功能，平台支持尤为重要。可能需要首先在 BIOS/EFI 中启用此功能，或者使用特定的 PCI(e) 端口使其工作。如有疑问，请查阅平台手册或联系其供应商。

10.9.4 中介设备 (vGPU, GVT-g)
中介设备是另一种将物理硬件的功能和性能用于虚拟化硬件的方法。这些设备在虚拟化 GPU 设置中最常见，例如英特尔的 GVT-g 和 NVIDIA 的 vGPU，用于其 GRID 技术。
借此，物理卡可以创建虚拟卡，类似于 SR-IOV。区别在于，中介设备不会作为 PCI(e) 设备出现在主机中，因此只适用于在虚拟机中使用。
主机配置
通常，您的卡的驱动程序必须支持该功能，否则将无法工作。因此，请参阅您的供应商以获取兼容的驱动程序以及如何配置它们。
英特尔的 GVT-g 驱动程序已集成在内核中，并且应与第 5 代、第 6 代和第 7 代英特尔 Core 处理器以及 E3 v4、E3 v5 和 E3 v6 Xeon 处理器一起使用。
要为英特尔图形启用它，您必须确保加载 kvmgt 模块（例如通过 /etc/modules），并在内核命令行（第 3.12.6 节）上启用它，添加以下参数：
i915.enable_gvt=1
此后，请记住更新 initramfs（第 10.9.1 节），然后重新启动主机。
VM 配置
要使用中介设备，只需在 hostpciX VM 配置选项上指定 mdev 属性。
您可以通过 sysfs 获取受支持的设备。例如，要列出设备 0000:00:02.0 的受支持类型，只需执行：
ls /sys/bus/pci/devices/0000:00:02.0/mdev_supported_types
每个条目都是一个目录，其中包含以下重要文件：
• available_instances 包含此类型仍可用的实例数量，每个 VM 中的 mdev 使用都会减少这个数量。
• description 包含有关类型功能的简短描述
• create 是创建此类设备的端点，如果配置了带有 mdev 的 hostpciX 选项，Proxmox VE 将自动为您执行此操作。
使用 Intel GVT-g vGPU（Intel Skylake 6700k）的示例配置：
qm set VMID -hostpci0 00:02.0,mdev=i915-GVTg_V5_4
设置此选项后，Proxmox VE 会在 VM 启动时自动创建此类设备，并在 VM 停止时清理它。

10.10 钩子脚本
您可以使用配置属性 hookscript 为 VM 添加钩子脚本。
qm set 100 --hookscript local:snippets/hookscript.pl
它将在客户机生命周期的各个阶段被调用。有关示例和文档，请参见 /usr/share/pve-docs/examples/guest-example-hookscript.pl 下的示例脚本。

10.11 休眠
您可以使用 GUI 选项 Hibernate 或
qm suspend ID --todisk
将 VM 挂起到磁盘。这意味着内存的当前内容将被保存到磁盘上，并停止 VM。在下一次启动时，将加载内存内容，VM 可以从中断处继续。
状态存储选择
如果没有给定内存的目标存储，将自动选择以下存储中的第一个：
    VM 配置中的 vmstatestorage 存储。
    任何 VM 磁盘的第一个共享存储。
    任何 VM 磁盘的第一个非共享存储。
    作为备用的 local 存储。

10.12 使用 qm 管理虚拟机
qm 是在 Proxmox VE 上管理 QEMU/KVM 虚拟机的工具。您可以创建和销毁虚拟机，并控制执行（启动/停止/挂起/恢复）。此外，您可以使用 qm 
设置关联配置文件中的参数。还可以创建和删除虚拟磁盘。

10.12.1 命令行使用示例
使用本地存储上的 iso 文件，在 local-lvm 存储上创建一个带有 4 GB IDE 磁盘的 VM
qm create 300 -ide0 local-lvm:4 -net0 e1000 -cdrom local:iso/proxmox- -
mailgateway_2.1.iso
启动新 VM
qm start 300
发送关机请求，然后等待 VM 停止。
qm shutdown 300 && qm wait 300
与上面相同，但只等待 40 秒。
qm shutdown 300 && qm wait 300 -timeout 40
销毁 VM 总是从访问控制列表中删除它，并始终删除 VM 的防火墙配置。如果您要将 VM 从复制作业、备份作业和 HA 资源配置中删除，请激活 --purge。
qm destroy 300 --purge
将磁盘映像移动到其他存储。
qm move-disk 300 scsi0 other-storage
将磁盘映像分配给其他 VM。这将从源 VM 中删除磁盘 scsi1 并将其作为 scsi3 附加到目标 VM。在后台，磁盘映像被重命名，以便名称与新所有者匹配。
qm move-disk 300 scsi1 --target-vmid 400 --target-disk scsi3

10.13 配置
VM 配置文件存储在 Proxmox 集群文件系统内，可以在 /etc/pve/qemu-server/ 中访问。与存储在 /etc/pve/ 中的其他文件一样，它们会自动复制到所有其他集群节点。
注意
VMID < 100 是为内部目的保留的，VMID 需要在整个集群范围内唯一。
示例 VM 配置
boot: order=virtio0;net0
cores: 1
sockets: 1
memory: 512
name: webmail
ostype: l26
net0: e1000=EE:D2:28:5F:B6:3E,bridge=vmbr0
virtio0: local:vm-100-disk-1,size=32G
这些配置文件是简单的文本文件，您可以使用普通的文本编辑器（如 vi、nano 等）进行编辑。这有时有助于进行小的更正，但请记住，您需要重新启动 VM 以应用此类更改。
出于这个原因，通常最好使用 qm 命令生成和修改这些文件，或者使用 GUI 完成整个操作。我们的工具包足够智能，可以立即应用对正在运行的 VM 的大多数更改。
此功能称为 "热插拔"，在这种情况下无需重新启动 VM。

10.13.1 文件格式
VM 配置文件使用简单的冒号分隔的键/值格式。每行的格式如下：
这是一条注释
OPTION: value
这些文件中的空行将被忽略，以 # 字符开头的行将被视为注释并被忽略。

10.13.2 快照
当您创建快照时，qm 将在同一配置文件内的单独快照部分中存储快照时间的配置。例如，在创建名为“testsnapshot”的快照后，您的配置文件将如下所示：
带快照的 VM 配置
memory: 512
swap: 512
parent: testsnaphot
...
[testsnaphot]
memory: 512
swap: 512
snaptime: 1457170803
...
有一些与快照相关的属性，如 parent 和 snaptime。parent 属性用于存储快照之间的父/子关系。snaptime 是快照创建时间戳（Unix 纪元）。
您可以选择使用 vmstate 选项保存正在运行的 VM 的内存。有关 VM 状态的目标存储如何选择的详细信息，请参阅休眠章节中的状态存储选择部分 10.11。

10.13.3 选项
acpi: <boolean> (默认 = 1)
启用/禁用 ACPI。
affinity: <string>
用于执行客户端进程的主机内核列表，例如：0,5,8-11
agent: [enabled=]<1|0> [,freeze-fs-on-backup=<1|0>]
[,fstrim_cloned_disks=<1|0>] [,type=<virtio|isa>]
启用/禁用与 QEMU Guest Agent 及其属性的通信。
enabled=<boolean> (默认 = 0)
启用/禁用与在 VM 中运行的 QEMU Guest Agent (QGA) 的通信。
freeze-fs-on-backup=<boolean> (默认 = 1)
在备份时冻结/解冻客户机文件系统以保持一致性。
fstrim_cloned_disks=<boolean> (默认 = 0)
在移动磁盘或迁移 VM 后运行 fstrim。
type=<isa | virtio> (默认 = virtio)
选择 agent 类型
arch: <aarch64 | x86_64>
虚拟处理器架构。默认为主机。
args: <string>
传递给 kvm 的任意参数，例如：
args: -no-reboot -no-hpet
注意
此选项仅适用于专家。
audio0: device=<ich9-intel-hda|intel-hda|AC97>
[,driver=<spice|none>]
配置音频设备，与 QXL/Spice 结合使用非常有用。
device=<AC97 | ich9-intel-hda | intel-hda>
配置音频设备。
driver=<none | spice> (默认 = spice)
音频设备的驱动程序后端。
autostart: <boolean> (默认 = 0)
崩溃后自动重启（当前被忽略）。
balloon: <integer> (0 - N)
以 MiB 为单位的 VM 目标 RAM 量。使用零禁用气球驱动。
bios: <ovmf | seabios> (默认 = seabios)
选择 BIOS 实现。
boot: [[legacy=]<[acdn]{1,4}>] [,order=<device[;device...]>]
指定客户机启动顺序。使用 order= 子属性，因为不使用键或 legacy= 已被弃用。
legacy=<[acdn]{1,4}> (默认 = cdn)
从软盘 (a)、硬盘 (c)、CD-ROM (d) 或网络 (n) 启动。已弃用，改用 order=。
order=<device[;device...]>
客户机将尝试按照这里出现的顺序从设备启动。
将直接从磁盘、光驱和传递存储 USB 设备启动，NIC 将加载 PXE，PCIe 设备将表现为磁盘（例如 NVMe）或加载选项 ROM（例如 RAID 控制器、硬件 NIC）。
请注意，只有此列表中的设备将被标记为可启动并由客户机固件（BIOS/UEFI）加载。如果需要多个磁盘进行启动（例如软件 RAID），则需要在此处指定所有磁盘。
在给定时覆盖已弃用的 legacy=[acdn]* 值。
bootdisk: (ide|sata|scsi|virtio)\d+
启用从指定磁盘启动。已弃用：改用 boot: order=foo;bar。
cdrom: <volume>
这是 -ide2 选项的别名
cicustom: [meta=<volume>] [,network=<volume>] [,user=<volume>]
[,vendor=<volume>]
cloud-init: 指定在启动时替换自动生成的文件。
meta=<volume>
指定包含通过 cloud-init 传递给 VM 的所有元数据的自定义文件。这是特定于提供商的，这意味着 configdrive2 和 nocloud 不同。
network=<volume>
传递包含所有网络数据的自定义文件通过 cloud-init 传递给 VM。
user=<volume>
传递包含所有用户数据的自定义文件通过 cloud-init 传递给 VM。
vendor=<volume>
传递包含所有供应商数据的自定义文件通过 cloud-init 传递给 VM。
cipassword: <string>
cloud-init: 分配给用户的密码。通常不建议使用这个。改用 ssh 密钥。还要注意，旧版本的 cloud-init 不支持哈希密码。
citype: <configdrive2 | nocloud | opennebula>
指定 cloud-init 配置格式。默认值取决于配置的操作系统类型（ostype）。对于 Linux，我们使用 nocloud 格式，对于 Windows，我们使用 configdrive2。
ciuser: <string>
cloud-init：要更改 ssh 密钥和密码的用户名，而不是映像配置的默认用户。
cores: <integer> (1 - N) (默认 = 1)
每个插槽的核心数。
cpu：[[cputype=]<string>] [,flags=<+FLAG[;-FLAG...]>]
[,hidden=<1|0>] [,hv-vendor-id=<vendor-id>]
[,phys-bits=<8-64|host>] [,reported-model=<enum>]
模拟的 CPU 类型。
cputype=<string> (默认 = kvm64)
模拟的 CPU 类型。可以是默认或自定义名称（自定义型号名称必须以 custom- 为前缀）。
flags=<+FLAG[;-FLAG...]>
由 ; 分隔的附加 CPU 标志列表。使用 +FLAG 启用，-FLAG 禁用标志。
自定义 CPU 型号可以指定 QEMU/KVM 支持的任何标志，出于安全原因，VM 特定标志必须来自以下集合：
pcid, spec-ctrl, ibpb, ssbd, virt-ssbd, amd-ssbd, amd-no-ssb, pdpe1gb, md-clear, hv-tlbflush, hv-evmcs, aes
hidden=<boolean> (默认 = 0)
不将其标识为 KVM 虚拟机。
hv-vendor-id=<vendor-id>
Hyper-V 供应商 ID。Windows 客户机中的一些驱动程序或程序需要特定的 ID。
phys-bits=<8-64|host>
报告给客户操作系统的物理内存地址位。应小于或等于主机的值。设置为 host 以使用来自主机 CPU 的值，但请注意，这样做将破坏对具有其他值的 CPU 的实时迁移。
reported-model=<486 | Broadwell | Broadwell-IBRS |
Broadwell-noTSX | Broadwell-noTSX-IBRS | Cascadelake-Server |
Cascadelake-Server-noTSX | Conroe | EPYC | EPYC-IBPB |
EPYC-Milan | EPYC-Rome | Haswell | Haswell-IBRS | Haswell-noTSX
| Haswell-noTSX-IBRS | Icelake-Client | Icelake-Client-noTSX |
Icelake-Server | Icelake-Server-noTSX | IvyBridge |
IvyBridge-IBRS | KnightsMill | Nehalem | Nehalem-IBRS |
Opteron_G1 | Opteron_G2 | Opteron_G3 | Opteron_G4 | Opteron_G5
| Penryn | SandyBridge | SandyBridge-IBRS | Skylake-Client |
Skylake-Client-IBRS | Skylake-Client-noTSX-IBRS |
Skylake-Server | Skylake-Server-IBRS |
Skylake-Server-noTSX-IBRS | Westmere | Westmere-IBRS | athlon |
core2duo | coreduo | host | kvm32 | kvm64 | max | pentium |
pentium2 | pentium3 | phenom | qemu32 | qemu64> (默认 = kvm64)
报告给客户机的 CPU 型号和供应商。必须是 QEMU/KVM 支持的型号。只
对于自定义 CPU 型号定义有效，缺省型号将始终向客户操作系统报告自身。
cpulimit: <number> (0 - 128) (默认 = 0)
CPU 使用限制。
注意
如果计算机有 2 个 CPU，那么它总共有 2 个 CPU 时间。值 0 表示没有 CPU 限制。
cpuunits: <integer> (1 - 262144) (默认 = cgroup v1: 1024, cgroup v2: 100)
VM 的 CPU 权重。参数用于内核公平调度器。数字越大，这个 VM 获得的 CPU 时间越多。数字与所有其他正在运行的 VM 的权重相关。
description: <string>
VM 的描述。显示在网络接口 VM 的摘要中。这将作为配置文件内的注释保存。
efidisk0：[file=]<volume> [,efitype=<2m|4m>] [,format=<enum>]
[,pre-enrolled-keys=<1|0>] [,size=<DiskSize>]
配置用于存储 EFI 变量的磁盘。
efitype=<2m | 4m> (默认 = 2m)
OVMF EFI 变量的大小和类型。4m 较新，建议使用，并且需要安全启动。为了向后兼容，如果没有另外指定，则使用 2m。对于具有 arch=aarc64 (ARM) 的 VM 会被忽略。
file=<volume>
驱动器的背景卷。
format=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>
驱动器背景文件的数据格式。
pre-enrolled-keys=<boolean> (默认 = 0)
如果与 efitype=4m 一起使用，则使用具有分发特定和已注册的 Microsoft 标准密钥的 EFI 变量模板。请注意，这将默认启用安全启动，尽管仍然可以从 VM 内部关闭。
size=<DiskSize>
磁盘大小。这仅仅是信息性的，没有影响。
freeze: <boolean>
在启动时冻结 CPU（使用 c 监视器命令开始执行）。
hookscript: <string>
在虚拟机生命周期的各个阶段执行的脚本。对于自定义 CPU 型号定义有效，缺省型号将始终向客户操作系统报告自身。
cpulimit: <number> (0 - 128) (默认 = 0)
CPU 使用限制。
注意
如果计算机有 2 个 CPU，那么它总共有 2 个 CPU 时间。值 0 表示没有 CPU 限制。
cpuunits: <integer> (1 - 262144) (默认 = cgroup v1: 1024, cgroup v2: 100)
VM 的 CPU 权重。参数用于内核公平调度器。数字越大，这个 VM 获得的 CPU 时间越多。数字与所有其他正在运行的 VM 的权重相关。
description: <string>
VM 的描述。显示在网络接口 VM 的摘要中。这将作为配置文件内的注释保存。
efidisk0：[file=]<volume> [,efitype=<2m|4m>] [,format=<enum>]
[,pre-enrolled-keys=<1|0>] [,size=<DiskSize>]
配置用于存储 EFI 变量的磁盘。
efitype=<2m | 4m> (默认 = 2m)
OVMF EFI 变量的大小和类型。4m 较新，建议使用，并且需要安全启动。为了向后兼容，如果没有另外指定，则使用 2m。对于具有 arch=aarc64 (ARM) 的 VM 会被忽略。
file=<volume>
驱动器的背景卷。
format=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>
驱动器背景文件的数据格式。
pre-enrolled-keys=<boolean> (默认 = 0)
如果与 efitype=4m 一起使用，则使用具有分发特定和已注册的 Microsoft 标准密钥的 EFI 变量模板。请注意，这将默认启用安全启动，尽管仍然可以从 VM 内部关闭。
size=<DiskSize>
磁盘大小。这仅仅是信息性的，没有影响。
freeze: <boolean>
在启动时冻结 CPU（使用 c 监视器命令开始执行）。
hookscript: <string>
在虚拟机生命周期的各个阶段执行的脚本。
hostpci[n]: [host=]<HOSTPCIID[;HOSTPCIID2...]> [,device-id=<hex id>]
[,legacy-igd=<1|0>] [,mdev=<string>] [,pcie=<1|0>] [,rombar=<1|0>]
[,romfile=<string>] [,sub-device-id=<hex id>] [,sub-vendor-id=<hex
id>] [,vendor-id=<hex id>] [,x-vga=<1|0>]
将主机 PCI 设备映射到客户机。
注意
此选项允许直接访问主机硬件。因此，不再可能迁移这样的机器 - 请特别小心使用。
警告
实验性！用户报告了此选项的问题。
device-id=<hex id>
覆盖对客户机可见的 PCI 设备 ID
host=<HOSTPCIID[;HOSTPCIID2...]>
主机 PCI 设备直通。主机的 PCI 设备的 PCI ID 或主机的 PCI 虚拟函数列表。HOSTPCIID 语法为：
bus:dev.func（十六进制数字）
您可以使用 lspci 命令列出现有的 PCI 设备。
legacy-igd=<boolean> (默认 = 0)
以传统 IGD 模式传递此设备，使其成为 VM 中的主要且独占的图形设备。需要 pc-i440fx 机器类型和将 VGA 设置为 none。
mdev=<string>
要使用的中介设备类型。在 VM 启动时将创建此类型的实例，并在 VM 停止时清理。
pcie=<boolean> (默认 = 0)
选择 PCI-express 总线（需要 q35 机器模型）。
rombar=<boolean> (默认 = 1)
指定设备的 ROM 是否在客户机的内存映射中可见。
romfile=<string>
自定义 pci 设备 rom 文件名（必须位于 /usr/share/kvm/ 中）。
sub-device-id=<hex id>
覆盖对客户机可见的 PCI 子系统设备 ID
sub-vendor-id=<hex id>
覆盖对客户机可见的 PCI 子系统厂商 ID
vendor-id=<hex id>
覆盖对客户机可见的 PCI 厂商 ID
x-vga=<boolean> (默认 = 0)
启用 vfio-vga 设备支持。
hotplug: <string> (默认 = network,disk,usb)
有选择地启用热插拔功能。这是一个用逗号分隔的热插拔功能列表：网络，磁盘，CPU，内存，USB 和 cloudinit。使用 0 完全禁用热插拔。
将 1 用作值是默认网络，磁盘，USB 的别名。对于机器版本 >= 7.1 且 ostype 为 l26 或 windows > 7 的客户端，可以进行 USB 热插拔。
hugepages: <1024 | 2 | any>
启用/禁用大页面内存。
ide[n]: [file=]<volume> [,aio=<native|threads|io_uring>]
[,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>]
[,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>]
[,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>]
[,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>]
[,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>]
[,iops_max_length=<seconds>] [,iops_rd=<iops>]
[,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>]
[,iops_wr=<iops>] [,iops_wr_max=<iops>]
[,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>]
[,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>]
[,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,model=<model>]
[,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>]
[,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>]
[,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>]
[,werror=<enum>] [,wwn=<wwn>]
将卷用作 IDE 硬盘或 CD-ROM（n 为 0 到 3）。
aio=<io_uring | native | threads>
要使用的 AIO 类型。
backup=<boolean>
在进行备份时是否应包含该驱动器。
bps=<bps>
每秒字节的最大读/写速度。
bps_max_length=<seconds>
以秒为单位的 I/O 突发的最大长度。
bps_rd=<bps>
每秒字节的最大读取速度。
bps_rd_max_length=<seconds>
以秒为单位的读取 I/O 突发的最大长度。
bps_wr=<bps>
每秒字节的最大写入速度。
bps_wr_max_length=<seconds>
以秒为单位的写入 I/O 突发的最大长度。
cache=<directsync | none | unsafe | writeback | writethrough>
驱动器的缓存模式
cyls=<integer>
强制驱动器的物理几何具有特定的柱面数。
detect_zeroes=<boolean>
控制是否检测并尝试优化零的写入。
discard=<ignore | on>
控制是否将丢弃/修剪请求传递给底层存储。
file=<volume>
驱动器的后备卷。
format=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>
驱动器的后备文件的数据格式。
heads=<integer>
强制驱动器的物理几何具有特定的磁头数。
iops=<iops>
每秒操作的最大读/写 I/O。
iops_max=<iops>
每秒操作的最大无限制读/写 I/O 池。
iops_max_length=<seconds>
以秒为单位的 I/O 突发的最大长度。
iops_rd=<iops>
每秒操作的最大读取 I/O。
iops_rd_max=<iops>
每秒操作的最大无限制读取 I/O 池。
iops_rd_max_length=<seconds>
以秒为单位的读取 I/O 突发的最大长度。
iops_wr=<iops>
每秒操作的最大写入 I/O。
iops_wr_max=<iops>
每秒操作的最大无限制写入 I/O 池。
iops_wr_max_length=<seconds>
以秒为单位的写入 I/O 突发的最大长度。
mbps=<mbps>
每秒兆字节的最大读/写速度。
mbps_max=<mbps>
每秒兆字节的最大无限制读/写池。
mbps_rd=<mbps>
每秒兆字节的最大读取速度。
mbps_rd_max=<mbps>
每秒兆字节的最大无限制读取池。
mbps_wr=<mbps>
每秒兆字节的最大写入速度。
mbps_wr_max=<mbps>
每秒兆字节的最大无限制写入池。
media=<cdrom | disk> (默认 = disk)
驱动器的媒体类型。
model=<model>
驱动器报告的型号名称，url-encoded，最长40个字节。
replicate=<boolean> (默认 = 1)
驱动器是否应考虑用于复制任务。
rerror=<ignore | report | stop>
读取错误操作。
secs=<integer>
强制驱动器的物理几何具有特定的扇区数。
serial=<serial>
驱动器报告的序列号，url-encoded，最长20个字节。
shared=<boolean> (默认 = 0)
将此本地管理的卷标记为所有节点上可用。
警告
此选项不会自动共享卷，它假定卷已经共享！
size=<DiskSize>
磁盘大小。这纯粹是信息性的，没有任何影响。
snapshot=<boolean>
控制 qemu 的快照模式功能。如果激活，对磁盘所做的更改将是临时的，并且在虚拟机关闭时将被丢弃。
ssd=<boolean>
是否将此驱动器暴露为SSD，而不是旋转硬盘。
trans=<auto | lba | none>
强制磁盘几何 BIOS 转换模式。
werror=<enospc | ignore | report | stop>
写入错误操作。
wwn=<wwn>
驱动器的全球唯一名称，以16字节十六进制字符串编码，前缀为 0x。
ipconfig[n]: [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>]
[,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]
cloud-init：为相应接口指定IP地址和网关。
IP地址使用CIDR表示法，网关是可选的，但需要指定相同类型的IP。特殊字符串 dhcp 可用于使用 DHCP 的 IP 地址，在这种情况下不应提供显式网关。
对于 IPv6，特殊字符串 auto 可用于使用无状态自动配置。这需要 cloud-init 19.4 或更新版本。
如果启用了 cloud-init 且未指定 IPv4 或 IPv6 地址，则默认在 IPv4 上使用 dhcp。
gw=<GatewayIPv4>
IPv4 流量的默认网关。
注意
需要选项：ip
gw6=<GatewayIPv6>
IPv6 流量的默认网关。
注意
需要选项：ip6
ip=<IPv4Format/CIDR> (默认 = dhcp)
CIDR 格式的 IPv4 地址。
ip6=<IPv6Format/CIDR> (默认 = dhcp)
CIDR 格式的 IPv6 地址。
ivshmem：size=<integer> [,name=<string>]
虚拟机间共享内存。对于虚拟机之间或虚拟机与主机之间的直接通信非常有用。
name=<string>
文件的名称。将使用 pve-shm- 作为前缀。默认为 VMID。在虚拟机停止时将被删除。
size=<integer> (1 - N)
文件大小（以 MB 为单位）。
keephugepages：<boolean>（默认值 = 0）
与 hugepages 一起使用。如果启用，hugepages 将在虚拟机关闭后不会被删除，并可在后续启动中使用。
keyboard：<da | de | de-ch | en-gb | en-us | es | fi | fr | fr-be |
fr-ca | fr-ch | hu | is | it | ja | lt | mk | nl | no | pl | pt |
pt-br | sl | sv | tr>
VNC 服务器的键盘布局。这个选项通常不是必需的，通常最好在客户操作系统内处理。
kvm：<boolean>（默认值 = 1）
启用/禁用 KVM 硬件虚拟化。
localtime：<boolean>
将实时时钟（RTC）设置为本地时间。如果 ostype 指示 Microsoft Windows 操作系统，则默认启用此选项。
lock：<backup | clone | create | migrate | rollback | snapshot |
snapshot-delete | suspended | suspending>
锁定/解锁虚拟机。
machine：
(pc|pc(-i440fx)?-\d+(.\d+)+(+pve\d+)?(.pxe)?|q35|pc-q35-\d+(.\d+)+(+pve\指定 QEMU 机器类型。
memory：<integer>（16 - N）（默认值 = 512）
以 MiB 为单位的虚拟机 RAM。当使用气球设备时，这是最大可用内存。
migrate_downtime：<number>（0 - N）（默认值 = 0.1）
设置迁移过程中允许的最大停机时间（以秒为单位）。
migrate_speed：<integer>（0 - N）（默认值 = 0）
设置迁移的最大速度（以 MB/s 为单位）。值 0 表示无限制。
name：<string>
为虚拟机设置一个名称。仅在配置 Web 界面上使用。
nameserver：<string>
cloud-init：为容器设置 DNS 服务器 IP 地址。如果未设置 searchdomain 或 nameserver，创建操作将自动使用主机的设置。
net[n]：[model=]<enum> [,bridge=<bridge>] [,firewall=<1|0>]
[,link_down=<1|0>] [,macaddr=XX:XX:XX:XX:XX:XX] [,mtu=<integer>]
[,queues=<integer>] [,rate=<number>] [,tag=<integer>]
[,trunks=<vlanid[;vlanid...]>] [,<model>=<macaddr>]
指定网络设备。
bridge=<bridge>
要将网络设备连接到的网桥。Proxmox VE 标准网桥称为 vmbr0。
如果您不指定网桥，我们将创建一个 kvm 用户（NATed）网络设备，提供
DHCP 和 DNS 服务。使用以下地址：
10.0.2.2 网关
10.0.2.3 DNS 服务器
10.0.2.4 SMB 服务器
DHCP 服务器从 10.0.2.15 开始为客户机分配地址。
firewall=<boolean>
此接口是否应受防火墙保护。
link_down=<boolean>
此接口是否应断开连接（如拔掉插头）。
macaddr=XX:XX:XX:XX:XX:XX
具有未设置的 I/G（个人/组）位的通用 MAC 地址。
model=<e1000 | e1000-82540em | e1000-82544gc | e1000-82545em |
e1000e | i82551 | i82557b | i82559er | ne2k_isa | ne2k_pci |
pcnet | rtl8139 | virtio | vmxnet3>
网络卡型号。virtio 型号在非常低的 CPU 开销下提供最佳性能。
如果您的客户机不支持此驱动程序，通常最好使用 e1000。
mtu=<integer>（1 - 65520）
仅对 VirtIO 强制 MTU。设置为 1 以使用网桥 MTU
queues=<integer>（0 - 64）
在设备上使用的数据包队列数量。
rate=<number>（0 - N）
浮点数表示的速率限制（以每秒兆字节为单位）。
tag=<integer>（1 - 4094）
在此接口上应用于数据包的 VLAN 标签。
trunks=<vlanid[;vlanid...]>
通过此接口传递的 VLAN 中继。
numa：<boolean>（默认值 = 0）
启用/禁用 NUMA。
numa[n]：cpus=<id[-id];...> [,hostnodes=<id[-id];...>]
[,memory=<number>] [,policy=<preferred|bind|interleave>]
NUMA 顶点。
cpus=<id[-id];...>
访问此 NUMA 节点的 CPU。
hostnodes=<id[-id];...>
要使用的主机 NUMA 节点。
memory=<number>
此 NUMA 节点提供的内存量。
policy=<bind | interleave | preferred>
NUMA 分配策略。
onboot：<boolean>（默认值 = 0）
指定在系统启动时是否启动 VM。
ostype：<l24 | l26 | other | solaris | w2k | w2k3 | w2k8 | win10 |
win11 | win7 | win8 | wvista | wxp>
指定客户操作系统。这用于为特定操作启用特殊优化/功能
系统：
other 未指定的操作系统
wxp 微软 Windows XP
w2k 微软 Windows 2000
w2k3 微软 Windows 2003
w2k8 微软 Windows 2008
wvista 微软 Windows Vista
win7 微软 Windows 7
win8 微软 Windows 8/2012/2012r2
win10 微软 Windows 10/2016/2019
win11 微软 Windows 11/2022
l24 Linux 2.4 内核
l26 Linux 2.6 - 6.X 内核
solaris Solaris/OpenSolaris/OpenIndiania内核
parallel[n]：/dev/parport\d+|/dev/usb/lp\d+
映射主机并行设备（n 为 0 到 2）。
注意
此选项允许直接访问主机硬件。因此，无法再迁移这样的机器 - 请特别小心使用。
警告
实验性！用户报告了此选项的问题。
protection：<boolean>（默认值 = 0）
设置 VM 的保护标志。这将禁用删除 VM 和删除磁盘操作。
reboot：<boolean>（默认值 = 1）
允许重新启动。如果设置为0，则 VM 在重新启动时退出。
rng0：[source=]</dev/urandom|/dev/random|/dev/hwrng>
[,max_bytes=<integer>] [,period=<integer>]
配置基于 VirtIO 的随机数生成器。
max_bytes=<integer>（默认值 = 1024）
每隔 period 毫秒允许注入到客户端的最大熵字节数。当使用 /dev/random 作为源时，选择较低的值。使用 0 禁用限制（可能危险！）。
period=<integer>（默认值 = 1000）
每隔 period 毫秒，熵注入配额重置，允许客户端检索另一个最大熵字节。
source=</dev/hwrng | /dev/random | /dev/urandom>
主机上收集熵的文件。在大多数情况下，/dev/urandom 应优先于 /dev/random 以避免主机上的熵饥饿问题。使用 urandom 
不会以任何有意义的方式降低安全性，因为它仍然由真正的熵提供种子，并且所提供的字节很可能也会与客户端的真正熵混合。
/dev/hwrng 可用于从主机传递硬件 RNG。
sata[n]: [file=]<volume> [,aio=<native|threads|io_uring>]
[,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>]
[,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>]
[,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>]
[,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>]
[,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>]
[,iops_max_length=<seconds>] [,iops_rd=<iops>]
[,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>]
[,iops_wr=<iops>] [,iops_wr_max=<iops>]
[,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>]
[,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>]
[,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>]
[,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>]
[,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>]
[,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]
将卷用作 SATA 硬盘或 CD-ROM（n 为 0 到 5）。
aio=<io_uring | native | threads>
要使用的 AIO 类型。
backup=<boolean>
制作备份时是否应包含该驱动器。
bps=<bps>
以字节/秒为单位的最大读/写速度。
bps_max_length=<seconds>
I/O 爆发的最大长度（以秒为单位）。
bps_rd=<bps>
以字节/秒为单位的最大读取速度。
bps_rd_max_length=<seconds>
读取 I/O 爆发的最大长度（以秒为单位）。
bps_wr=<bps>
以字节/秒为单位的最大写入速度。
bps_wr_max_length=<seconds>
写入 I/O 爆发的最大长度（以秒为单位）。
cache=<directsync | none | unsafe | writeback | writethrough>
驱动器的缓存模式。
cyls=<integer>
强制驱动器的物理几何形状具有特定的柱面计数。
detect_zeroes=<boolean>
控制是否检测并尝试优化零的写入。
discard=<ignore | on>
控制是否将丢弃/修剪请求传递到底层存储。
file=<volume>
驱动器的支持卷。
format=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>
驱动器支持文件的数据格式。
heads=<integer>
强制驱动器的物理几何形状具有特定的磁头数量。
iops=<iops>
每秒最大读/写 I/O 操作数。
iops_max=<iops>
每秒最大未限制的读/写 I/O 池操作数。
iops_max_length=<seconds>
I/O 爆发的最大长度（以秒为单位）。
iops_rd=<iops>
每秒最大读取 I/O 操作数。
iops_rd_max=<iops>
每秒最大未限制的读取 I/O 池操作数。
iops_rd_max_length=<seconds>
读取 I/O 爆发的最大长度（以秒为单位）。
iops_wr=<iops>
每秒最大写入 I/O 操作数。
iops_wr_max=<iops>
每秒最大未限制的写入 I/O 池操作数。
iops_wr_max_length=<seconds>
写入 I/O 爆发的最大长度（以秒为单位）。
mbps=<mbps>
以兆字节/秒为单位的最大读/写速度。
mbps_max=<mbps>
以兆字节/秒为单位的最大未限制的读/写池。
mbps_rd=<mbps>
以兆字节/秒为单位的最大读取速度。
mbps_rd_max=<mbps>
以兆字节/秒为单位的最大未限制的读取池。
mbps_wr=<mbps>
以兆字节/秒为单位的最大写入速度。
mbps_wr_max=<mbps>
每秒兆字节的最大未限制写入池。
media=<cdrom | disk> (default = disk)
驱动器的介质类型。
replicate=<boolean> (default = 1)
驱动器是否应考虑用于复制作业。
rerror=<ignore | report | stop>
读取错误操作。
secs=<integer>
强制驱动器的物理几何形状具有特定的扇区数量。
serial=<serial>
驱动器报告的序列号，url-encoded，最长20字节。
shared=<boolean> (default = 0)
将此本地管理的卷标记为所有节点上可用。
警告
此选项不会自动共享卷，而是假定卷已经共享！
size=<DiskSize>
磁盘大小。这纯粹是信息性的，没有任何影响。
snapshot=<boolean>
控制 qemu 的快照模式功能。如果激活，对磁盘所做的更改是临时的，并且在虚拟机关闭时将被丢弃。
ssd=<boolean>
是否将此驱动器暴露为固态硬盘，而不是旋转硬盘。
trans=<auto | lba | none>
强制磁盘几何 BIOS 转换模式。
werror=<enospc | ignore | report | stop>
写入错误操作。
wwn=<wwn>
驱动器的全球名称，编码为16字节十六进制字符串，前缀为0x。
scsi[n]: [file=]<volume> [,aio=<native|threads|io_uring>]
[,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>]
[,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>]
[,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>]
[,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>]
[,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>]
[,iops_max_length=<seconds>] [,iops_rd=<iops>]
[,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>]
[,iops_wr=<iops>] [,iops_wr_max=<iops>]
[,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>]
[,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>]
[,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>]
[,queues=<integer>] [,replicate=<1|0>]
[,rerror=<ignore|report|stop>] [,ro=<1|0>] [,scsiblock=<1|0>]
[,secs=<integer>] [,serial=<serial>] [,shared=<1|0>]
[,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>]
[,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]
将卷用作 SCSI 硬盘或 CD-ROM（n为 0 到 30）。
aio=<io_uring | native | threads>
要使用的 AIO 类型。
backup=<boolean>
制作备份时是否应包含驱动器。
bps=<bps>
每秒字节的最大读/写速度。
bps_max_length=<seconds>
以秒为单位的 I/O 突发的最大长度。
bps_rd=<bps>
每秒字节的最大读取速度。
bps_rd_max_length=<seconds>
以秒为单位的读取 I/O 突发的最大长度。
bps_wr=<bps>
每秒字节的最大写入速度。
bps_wr_max_length=<seconds>
以秒为单位的写入 I/O 突发的最大长度。
cache=<directsync | none | unsafe | writeback | writethrough>
驱动器的缓存模式
cyls=<integer>
强制驱动器的物理几何形状具有特定的柱面计数。
detect_zeroes=<boolean>
控制是否检测并尝试优化零写入。
discard=<ignore | on>
控制是否将丢弃/修剪请求传递给底层存储。
file=<volume>
驱动器的支持卷。
format=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>
驱动器支持文件的数据格式。
heads=<integer>
强制驱动器的物理几何形状具有特定的磁头数量。
iops=<iops>
每秒最大读/写 I/O 操作数。
iops_max=<iops>
每秒最大未限制的读/写 I/O 操作池。
iops_max_length=<seconds>
以秒为单位的 I/O 突发的最大长度。
iops_rd=<iops>
每秒最大读取 I/O 操作数。
iops_rd_max=<iops>
每秒最大未限制的读取 I/O 操作池。
iops_rd_max_length=<seconds>
以秒为单位的读取 I/O 突发的最大长度。
iops_wr=<iops>
每秒最大写入 I/O 操作数。
iops_wr_max=<iops>
每秒最大未限制的写入 I/O 操作池。
iops_wr_max_length=<seconds>
以秒为单位的写入 I/O 突发的最大长度。
iothread=<boolean>
是否为此驱动器使用 iothreads
mbps=<mbps>
每秒最大读/写速度，以兆字节为单位。
mbps_max=<mbps>
以兆字节每秒为单位的最大未限制读/写速度池。
mbps_rd=<mbps>
以兆字节每秒为单位的最大读取速度。
mbps_rd_max=<mbps>
以兆字节每秒为单位的最大未限制读取速度池。
mbps_wr=<mbps>
每秒最大写入速度，以兆字节为单位。
mbps_wr_max=<mbps>
每秒最大未限制写入速度池，以兆字节为单位。
media=<cdrom | disk> (default = disk)
驱动器的媒体类型。
queues=<integer> (2 - N)
队列数量。
replicate=<boolean> (default = 1)
驱动器是否应考虑用于复制作业。
rerror=<ignore | report | stop>
读取错误操作。
ro=<boolean>
驱动器是否为只读。
scsiblock=<boolean> (default = 0)
是否使用 scsi-block 完全透传主机块设备
警告
与低内存或主机上的高内存碎片化结合可能导致 I/O 错误
secs=<integer>
强制驱动器的物理几何形状具有特定的扇区数量。
serial=<serial>
驱动器报告的序列号，URL 编码，最长 20 个字节。
shared=<boolean> (default = 0)
将此本地管理卷标记为在所有节点上可用。
警告
此选项不会自动共享卷，而是假设卷已经共享！
size=<DiskSize>
磁盘大小。这纯粹是信息性的，没有任何效果。
snapshot=<boolean>
控制 qemu 的快照模式功能。如果激活，对磁盘所做的更改将是临时的，并在虚拟机关闭时丢弃。
ssd=<boolean>
是否将此驱动器暴露为 SSD，而不是旋转硬盘。
trans=<auto | lba | none>
强制磁盘几何 BIOS 翻译模式。
werror=<enospc | ignore | report | stop>
写入错误操作。
wwn=<wwn>
驱动器的全球名称，编码为 16 字节十六进制字符串，前缀为 0x。
scsihw: <lsi | lsi53c810 | megasas | pvscsi | virtio-scsi-pci |
virtio-scsi-single> (default = lsi)
SCSI 控制器型号
searchdomain: <string>
cloud-init：为容器设置 DNS 搜索域。如果没有设置 searchdomain 或 nameserver，创建将自动使用主机的设置。
serial[n]: (/dev/.+|socket)
在虚拟机内创建一个串行设备（n 为 0 到 3），并通过主机串行设备（即 /dev/ttyS0）或在主机侧创建一个 Unix 套接字（使用 qm 终端打开一个终端连接）。
注意
如果您通过主机串行设备，将无法再迁移这样的机器 - 请谨慎使用。
警告
实验性！用户报告了此选项的问题。
shares: <integer> (0 - 50000) (default = 1000)
自动调整内存的内存份额。数字越大，此虚拟机获得的内存越多。数字与所有其他正在运行的虚拟机的权重相关。使用零禁用自动调整内存。由 pvestatd 完成自动调整内存。
smbios1: [base64=<1|0>] [,family=<Base64 编码字符串>]
[,manufacturer=<Base64 编码字符串>] [,product=<Base64 编码
字符串>] [,serial=<Base64 编码字符串>] [,sku=<Base64 编码
字符串>] [,uuid=<UUID>] [,version=<Base64 编码字符串>]
指定 SMBIOS 类型 1 字段。
base64=<boolean>
标志，表示 SMBIOS 值是 base64 编码的
family=<Base64 编码字符串>
设置 SMBIOS1 系列字符串。
manufacturer=<Base64 编码字符串>
设置 SMBIOS1 制造商。
product=<Base64 编码字符串>
设置 SMBIOS1 产品 ID。
serial=<Base64 编码字符串>
设置 SMBIOS1 序列号。
sku=<Base64 编码字符串>
设置 SMBIOS1 SKU 字符串。
uuid=<UUID>
设置 SMBIOS1 UUID。
version=<Base64 编码字符串>
设置 SMBIOS1 版本。
smp: <integer> (1 - N) (default = 1)
CPU 数量。请改用 -sockets 选项。
sockets: <integer> (1 - N) (default = 1)
CPU 插槽数量。
spice_enhancements: [foldersharing=<1|0>]
[,videostreaming=<off|all|filter>]
为 SPICE 配置额外增强功能。
foldersharing=<boolean> (default = 0)
通过 SPICE 启用文件夹共享。VM 中需要安装 Spice-WebDAV 守护程序。
videostreaming=<all | filter | off> (default = off)
启用视频流。对检测到的视频流使用压缩。
sshkeys: <string>
cloud-init：设置公共 SSH 密钥（每行一个密钥，OpenSSH 格式）。
startdate: (now | YYYY-MM-DD | YYYY-MM-DDTHH:MM:SS) (default = now)
设置实时时钟的初始日期。有效的日期格式为：'now' 或 2006-06-17T16:01:21 或 2006-06-17。
startup: [[order=]\d+] [,up=\d+] [,down=\d+] 启动和关闭行为。顺序是一个非负数，定义了一般的启动顺序。关闭按相反的顺序进行。
另外，您可以设置 up 或 down 延迟（以秒为单位），这会在启动或停止下一个虚拟机之前等待指定的延迟。
tablet: <boolean> (default = 1)
启用/禁用 USB 平板设备。通常需要此设备才能在 VNC 上允许绝对鼠标定位。否则，鼠标与普通 VNC 
客户端脱节。如果您在一台主机上运行很多仅控制台的客户机，您可以考虑禁用此功能以节省一些上下文切换。
如果您使用 spice（qm set <vmid> --vga qxl），默认情况下此功能将关闭。
tags: <string>
虚拟机的标签。这只是元信息。
tdf: <boolean> (default = 0)
启用/禁用时间漂移修复。
template: <boolean> (default = 0)
启用/禁用模板。
tpmstate0: [file=]<volume> [,size=<DiskSize>] [,version=<v1.2|v2.0>]
配置用于存储 TPM 状态的磁盘。格式固定为 raw。
file=<volume>
驱动器的后备卷。
size=<DiskSize>
磁盘大小。这纯粹是信息性的，没有影响。
version=<v1.2 | v2.0> (default = v2.0)
TPM 接口版本。v2.0 较新，应优先选择。请注意，此选项稍后无法更改。
unused[n]: [file=]<volume>
引用未使用的卷。这是内部使用的，不应手动修改。
file=<volume>
驱动器的后备卷。
usb[n]: [host=]<HOSTUSBDEVICE|spice> [,usb3=<1|0>]
配置 USB 设备（n 为 0 到 4，对于机器版本 >= 7.1 且 ostype 为 l26 或 windows > 7，n 可以达到 14）。
host=<HOSTUSBDEVICE|spice>
主机 USB 设备或端口或值 spice。HOSTUSBDEVICE 语法是：
'bus-port(.port)*'（十进制数）或
'vendor_id:product_id'（十六进制数）或
'spice'
您可以使用 lsusb -t 命令列出现有的 USB 设备。
注意
此选项允许直接访问主机硬件。因此，不再可能迁移这样的机器 - 请特别小心使用。
值 spice 可用于为 spice 添加 USB 重定向设备。
usb3=<boolean> (default = 0)
指定给定主机选项是否为 USB3 设备或端口。对于现代客户端（机器版本 >= 7.1，ostype 为 l26，以及 Windows 版本大于 7），
此标志无关紧要（所有设备都插入到 xhci 控制器中）。
vcpus: <integer> (1 - N) (default = 0)
热插拔 vCPU 的数量。
vga: [[type=]<enum>] [,memory=<integer>]
配置 VGA 硬件。如果要使用高分辨率模式（>= 1280x1024x16），您可能需要增加 vga 内存选项。自 QEMU 2.9 起，
除某些 Windows 版本（XP 及更早版本）使用 cirrus 外，所有操作系统类型的默认 VGA 显示类型均为 std。qxl 选项启用 SPICE 显示服务器。对于 win* 
操作系统，您可以选择要使用的独立显示器数量，Linux 客户机可以自行添加显示器。您还可以在没有任何显卡的情况下运行，使用串行设备作为终端。
memory=<integer> (4 - 512)
设置 VGA 内存（以 MiB 为单位）。对串行显示无效。
type=<cirrus | none | qxl | qxl2 | qxl3 | qxl4 | serial0 |
serial1 | serial2 | serial3 | std | virtio | virtio-gl |
vmware> (default = std)
选择 VGA 类型。
virtio[n]: [file=]<volume> [,aio=<native|threads|io_uring>]
[,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>]
[,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>]
[,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>]
[,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>]
[,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>]
[,iops_max_length=<seconds>] [,iops_rd=<iops>]
[,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>]
[,iops_wr=<iops>] [,iops_wr_max=<iops>]
[,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>]
[,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>]
[,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>]
[,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>]
[,secs=<integer>] [,serial=<serial>] [,shared=<1|0>]
[,size=<DiskSize>] [,snapshot=<1|0>] [,trans=<none|lba|auto>]
[,werror=<enum>]
将卷用作 VIRTIO 硬盘（n 为 0 到 15）。
Proxmox VE 管理指南 243 / 534
aio=<io_uring | native | threads>
要使用的 AIO 类型。
backup=<boolean>
在进行备份时是否应包含该驱动器。
bps=<bps>
以每秒字节数为单位的最大读/写速度。
bps_max_length=<seconds>
以秒为单位的 I/O 爆发的最大长度。
bps_rd=<bps>
以每秒字节数为单位的最大读取速度。
bps_rd_max_length=<seconds>
以秒为单位的读取 I/O 爆发的最大长度。
bps_wr=<bps>
以每秒字节数为单位的最大写入速度。
bps_wr_max_length=<seconds>
以秒为单位的写入 I/O 爆发的最大长度。
cache=<directsync | none | unsafe | writeback | writethrough>
驱动器的缓存模式
cyls=<integer>
强制驱动器的物理几何结构具有特定的柱面计数。
detect_zeroes=<boolean>
控制是否检测并尝试优化零写入。
discard=<ignore | on>
控制是否将丢弃/修剪请求传递给底层存储。
file=<volume>
驱动器的支持卷。
format=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>
驱动器支持文件的数据格式。
heads=<integer>
强制驱动器的物理几何结构具有特定的磁头计数。
iops=<iops>
每秒最大读/写 I/O 操作数。
iops_max=<iops>
以每秒操作数为单位的最大无节制读/写 I/O 池。
iops_max_length=<seconds>
以秒为单位的 I/O 爆发的最大长度。
iops_rd=<iops>
每秒最大读取 I/O 操作数。
iops_rd_max=<iops>
每秒最大无节制读取 I/O 池操作数。
iops_rd_max_length=<seconds>
以秒为单位的读取 I/O 爆发的最大长度。
iops_wr=<iops>
每秒最大写入 I/O 操作数。
iops_wr_max=<iops>
每秒最大无节制写入 I/O 池操作数。
iops_wr_max_length=<seconds>
以秒为单位的写入 I/O 爆发的最大长度。
iothread=<boolean>
是否为此驱动器使用 iothreads
mbps=<mbps>
以兆字节每秒为单位的最大读/写速度。
mbps_max=<mbps>
以兆字节每秒为单位的最大无节制读/写池。
mbps_rd=<mbps>
以兆字节每秒为单位的最大读取速度。
mbps_rd_max=<mbps>
以兆字节每秒为单位的最大无节制读取池。
mbps_wr=<mbps>
以兆字节每秒为单位的最大写入速度。
mbps_wr_max=<mbps>
以兆字节每秒为单位的最大无节制写入池。
media=<cdrom | disk> (default = disk)
驱动器的媒体类型。
replicate=<boolean> (default = 1)
驱动器是否应被视为复制任务。
rerror=<ignore | report | stop>
读取错误操作。
ro=<boolean>
驱动器是否为只读。
secs=<integer>
强制驱动器的物理几何结构具有特定的扇区计数。
serial=<serial>
驱动器报告的序列号，url 编码，最长 20 个字节。
shared=<boolean> (default = 0)
将此本地管理的卷标记为在所有节点上可用。
警告
此选项不会自动共享卷，而是假定它已经共享！
size=<DiskSize>
磁盘大小。这纯粹是信息性的，没有任何影响。
snapshot=<boolean>
控制 qemu 的快照模式特性。如果激活，对磁盘所做的更改是临时的，并且在虚拟机关闭时将被丢弃。
trans=<auto | lba | none>
强制磁盘几何 BIOS 转换模式。
werror=<enospc | ignore | report | stop>
写入错误操作。
vmgenid: <UUID> (default = 1 (autogenerated))
虚拟机生成 ID（vmgenid）设备向客户操作系统公开一个 128 位整数值标识符。这允许在虚拟机以不同的配置执行时（例如，快照执行或从模板创建）通知客户操作系统。
客户操作系统注意到变化，然后可以通过将其分布式数据库的副本标记为脏，重新初始化其随机数生成器等来适当地作出反应。请注意，仅通过 API/CLI 
创建或更新方法自动创建才有效，但手动编辑配置文件时不起作用。
vmstatestorage: <string>
虚拟机状态卷/文件的默认存储。
watchdog: [[model=]<i6300esb|ib700>] [,action=<enum>]
创建一个虚拟硬件看门狗设备。一旦启用（由客户操作触发），看门狗必须定期由客户内部的代理进行轮询，否则看门狗将重置客户（或执行指定的相应操作）
action=<debug | none | pause | poweroff | reset | shutdown>
在激活后客户未能及时轮询看门狗的情况下执行的操作。
model=<i6300esb | ib700> (default = i6300esb)
要模拟的看门狗类型。

10.14 锁
在线迁移、快照和备份（vzdump）在受影响的虚拟机上设置锁，以防止不兼容的并发操作。有时候您需要手动解除这样的锁（例如，在断电之后）。
qm unlock <vmid>
警告
只有在确定设置锁的操作不再运行时才这样做。