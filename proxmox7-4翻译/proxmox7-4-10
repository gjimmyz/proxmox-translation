proxmox7-4-10

第10章
QEMU/KVM 虚拟机

QEMU（Quick Emulator 的简称）是一个开源的虚拟机管理程序，可以模拟物理计算机。从运行 QEMU 的宿主系统的角度来看，QEMU 
是一个用户程序，可以访问诸如分区、文件、网络卡等一系列本地资源，然后将这些资源传递给模拟的计算机，使其看起来像是真实的设备。
运行在模拟计算机中的客户操作系统访问这些设备，并像在真实硬件上运行一样运行。
例如，您可以将 ISO 镜像作为参数传递给 QEMU，而在模拟计算机中运行的操作系统将看到一个真正的 CD-ROM 插入到 CD 驱动器中。
QEMU 可以模拟从 ARM 到 Sparc 的各种硬件，但 Proxmox VE 只关心 32 位和 64 位的 PC 
克隆仿真，因为它代表了绝大多数服务器硬件。由于处理器扩展的可用性，使得当模拟架构与主机架构相同时，QEMU 可以大大加速 PC 克隆的仿真。
注意
您有时可能会遇到术语 KVM（基于内核的虚拟机）。这意味着 QEMU 正在运行在虚拟化处理器扩展的支持下，通过 Linux KVM 模块。
在 Proxmox VE 的上下文中，QEMU 和 KVM 可以互换使用，因为 Proxmox VE 中的 QEMU 总是会尝试加载 KVM 模块。
Proxmox VE 内部的 QEMU 以 root 进程运行，因为这是访问块和 PCI 设备所必需的。

10.1 模拟设备和半虚拟化设备
QEMU 模拟的 PC 硬件包括主板、网络控制器、SCSI、IDE 和 SATA 控制器、串行端口（完整列表可以在 kvm(1) 手册页中查看），所有这些都是通过软件模拟的。
所有这些设备都是现有硬件设备的精确软件等价物，如果客户机中运行的操作系统具有适当的驱动程序，它将像在真实硬件上运行一样使用这些设备。
这使得 QEMU 可以运行未经修改的操作系统。然而，这会带来性能成本，因为在软件中运行本来在硬件中运行的东西会为宿主 CPU 带来很多额外工作。
为了缓解这种情况，QEMU 可以向客户操作系统提供半虚拟化设备，其中客户操作系统意识到它正在 QEMU 内运行并与虚拟机管理程序合作。
QEMU 依赖于 virtio 虚拟化标准，因此可以提供半虚拟化的 virtio 设备，包括半虚拟化的通用磁盘控制器、半虚拟化的网络卡、半虚拟化的串行端口、半虚拟化的 SCSI 控制器等。
提示
强烈建议您在可以使用 virtio 设备的情况下使用它们，因为它们可以大大提高性能并且通常维护得更好。使用 virtio 通用磁盘控制器与模拟 IDE 
控制器相比，可以将顺序写入吞吐量提高一倍，如通过 bonnie++(8) 测量。使用 virtio 网络接口可以提供高达三倍于模拟的 Intel E1000 网络卡的吞吐量，
如通过 iperf(1) 测量。aa请参阅 KVM wiki 上的此基准测试 https://www.linux-kvm.org/page/Using_VirtIO_NIC

10.2 虚拟机设置
一般来说，Proxmox VE 会为虚拟机（VM）选择合理的默认设置。确保您了解所更改设置的含义，因为它可能导致性能降低或使您的数据处于风险之中。

10.2.1 通用设置
虚拟机的通用设置包括：
• 节点：运行 VM 的物理服务器
• VM ID：用于标识您的 VM 的此 Proxmox VE 安装中的唯一编号
• 名称：一个自由形式的文本字符串，您可以用来描述 VM
• 资源池：VM 的逻辑分组

10.2.2 操作系统设置
在创建虚拟机（VM）时，设置适当的操作系统（OS）允许 Proxmox VE 优化一些底层参数。
例如，Windows 操作系统希望 BIOS 时钟使用本地时间，而基于 Unix 的操作系统希望 BIOS 时钟具有 UTC 时间。

10.2.3 系统设置
在创建 VM 时，您可以更改新 VM 的一些基本系统组件。您可以指定要使用的显示类型（第 10.2.8 节）。
此外，还可以更改 SCSI 控制器（第 10.2.4 节）。
如果您打算安装 QEMU Guest Agent，或者您选择的 ISO 映像已经包含并自动安装它，您可能希望勾选 QEMU Agent 框，让 Proxmox VE 
知道它可以使用其功能来显示更多信息，并更智能地完成某些操作（例如，关闭或快照）。
Proxmox VE 允许使用不同的固件和机器类型引导 VM，即 SeaBIOS 和 OVMF（第 10.2.10 节）。
在大多数情况下，只有当您计划使用 PCIe 透传（第 10.9 节）时，才需要从默认的 SeaBIOS 切换到 OVMF。VM 的机器类型定义了 VM 虚拟主板的硬件布局。
您可以在默认的 Intel 440FX 和 Q35 芯片组之间进行选择，Q35 芯片组还提供了一个虚拟 PCIe 总线，因此，如果要通过 PCIe 硬件，可能是个理想的选择。

10.2.4 硬盘
总线/控制器
QEMU 可以模拟许多存储控制器：
提示
出于性能原因和更好的维护，强烈建议使用 VirtIO SCSI 或 VirtIO Block 控制器。
• IDE 控制器，其设计可以追溯到 1984 年的 PC/AT 磁盘控制器。尽管此控制器已被最新设计所取代，但您可以想象的每一个操作系统都支持它，使其成为运行 2003 
年之前发布的操作系统的绝佳选择。您可以在此控制器上连接多达 4 个设备。
• SATA（Serial ATA）控制器，始于 2003 年，具有更现代的设计，允许更高的吞吐量以及连接更多设备。您可以在此控制器上连接多达 6 个设备。
• SCSI 控制器，设计于 1985 年，通常在服务器级硬件上找到，可以连接多达 14 个存储设备。Proxmox VE 默认模拟 LSI 53C895A 控制器。
如果追求性能，建议使用 VirtIO SCSI single 类型的 SCSI 控制器并为附加磁盘启用 IO Thread（第 10.2.4 节）设置。
这是自 Proxmox VE 7.3 以来新创建的 Linux VM 的默认设置。每个磁盘将拥有自己的 VirtIO SCSI 控制器，QEMU 将在专用线程中处理磁盘 IO。
Linux 发行版自 2012 年以来支持此控制器，FreeBSD 自 2014 年以来支持。对于 
Windows 操作系统，您需要在安装过程中提供一个包含驱动程序的额外 ISO。
• VirtIO Block 控制器，通常称为 VirtIO 或 virtio-blk，是一种较旧类型的半虚拟化控制器。在功能上，它已被 VirtIO SCSI 控制器所取代。
映像格式
在每个控制器上，您可以附加一些模拟硬盘，这些硬盘由位于配置存储中的文件或块设备支持。存储类型的选择将决定硬盘映像的格式。
呈现块设备的存储（LVM、ZFS、Ceph）将需要原始磁盘映像格式，而基于文件的存储（Ext4、NFS、CIFS、GlusterFS）将允许您选择原始磁盘映像格式或 QEMU 映像格式。
• QEMU 映像格式是一种写时复制格式，允许对磁盘映像进行快照和薄配置。
• 原始磁盘映像是硬盘的位对位映像，类似于在 Linux 中对块设备执行 dd 命令时所得到的内容。
此格式本身不支持薄配置或快照，需要存储层进行这些任务的协作。但是，它可能比 QEMU 映像格式快 10%。1
• VMware 映像格式仅在您打算将磁盘映像导入/导出到其他虚拟机管理程序时才有意义。
缓存模式
设置硬盘的缓存模式将影响主机系统如何通知客户机系统块写入完成。无缓存默认表示在每个块到达物理存储写入队列时，将通知客户机系统写入已完成，忽略主机页面缓存。
这在安全性和速度之间提供了良好的平衡。
如果您希望 Proxmox VE 备份管理器在对 VM 进行备份时跳过某个磁盘，可以在该磁盘上设置“无备份”选项。
如果您希望 Proxmox VE 存储复制机制在启动复制作业时跳过一个磁盘，您可以在该磁盘上设置“跳过复制”选项。
从 Proxmox VE 5.0 开始，复制要求将磁盘映像放在 zfspool 类型的存储上，因此当 VM 配置了复制时，将磁盘映像添加到其他存储需要为此磁盘映像跳过复制。
Trim/Discard
如果您的存储支持薄配置（请参阅 Proxmox VE 指南中的存储章节），您可以在驱动器上激活 Discard 选项。
设置了 Discard 并且启用了 TRIM 的客户机操作系统2时，当 VM 
的文件系统在删除文件后将块标记为未使用时，控制器会将此信息传递给存储，存储会相应地缩小磁盘映像。要使客户机能够发出 TRIM 命令，必须在驱动器上启用 Discard 
选项。某些客户机操作系统可能还需要设置 SSD 仿真标志。请注意，仅在使用 Linux 内核 5.0 或更高版本的客户机上支持 VirtIO Block 驱动器上的 Discard。
如果您希望将驱动器显示为固态驱动器而不是旋转硬盘，则可以在该驱动器上设置 SSD 仿真选项。无需底层存储实际由 SSD 支持；此功能可与任何类型的物理介质一起使用。
请注意，SSD 仿真不支持 VirtIO Block 驱动器。
IO 线程
IO 线程选项仅在使用 VirtIO 控制器的磁盘或在模拟控制器类型为 VirtIO SCSI single 的 SCSI 控制器时可用。
启用 IO 线程后，QEMU 为每个存储控制器创建一个 I/O 线程，而不是在主事件循环或 vCPU 线程中处理所有 I/O。一个好处是更好地分配工作和利用底层存储。
另一个好处是对于非常 I/O 密集型的主机工作负载，降低了客户端的延迟（挂起），因为既不是主线程也不是 vCPU 线程会被磁盘 I/O 阻塞。
1有关详细信息，请参阅此基准测试 https://events.static.linuxfound.org/sites/events/files/slides/-
CloudOpen2013_Khoa_Huynh_v3.pdf
2TRIM、UNMAP 和丢弃 https://en.wikipedia.org/wiki/Trim_%28computing%29

10.2.5 CPU
CPU 插槽是PC主板上的一个物理插槽，可以插入CPU。然后，这个CPU可以包含一个或多个内核，它们是独立的处理单元。
从性能角度来看，拥有一个单一的CPU插槽，具有4个核心，或者两个CPU插槽，每个插槽具有两个核心，这两者之间的差别并不大。
然而，某些软件许可证取决于一台计算机拥有的插槽数量， 在这种情况下，将插槽数量设置为许可证允许的数量是有意义的。
增加虚拟 CPU（核心和插槽）的数量通常会提高性能，但这在很大程度上取决于虚拟机的使用情况。
多线程应用程序将从大量虚拟 CPU 中受益，因为对于您添加的每个虚拟 CPU，QEMU 都会在主机系统上创建一个新的执行线程。
如果您不确定虚拟机的工作负载，通常将总核心数量设置为 2 是一个安全的选择。
请注意
如果您所有虚拟机的总核心数量大于服务器上的核心数量（例如，一台只有 8 个核心的计算机上有 4 个虚拟机，每个虚拟机有 4 个核心（=总计 16 
个核心）），这是完全安全的。在这种情况下，主机系统将在您的服务器核心之间平衡 QEMU 执行线程，就像您运行标准多线程应用程序一样。
然而，Proxmox VE 将阻止您启动具有多于物理可用的虚拟 CPU 核心的虚拟机，因为这只会由于上下文切换的成本而降低性能。
资源限制
除了虚拟核心数量外，您还可以配置 VM 可以获得的资源，以便与主机 CPU 时间以及与其他 VM 的关系。使用 cpulimit（“主机 CPU 时间”）选项，您可以限制整个 VM 
可以在主机上使用多少 CPU 时间。这是一个表示 CPU 时间百分比的浮点值，因此 1.0 等于 100%，2.5 等于 250% 以此类推。
如果一个单独的进程完全使用一个单独的核心，它将占用 100% 的 CPU 时间。如果一个具有四个核心的虚拟机充分利用其所有核心，理论上它将使用 400%。
实际上，由于 QEMU 可以为 VM 外围设备（除了 vCPU 核心线程）创建额外线程，因此使用量可能会更高。
如果 VM 应该具有多个 vCPUs，因为它并行运行了几个进程，但整个 VM 不应该能够同时运行所有 vCPUs 100%，则此设置可能很有用。
以一个特定的示例：假设我们有一个 VM，它可以从拥有 8 个 vCPUs 中受益，但是在任何时候，这 8 个核心都不应该满负荷运行 - 
因为这会使服务器过载，以至于其他 VM 和 CT 获得更少的 CPU。因此，我们将 cpulimit 限制设置为 4.0（= 400%）。
如果所有核心都执行相同的繁重工作，它们都将获得 50% 的真实主机核心 CPU 时间。但是，如果只有 4 个核心工作，它们仍然可以获得接近每个核心 100% 的实际核心。
请注意
根据其配置，VM 可以使用额外的线程，例如用于网络或 IO 操作，还有实时迁移。因此，VM 可以显示使用的 CPU 时间超过其虚拟 CPU 可以使用的时间。
要确保 VM 从不使用比分配的虚拟 CPU 更多的 CPU 时间，请将 cpulimit 设置为与总核心计数相同的值。
第二个 CPU 资源限制设置是 cpuunits（现在通常称为 CPU 共享或 CPU 权重），它控制 VM 与其他正在运行的 VM 相比获得多少 CPU 时间。
默认值为 100（如果主机使用旧的 cgroup v1，则为 1024）。如果为 VM 增加此值，调度程序将根据其他权重较低的 VM 对其进行优先级排序。
例如，如果 VM 100 设置了默认值 100，VM 200 更改为 200，后者 VM 200 将获得比第一个 VM 100 多两倍的 CPU 带宽。
有关更多信息，请参阅 man systemd.resource-control，其中 CPUQuota 对应于 cpulimit，CPUWeight 对应于我们的 cpuunits 
设置，请访问其“注意事项”部分以获取参考和实现细节。第三个 CPU 资源限制设置，affinity，控制虚拟机将被允许在哪些主机核心上执行。
例如，如果提供了亲和值 0-3,8-11，虚拟机将受限于使用主机核心 0,1,2,3,8,9,10 和 11。
有效的亲和值以 cpuset 列表格式编写。列表格式是一个以 ASCII 十进制表示的 CPU 数字和数字范围的逗号分隔列表。
请注意
CPU 亲和性使用 taskset 命令将虚拟机限制在给定的一组内核上。此限制对于可能为 IO 创建的某些类型的进程不会生效。CPU 亲和性不是一项安全功能。
有关亲和性的更多信息，请参阅 man cpuset。在这里，列表格式对应于有效的亲和值。访问其“格式”部分以获取更多示例。
CPU 类型
QEMU 可以模拟从 486 到最新 Xeon 处理器的不同 CPU 类型。每个新处理器代都会添加新功能，如硬件辅助的 3d 渲染，随机数生成，内存保护等。
通常，您应该为 VM 选择一个与主机系统 CPU 类型非常接近的处理器类型，因为这意味着主机 CPU 功能（也称为 CPU 标志）将在您的 VM 中可用。
如果您想要完全匹配，可以将 CPU 类型设置为主机，这样 VM 将具有与主机系统完全相同的 CPU 标志。
虽然如此，这也存在一个缺点。如果您想在不同的主机之间进行 VM 的实时迁移，您的 VM 可能会在具有不同 CPU 类型的新系统上运行。
如果传递给客户机的 CPU 标志丢失，qemu 进程将停止。为了解决这个问题，QEMU 还有自己的 CPU 类型 kvm64，Proxmox VE 默认使用它。
kvm64 是一个类似 Pentium 4 的 CPU 类型，具有较少的 CPU 标志集，但可以确保在任何地方都能正常工作。
简而言之，如果您关心实时迁移和在节点之间移动 VM，请保留 kvm64 默认值。
如果您不关心实时迁移或者拥有一个同构集群，其中所有节点都具有相同的 CPU，请将 CPU 类型设置为 host，因为理论上这将为您的客户机提供最大性能。
自定义 CPU 类型
您可以指定具有可配置功能集的自定义 CPU 类型。这些由管理员在配置文件 /etc/pve/virtual-guest/cpu-models.conf 中维护。
有关格式详细信息，请参阅 man cpu-models.conf。
指定的自定义类型可以由在 /nodes 上具有 Sys.Audit 权限的任何用户选择。通过 CLI 或 API 为 VM 配置自定义 CPU 类型时，名称需要以 custom- 为前缀。
与熔断/幽灵相关的 CPU 标志
有几个与熔断和幽灵漏洞相关的 CPU 标志，除非您选择的 VM 的 CPU 类型默认启用它们，否则需要手动设置。
要使用这些 CPU 标志，需要满足以下两个要求：
    主机 CPU 必须支持该功能并将其传播到客户机的虚拟 CPU
    客户操作系统必须更新到能够缓解攻击并能够利用 CPU 功能的版本
否则，您需要设置虚拟 CPU 的所需 CPU 标志，可以通过编辑 WebUI 中的 CPU 选项，或者设置 VM 配置文件中的 cpu 选项的 flags 属性来实现。
对于 Spectre v1, v2, v4 修复，您的 CPU 或系统供应商还需要为您的 CPU 提供所谓的“微码更新”4。
要检查 Proxmox VE 主机是否存在漏洞，请以 root 用户身份执行以下命令：
for f in /sys/devices/system/cpu/vulnerabilities/; do echo "${f##/} -" $( -
cat "$f"); done
社区脚本也可用于检测主机是否仍然存在漏洞。5
英特尔处理器
    pcid
    这减小了熔断（CVE-2017-5754）缓解措施内核页表隔离（KPTI）对性能的影响，有效地将内核内存从用户空间隐藏起来。没有 PCID，KPTI 是一种相当昂贵的机制6。
    要检查 Proxmox VE 主机是否支持 PCID，请以 root 用户身份执行以下命令：
    3Meltdown Attack https://meltdownattack.com/
    4如果您的供应商不提供此类更新，可以使用 Debian non-free 的 'intel-microcode' / 'amd-microcode'。
    请注意，并非所有受影响的 CPU 都可以更新以支持 spec-ctrl。
    5spectre-meltdown-checker https://meltdown.ovh/
    6PCID 现在是 x86 上的关键性能/安全特性 https://groups.google.com/forum/m/#!topic/mechanicalsympathy/L9mHTbeQLNU
    Proxmox VE Administration Guide 189 / 534
    ＃grep ' pcid ' /proc/cpuinfo
    如果返回不为空，则您的主机 CPU 支持 pcid。
    spec-ctrl
    在 retpolines 不足以应对的情况下，启用 Spectre v1（CVE-2017-5753）和 Spectre v2（CVE-2017-5715）修复所需。
    默认包含在带有 -IBRS 后缀的 Intel CPU 型号中。必须明确地为没有 -IBRS 后缀的 Intel CPU 型号打开。
    需要更新的主机 CPU 微码（intel-microcode >= 20180425）。
    ssbd
    启用 Spectre V4（CVE-2018-3639）修复所需。默认情况下，没有任何 Intel CPU 型号包括它。必须明确地为所有 Intel CPU 型号打开。
    需要更新的主机 CPU 微码（intel-microcode >= 20180703）。
AMD 处理器
    ibpb
    在 retpolines 不足以应对的情况下，启用 Spectre v1（CVE-2017-5753）和 Spectre v2（CVE-2017-5715）修复所需。
    默认包含在带有 -IBPB 后缀的 AMD CPU 型号中。必须明确地为没有 -IBPB 后缀的 AMD CPU 型号打开。
    要在客户端 CPU 上使用此功能，需要主机 CPU 微码支持此功能。
    virt-ssbd
    启用 Spectre v4（CVE-2018-3639）修复所需。默认情况下，没有任何 AMD CPU 型号包括它。必须明确地为所有 AMD CPU 型号打开。即使提供了 
    amd-ssbd，也应为最大客户端兼容性提供给客户端。请注意，当使用 "host" cpu 模型时，必须明确启用此功能，因为这是物理 CPU 中不存在的虚拟功能。
    amd-ssbd
    启用 Spectre v4（CVE-2018-3639）修复所需。默认情况下，没有任何 AMD CPU 型号包括它。必须明确地为所有 AMD CPU 型号打开。
    这比 virt-ssbd 提供更高的性能，因此支持此功能的主机应尽可能将此功能暴露给客户端。尽管如此，还应暴露 virt-ssbd 以获得最大的客户端兼容性，
    因为一些内核只知道 virt-ssbd。
    amd-no-ssb
    建议表示主机不易受 Spectre V4（CVE-2018-3639）攻击。默认情况下，没有任何 AMD CPU 型号包括它。
    未来的 CPU 硬件代不容易受到 CVE-2018-3639 攻击，因此应通过暴露 amd-no-ssb 告知客户端不要启用其缓解措施。这与 virt-ssbd 和 amd-ssbd 互斥。
NUMA
您还可以选择在 VM 中模拟 NUMA 7 架构。NUMA 架构的基本原理是，不是让所有核心共享一个全局内存池，而是将内存分散到靠近每个插槽的本地存储区域。
这样可以提高速度，因为内存总线不再成为瓶颈。
7https://en.wikipedia.org/wiki/Non-uniform_memory_access
Proxmox VE Administration Guide 190 / 534
如果您的系统具有 NUMA 架构 8，我们建议激活该选项，因为这将允许在主机系统上正确分配 VM 资源。此选项还需要在 VM 中热插拔内核或 RAM。
如果使用 NUMA 选项，建议将套接字数设置为主机系统节点数。
vCPU 热插拔
现代操作系统引入了在运行系统中热插拔和在一定程度上热拔出 CPU 的功能。虚拟化使我们能够避免实际硬件在这种场景下可能导致的许多（物理）问题。
然而，这仍然是一个相对较新且复杂的功能，因此应将其使用限制在绝对需要的情况下。大部分功能可以通过其他经过充分测试且不那么复杂的功能实现，参见资源限制第 10.2.5 节。
在 Proxmox VE 中，插入的 CPU 最大数量始终为 cores * sockets。要使用少于此总核心数的 CPU 启动 VM，可以使用 vpus 设置，它表示在 VM 启动时应插入多少个 vCPU。
目前只有 Linux 支持此功能，需要 3.10 以上的内核，推荐使用 4.7 以上的内核。
您可以使用以下 udev 规则在客户端中自动将新 CPU 设置为在线状态：
SUBSYSTEM=="cpu", ACTION=="add", TEST=="online", ATTR{online}=="0", ATTR{online}="1"
将此内容保存在 /etc/udev/rules.d/ 目录下，文件以 .rules 结尾。
注意：CPU 热拔除依赖于机器并需要客户端协作。
删除命令不能保证 CPU 实际被移除，通常它是一个请求，通过目标相关的机制（如 x86/amd64 上的 ACPI）转发给客户端操作系统。

10.2.6 内存
对于每个 VM，您可以选择设置固定大小的内存，或者要求 Proxmox VE 根据主机当前 RAM 使用情况动态分配内存。
8 如果命令 numactl --hardware | grep available 返回多个节点，则您的主机系统具有 NUMA 架构。
固定内存分配
当将内存和最小内存设置为相同数量时，Proxmox VE 将简单地为您的 VM 分配您指定的内存。
即使使用固定内存大小，气球设备也会添加到 VM 中，因为它提供了有用的信息，例如客户机实际使用了多少内存。通常情况下，您应该保持气球设备启用，
但如果您想禁用它（例如出于调试目的），只需取消选中“气球设备”或在配置中设置：
balloon: 0
自动内存分配
当将最小内存设置得低于内存时，Proxmox VE 将确保为 VM 始终提供您指定的最小内存，如果主机上的 RAM 使用率低于 80%，则会动态向客户机添加内存，直到达到指定的最大内存。 
当主机 RAM 不足时，VM 会将一些内存释放回主机，如果需要，交换运行中的进程，并在最后一招中启动 oom 杀手。 
主机和客户机之间的内存传递是通过在客户机内运行的特殊气球内核驱动程序完成的，该驱动程序将从主机获取或释放内存页面。9
当多个 VM 使用自动分配功能时，可以设置一个 Shares 系数，表示每个 VM 应占用的空闲主机内存的相对数量。 假设您有四个 VM，其中三个运行 HTTP 
服务器，最后一个是数据库服务器。 为了在数据库服务器 RAM 中缓存更多的数据库块，您希望在有空闲 RAM 时优先考虑数据库 VM。 
为此，您为数据库 VM 分配一个 Shares 属性值 3000，将其他 VM 的 Shares 默认设置为 1000。 
主机服务器有 32GB 的 RAM，目前使用了 16GB，剩余 32 * 80/100 - 16 = 9GB RAM 可分配给 VM。 
数据库 VM 将获得额外的 9 * 3000 / (3000 + 1000 + 1000 + 1000) = 4.5 GB RAM，每个 HTTP 服务器将获得 1.5 GB。
在 2010 年之后发布的所有 Linux 发行版都包含了气球内核驱动程序。 对于 Windows 
操作系统，需要手动添加气球驱动程序，这可能会导致客户机速度变慢，因此我们不建议在关键系统上使用它。
在为 VM 分配 RAM 时，一个经验法则是始终为主机留出 1GB 的 RAM。

10.2.7 网络设备
每个 VM 可以有四种不同类型的多个网络接口控制器（NIC）：
    Intel E1000 是默认值，模拟 Intel 千兆网络卡。
    如果您追求最大性能，应使用 VirtIO 抛出虚拟化的 NIC。和所有 VirtIO 设备一样，客户机操作系统应安装适当的驱动程序。
    Realtek 8139 模拟较旧的 100 MB/s 网络卡，仅应在模拟较旧操作系统时使用（2002 年之前发布）
    vmxnet3 是另一种抛出虚拟化设备，只有在从另一个虚拟机管理程序导入 VM 时才应使用。
Proxmox VE 将为每个 NIC 生成一个随机 MAC 地址，以便您的 VM 可以在以太网网络上寻址。
您添加到 VM 的 NIC 可以遵循以下两种不同的模型之一：
    在默认的桥接模式下，每个虚拟 NIC 都由主机上的一个 tap 设备支持（模拟以太网 NIC 的软件环回设备）。
    此 tap 设备将添加到一个桥中，在 Proxmox VE 中默认为 vmbr0。在此模式下，VM 可以直接访问主机所在的以太网局域网。
    在另一种 NAT 模式下，每个虚拟 NIC 只会与 QEMU 用户网络堆栈通信，其中内置的路由器和 DHCP 服务器可以提供网络访问。
    这个内置的 DHCP 将在私有 10.0.2.0/24 范围内提供地址。 NAT 模式比桥接模式慢得多，只应用于测试。此模式仅通过 CLI 或 API 提供，但不通过 WebUI 提供。
在创建 VM 时，您还可以通过选择“无网络设备”来跳过添加网络设备。
您可以为每个 VM 网络设备覆盖 MTU 设置。选项 mtu=1 表示一个特殊情况，在这种情况下，MTU 值将从底层桥继承。此选项仅适用于 VirtIO 网络设备。
如果您使用 VirtIO 驱动程序，您可以选择性地激活 Multiqueue 选项。此选项允许客户机操作系统使用多个虚拟 CPU 来处理网络数据包，从而增加传输的数据包总数。
在使用 Proxmox VE 的 VirtIO 驱动程序时，每个 NIC 网络队列都会传递给主机内核，队列将由 vhost 驱动程序生成的内核线程处理。
通过激活此选项，可以为每个 NIC 将多个网络队列传递给主机内核。
使用 Multiqueue 时，建议将其设置为等于客户机总核心数的值。您还需要使用 ethtool 命令在 VM 中为每个 VirtIO NIC 设置多用途通道的数量：
ethtool -L ens1 combined X
其中 X 是 VM 的 vcpus 数量。
请注意，将 Multiqueue 参数设置为大于 1 的值将随着流量的增加增加主机和客户机系统上的 CPU 负载。
我们建议仅在 VM 需要处理大量传入连接时设置此选项，例如当 VM 作为路由器、反向代理或执行长轮询的繁忙 HTTP 服务器运行时。

10.2.8 显示
QEMU 可以虚拟几种类型的 VGA 硬件。一些例子是：
    std，默认值，模拟带有 Bochs VBE 扩展的卡。
    cirrus，曾经是默认值，它模拟了一个非常旧的硬件模块及其所有问题。此显示类型应仅在确实需要时使用 10，例如，如果使用 Windows XP 或更早版本
    vmware，是一个兼容 VMWare SVGA-II 的适配器。
    qxl，是 QXL 虚拟化图形卡。选择此选项还会为 VM 启用 SPICE（一种远程查看器协议）。
    virtio-gl，通常称为 VirGL，是供 VM 内部使用的虚拟 3D GPU，可以将工作负载卸载到主机 GPU，无需特殊（昂贵）型号和驱动程序，
    也无需完全绑定主机 GPU，允许在多个客户机和/或主机之间重用。
10 https://www.kraxel.org/blog/2014/10/qemu-using-cirrus-considered-harmful/ qemu: 使用 cirrus 被认为是有害的
注意
VirGL 支持需要一些额外的库，这些库由于相对较大且对于所有 GPU 型号/供应商并非全部开源，因此默认情况下没有安装。对于大多数设置，您只需执行以下操作：
apt install libgl1 libegl1
您可以通过设置 memory 选项来编辑分配给虚拟 GPU 的内存量。这可以在 VM 内启用更高的分辨率，尤其是在使用 SPICE/QXL 时。
由于内存由显示设备预留，为 SPICE 选择多监视器模式（例如 qxl2 用于双监视器）有一些影响：
    Windows 需要每个监视器一个设备，所以如果您的 ostype 是 Windows 的某个版本，Proxmox VE 会为 VM 提供每个监视器一个额外的设备。
    每个设备都会得到指定的内存量。Linux VM 可以始终启用更多虚拟监视器，但选择多监视器模式会使设备的内存乘以监视器的数量。
选择 serialX 作为显示类型将禁用 VGA 输出，并将 Web 控制台重定向到所选串行端口。在这种情况下，配置的显示内存设置将被忽略。

10.2.9 USB 透传
有两种不同类型的 USB 透传设备：
    主机 USB 透传
    SPICE USB 透传
主机 USB 透传是通过为 VM 提供主机的 USB 设备来实现的。这可以通过供应商和产品 ID 来完成，也可以通过主机总线和端口来完成。
供应商/产品 ID 的格式如下：0123:abcd，其中 0123 是供应商的 ID，abcd 是产品的 ID，这意味着相同的 usb 设备有相同的 ID。
总线/端口的格式如下：1-2.3.4，其中 1 是总线，2.3.4 是端口路径。这表示您主机的物理端口（取决于 usb 控制器的内部顺序）。
如果在 VM 配置中存在一个设备，当 VM 启动时，设备不在主机中，则 VM 可以正常启动。只要设备/端口在主机中可用，就会通过它。
警告
使用这种类型的 USB 透传意味着您无法将 VM 在线移动到另一个主机，因为硬件仅在当前 VM 所在的主机上可用。
第二种类型的透传是 SPICE USB 透传。如果您使用支持它的 SPICE 客户端，这会很有用。
如果您为 VM 添加一个 SPICE USB 端口，您可以将 USB 设备从 SPICE 客户端所在的位置直接透传到 VM（例如输入设备或硬件加密狗）。

10.2.10 BIOS 和 UEFI
为了正确模拟计算机，QEMU 需要使用固件。在常见的 PC 上通常被称为 BIOS 或 (U)EFI，在启动 VM 
时作为第一步之一执行。它负责进行基本硬件初始化，并为操作系统提供固件和硬件接口。默认情况下，QEMU 使用 SeaBIOS，这是一个开源的 x86 BIOS 
实现。对于大多数标准设置，SeaBIOS 是一个不错的选择。
某些操作系统（如 Windows 11）可能需要使用兼容 UEFI 的实现。在这种情况下，您必须使用 OVMF，这是一个开源的 UEFI 实现。11
还有其他情况，其中 SeaBIOS 可能不是从其启动的理想固件，例如，如果您想进行 VGA 透传。12
如果您想使用 OVMF，有几件事情需要考虑：
为了保存诸如启动顺序之类的事物，需要有一个 EFI 磁盘。此磁盘将包含在备份和快照中，且只能有一个。
您可以使用以下命令创建这样一个磁盘：
# qm set <vmid> -efidisk0 <storage>:1,format=<format>,efitype=4m,pre-  -
enrolled-keys=1
其中 <storage> 是您想要存放磁盘的存储，<format> 是存储支持的格式。或者，您可以通过在 VM 的硬件部分添加 Add ! EFI Disk 通过 Web 界面创建这样一个磁盘。
efitype 选项指定应使用哪个版本的 OVMF 固件。对于新 VM，这应始终是 4m，因为它支持 Secure Boot 并为未来的开发分配了更多空间（这是 GUI 中的默认值）。
pre-enroll-keys 指定 efidisk 是否应预先加载特定于发行版的和 Microsoft Standard Secure Boot 密钥。
它还默认启用 Secure Boot（尽管仍可以在 VM 内的 OVMF 菜单中禁用）。
注意
如果您想在现有 VM（仍然使用 2m efidisk）中开始使用 Secure Boot，您需要重新创建 efidisk。
为此，请删除旧的 efidisk（qm set <vmid> -delete efidisk0）并按照上述方法添加新的 efidisk。这将重置您在 OVMF 菜单中所做的任何自定义配置！
在使用虚拟显示器的 OVMF（无 VGA 透传）时，您需要在 OVMF 菜单中设置客户端分辨率（在启动过程中按 ESC 键即可进入），或者选择 SPICE 作为显示类型。

10.2.11 可信平台模块 (TPM)
可信平台模块是一种安全存储机密数据（如加密密钥）并为验证系统启动提供防篡改功能的设备。
11请参阅 OVMF 项目 https://github.com/tianocore/tianocore.github.io/wiki/OVMF
12AlexWilliamson 有一篇关于此的很好的博客文章 https://vfio.blogspot.co.at/2014/08/primary-graphics-assignment-withoutvga.html
某些操作系统（如 Windows 11）要求在机器（物理或虚拟）上安装这样的设备。
通过指定 tpmstate 卷添加 TPM。这类似于 efidisk，一旦创建就不能更改（只能删除）。您可以通过以下命令添加一个：
# qm set <vmid> -tpmstate0 <storage>:1,version=<version>
其中 <storage> 是您想要将状态放在的存储，<version> 是 v1.2 或 v2.0。您还可以通过 Web 界面添加一个，在 VM 的硬件部分选择 Add ! TPM State。
v2.0 TPM 规范较新，支持性更好，因此除非您有需要 v1.2 TPM 的特定实现，否则应首选 v2.0 TPM。
注意
与物理 TPM 相比，模拟的 TPM 并没有提供任何实质性的安全优势。TPM 的重点是其中的数据除了通过 TPM 
规范指定的命令之外，不能轻易修改。由于在模拟设备中，数据存储在普通卷上，所以任何有权访问该卷的人都有可能编辑它。

10.2.12 虚拟机间共享内存
您可以添加一个虚拟机间共享内存设备 (ivshmem)，它允许在主机和客户机之间或多个客户机之间共享内存。要添加这样的设备，您可以使用 qm：
# qm set <vmid> -ivshmem size=32,name=foo
大小以 MiB 为单位。文件将位于 /dev/shm/pve-shm-$name 下（默认名称为 vmid）。
注意
目前，只要使用它的任何虚拟机关闭或停止，设备将被删除。打开的连接仍将持续，但无法再建立与同一设备的新连接。
这种设备的一个用例是 Looking Glass 13 项目，它可以在主机和客户机之间实现高性能、低延迟的显示镜像。

10.2.13 音频设备
要添加音频设备，请运行以下命令：
qm set <vmid> -audio0 device=<device>
支持的音频设备有：
    ich9-intel-hda：Intel HD Audio 控制器，模拟 ICH9
    intel-hda：Intel HD Audio 控制器，模拟 ICH6
    AC97：音频编解码器 '97，适用于较旧的操作系统，如 Windows XP
有两个可用的后端：
    spice
    none
spice 后端可与 SPICE Section 10.2.8 结合使用，而 none 后端可在需要在虚拟机中使用音频设备以使某些软件工作时使用。
要使用主机的物理音频设备，请使用设备直通（参见 PCI 直通第 10.9 节和 USB 直通第 10.2.9 节）。
像 Microsoft 的 RDP 这样的远程协议具有播放声音的选项。

10.2.14 VirtIO RNG
RNG（随机数生成器）是向系统提供熵（随机性）的设备。虚拟硬件-RNG 
可用于将来自主机系统的熵提供给客户机虚拟机。这有助于避免客户机中的熵饥饿问题（熵不足的情况，可能导致系统速度减慢或出现问题），特别是在客户机启动过程中。
要添加基于 VirtIO 的仿真 RNG，请运行以下命令：
qm set <vmid> -rng0 source=<source>[,max_bytes=X,period=Y]
source 指定在主机上从哪里读取熵，必须是以下之一：
    /dev/urandom：非阻塞内核熵池（首选）
    /dev/random：阻塞内核池（不推荐，可能导致主机系统上的熵饥饿）
    /dev/hwrng：传递连接到主机的硬件 RNG（如果有多个可用，则将使用在 /sys/devices/virtual/misc/hw_random/rng_current 中选择的一个）
通过 max_bytes 和 period 参数可以指定限制，它们以每 period 毫秒为单位读取 max_bytes。
然而，它并不代表线性关系：1024B/1000ms 意味着在 1 秒的计时器上，最多有 1 KiB 的数据可用，而不是在 1 秒的过程中将 1 KiB 
数据流传输到客户机。因此，减少周期可以用于以更快的速度向客户机注入熵。
默认情况下，限制设置为每 1000 毫秒 1024 字节（1 KiB/s）。建议始终使用限制器以避免客户端使用过多的主机资源。如果需要，可以使用 max_bytes 的值 0 来禁用所有限制。

10.2.15 设备启动顺序
QEMU 可以告诉客户机应该从哪些设备启动，以及启动顺序。这可以在配置中通过 boot 属性指定，例如：
vbnet
boot: order=scsi0;net0;hostpci0
这样，客户机首先尝试从磁盘 scsi0 启动，如果失败，它将继续尝试从 net0 网络启动，如果还是失败，最后尝试从传递的 PCIe 设备启动（如果是 
NVMe，则显示为磁盘，否则尝试启动到选项 ROM）。
在 GUI 上，您可以使用拖放编辑器指定启动顺序，并使用复选框启用或禁用某些设备的启动。
注意
如果您的客户机使用多个磁盘启动操作系统或加载引导程序，则所有这些磁盘都必须标记为可启动（即它们必须启用复选框或出现在配置列表中），以便客户机能够启动。
这是因为最近的 SeaBIOS 和 OVMF 版本只有在磁盘被标记为可启动时才会初始化磁盘。
在任何情况下，即使设备未出现在列表中或禁用了复选标记，一旦客户机的操作系统启动并初始化它们，它们仍将对客户机可用。可启动标志仅影响客户机 BIOS 和引导加载程序。

10.2.16 虚拟机的自动启动和关闭
创建 VM 后，您可能希望它们在主机系统启动时自动启动。为此，您需要从 Web 界面中的 VM 的“选项”选项卡中选择“启动时启动”选项，或使用以下命令设置：
# qm set <vmid> -onboot 1
启动和关机顺序
在某些情况下，您可能希望能够微调 VM 的启动顺序，例如，如果您的一个 VM 为其他客户机系统提供防火墙或 DHCP。对此，您可以使用以下参数：
    启动/关机顺序：定义启动顺序优先级。例如，如果您希望 VM 首先启动，请将其设置为 1。（我们为关闭使用相反的启动顺序，因此启动顺序为 1 
    的计算机将是最后一个被关闭的计算机）。如果一个主机上定义了多个具有相同顺序的 VM，它们将按 VMID 升序排序。
    启动延迟：定义此 VM 启动和后续 VM 启动之间的时间间隔。例如，如果您希望在启动其他 VM 之前等待 240 秒，请将其设置为 240。
    关机超时：定义 Proxmox VE 在发出关机命令后应等待 VM 离线的时间（以秒为单位）。
    默认情况下，此值设置为 180，这意味着 Proxmox VE 将发出关机请求并等待 180 秒以使计算机离线。如果计算机在超时后仍在线，它将被强制停止。
注意
由 HA 堆栈管理的 VM 当前不遵循启动顺序和启动顺序选项。启动和关机算法将跳过这些 VM，因为 HA 管理器本身确保 VM 被启动和停止。
请注意，没有设置启动/关机顺序参数的计算机将始终在设置参数的计算机之后启动。此外，此参数只能在同一主机上运行的虚拟机之间强制执行，而不是在整个集群范围内。
如果您需要在主机启动和第一个 VM 启动之间有延迟，请参阅有关 Proxmox VE 节点管理的部分 3.10.4。






























