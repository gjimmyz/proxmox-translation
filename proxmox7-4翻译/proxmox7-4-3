proxmox7-4-3

第3章
主机系统管理

以下部分将关注常见的虚拟化任务，并解释有关 Proxmox VE 主机机器的管理和管理的 Proxmox VE 特定信息。
Proxmox VE 基于 Debian GNU/Linux，并附带额外的存储库以提供与 Proxmox VE 相关的软件包。
这意味着可以使用所有范围的 Debian 软件包，包括安全更新和错误修复。Proxmox VE 
提供了基于 Ubuntu 内核的自己的 Linux 内核。它启用了所有必要的虚拟化和容器功能，并包括 ZFS 和一些额外的硬件驱动程序。
对于以下部分未包括的其他主题，请参阅 Debian 文档。Debian 管理员手册可以在线获取，提供了对 Debian 操作系统的全面介绍（参见 [Hertzog13]）。

3.1 软件包仓库
Proxmox VE 像其他基于 Debian 的系统一样，使用 APT 作为其软件包管理工具。

3.1.1 Proxmox VE 中的存储库
存储库是软件包的集合，可以用于安装新软件，但也可以用于获取新的更新。
注意
您需要有效的 Debian 和 Proxmox 存储库才能获取最新的安全更新、错误修复和新功能。
APT 存储库在文件 /etc/apt/sources.list 中定义，并在 /etc/apt/sources 中的 .list 文件中放置。
存储库管理
自 Proxmox VE 7.0 起，您可以在 Web 
界面中检查存储库状态。节点摘要面板显示高级状态概述，而单独的存储库面板显示深入状态和所有配置存储库的列表。基本存储库管理，例如激活或停用存储库，也得到了支持。
sources.list
在 sources.list 文件中，每行定义一个软件包存储库。首选源必须位于第一位。空行将被忽略。
行中任何地方的 # 字符将该行剩余部分标记为注释。通过运行 apt-get update 从存储库获取可用软件包。可以直接使用 apt-get 或通过 GUI（节点! 更新）安装更新。
文件 /etc/apt/sources.list
deb http://ftp.debian.org/debian bullseye main contrib
deb http://ftp.debian.org/debian bullseye-updates main contrib
security updates
deb http://security.debian.org/debian-security bullseye-security main -
contrib
Proxmox VE 提供了三个不同的软件包存储库。

3.1.2 Proxmox VE 企业存储库
这是默认的、稳定的和推荐的存储库，适用于所有 Proxmox VE 订阅用户。它包含了最稳定的软件包，适合生产环境使用。pve-enterprise 存储库默认启用：
文件 /etc/apt/sources.list.d/pve-enterprise.list
deb https://enterprise.proxmox.com/debian/pve bullseye pve-enterprise
root@pam 用户会通过电子邮件收到关于可用更新的通知。在 GUI 中点击 Changelog 按钮以查看有关所选更新的更多详细信息。
您需要一个有效的订阅密钥才能访问 pve-enterprise 存储库。有不同的支持级别可供选择。更多详细信息请访问 https://www.proxmox.com/en/proxmox-ve/pricing。
注意
您可以通过使用 #（位于行的开头）注释掉上面的行来禁用此存储库。如果您没有订阅密钥，这可以防止错误消息。在这种情况下，请配置 pve-no-subscription 存储库。

3.1.3 Proxmox VE 无订阅存储库
这是用于测试和非生产环境使用的推荐存储库。其软件包没有经过严格测试和验证。您无需订阅密钥即可访问 pve-no-subscription 存储库。
我们建议在 /etc/apt/sources.list 中配置此存储库。
文件 /etc/apt/sources.list
deb http://ftp.debian.org/debian bullseye main contrib
deb http://ftp.debian.org/debian bullseye-updates main contrib
由proxmox.com提供的 PVE pve-no-subscription 存储库，
不推荐用于生产环境
deb http://download.proxmox.com/debian/pve bullseye pve-no-subscription
安全更新
deb http://security.debian.org/debian-security bullseye-security main -
contrib

3.1.4 Proxmox VE 测试存储库
此存储库包含最新的软件包，主要供开发人员测试新功能。要配置它，请将以下行添加到 /etc/apt/sources.list：
sources.list entry for pvetest
deb http://download.proxmox.com/debian/pve bullseye pvetest
警告
pvetest 存储库（顾名思义）仅应用于测试新功能或修复错误。

3.1.5 Ceph Quincy 存储库
注意
Ceph Quincy（17.2）在 Proxmox VE 7.3 或之后的版本中被声明为稳定，使用 Ceph 17.2.1 版本。
此存储库包含主要的 Proxmox VE Ceph Quincy 软件包。它们适用于生产环境。如果在 Proxmox VE 上运行 Ceph 客户端或完整的 Ceph 集群，请使用此存储库。
文件 /etc/apt/sources.list.d/ceph.list
deb http://download.proxmox.com/debian/ceph-quincy bullseye main

3.1.6 Ceph Quincy 测试存储库
此 Ceph 存储库包含在 Ceph Quincy 软件包移至主存储库之前的软件包。它用于在 Proxmox VE 上测试新的 Ceph 版本。
文件 /etc/apt/sources.list.d/ceph.list
deb http://download.proxmox.com/debian/ceph-quincy bullseye test

3.1.7 Ceph Pacific 存储库
注意
Ceph Pacific（16.2）在 Proxmox VE 7.0 中被声明为稳定。
此存储库包含主要的 Proxmox VE Ceph Pacific 软件包。它们适用于生产环境。如果在 Proxmox VE 上运行 Ceph 客户端或完整的 Ceph 集群，请使用此存储库。
文件 /etc/apt/sources.list.d/ceph.list
deb http://download.proxmox.com/debian/ceph-pacific bullseye main

3.1.8 Ceph Pacific 测试存储库
此 Ceph 存储库包含在 Ceph Pacific 软件包移至主存储库之前的软件包。它用于在 Proxmox VE 上测试新的 Ceph 版本。
文件 /etc/apt/sources.list.d/ceph.list
deb http://download.proxmox.com/debian/ceph-pacific bullseye test

3.1.9 Ceph Octopus 存储库
注意
Ceph Octopus（15.2）在 Proxmox VE 6.3 中被声明为稳定。在 6.x 版本的剩余生命周期内，
它将继续获得更新[?informaltable]，同时在 Proxmox VE 7.x 中，直到 Ceph Octopus 上游 EOL（大约 2022-07）也会继续获得更新。
此存储库包含主要的 Proxmox VE Ceph Octopus 软件包。它们适用于生产环境。如果在 Proxmox VE 上运行 Ceph 客户端或完整的 Ceph 集群，请使用此存储库。
文件 /etc/apt/sources.list.d/ceph.list
deb http://download.proxmox.com/debian/ceph-octopus bullseye main
请注意，在较旧的 Proxmox VE 6.x 上，您需要将上述存储库规范中的 bullseye 更改为 buster。

3.1.10 Ceph Octopus 测试存储库
此 Ceph 存储库包含在 Ceph 软件包移至主存储库之前的软件包。它用于在 Proxmox VE 上测试新的 Ceph 版本。
文件 /etc/apt/sources.list.d/ceph.list
deb http://download.proxmox.com/debian/ceph-octopus bullseye test

3.1.11 SecureApt
存储库中的发布文件使用 GnuPG 进行签名。APT 使用这些签名来验证所有软件包来自可信来源。
如果您从官方 ISO 映像安装 Proxmox VE，则已经安装了用于验证的密钥。
如果您在 Debian 上安装 Proxmox VE，请使用以下命令下载并安装密钥：
wget https://enterprise.proxmox.com/debian/proxmox-release-bullseye.gpg --O /etc/apt/trusted.gpg.d/proxmox-release-bullseye.gpg
之后使用 sha512sum CLI 工具验证校验和：
sha512sum /etc/apt/trusted.gpg.d/proxmox-release-bullseye.gpg7 -
fb03ec8a1675723d2853b84aa4fdb49a46a3bb72b9951361488bfd19b29aab0a789a4f8c7406e71a69aabbc727c936d3549731c4659ffa1a08f44db8fdcebfa 
/etc/apt/trusted.gpg.d/
proxmox-release-bullseye.gpg
或者使用 md5sum CLI 工具：
md5sum /etc/apt/trusted.gpg.d/proxmox-release-bullseye.gpg
bcc35c7173e0845c0d6ad6470b70f50e /etc/apt/trusted.gpg.d/proxmox-release- -
bullseye.gpg

3.2 系统软件更新
Proxmox 定期为所有存储库提供更新。要安装更新，请使用基于 Web 的 GUI 或以下 CLI 命令：
apt-get update
apt-get dist-upgrade
注意
APT 软件包管理系统非常灵活，提供了许多功能，请参阅 man apt-get 或 [Hertzog13] 以获取其他信息。
提示
定期更新对于获取最新的补丁和安全相关修复至关重要。Proxmox VE 社区论坛宣布主要系统升级。

3.3 网络配置
Proxmox VE 使用 Linux 网络堆栈。这为在 Proxmox VE 节点上设置网络提供了很多灵活性。
配置可以通过 GUI 完成，也可以通过手动编辑包含整个网络配置的文件 /etc/network/interfaces 完成。interfaces(5) 手册页包含完整的格式描述。所有 Proxmox VE 
工具都尽力保留直接用户修改，但使用 GUI 仍然更可取，因为它可以防止出错。
需要 vmbr 接口将客户机连接到底层物理网络。它们是一个 Linux 网桥，可以被认为是一个虚拟交换机，客户机和物理接口连接到该虚拟交换机。
本节提供了一些关于如何设置网络以适应不同用例的示例，如使用 bond 第 3.3.7 节的冗余，vlans 第 3.3.8 节或路由第 3.3.5 节和 NAT 第 3.3.6 节设置。
软件定义网络第 12 章是 Proxmox VE 集群中更复杂虚拟网络的一个选项。
警告
不建议使用传统的 Debian 工具 ifup 和 ifdown（如果不确定），因为它们有一些陷阱，如在 ifdown vmbrX 时中断所有客户端流量，
但在稍后在同一桥上执行 ifup 时不重新连接这些客户端。

3.3.1 应用网络更改
Proxmox VE 不会直接将更改写入 /etc/network/interfaces。相反，我们将其写入一个名为 /etc/network/interfaces.new 
的临时文件，这样您就可以一次执行许多相关更改。这还允许在应用更改之前确保更改是正确的，因为错误的网络配置可能使节点无法访问。
使用 ifupdown2 实时重载网络
使用推荐的 ifupdown2 软件包（自 Proxmox VE 7.0 起为新安装的默认值），可以在不重新启动的情况下应用网络配置更改。
如果通过 GUI 更改网络配置，可以单击 Apply Configuration 
按钮。这将把更改从暂存的 interfaces.new 文件移动到 /etc/network/interfaces，并实时应用它们。
如果您直接对 /etc/network/interfaces 文件进行了手动更改，可以通过运行 ifreload -a 来应用它们。
注意
如果您在 Debian 上安装了 Proxmox VE，或者从旧版本的 Proxmox VE 升级到 Proxmox VE 7.0，请确保安装了 ifupdown2：apt install ifupdown2
重新启动节点以应用
另一种应用新网络配置的方法是重新启动节点。在这种情况下，systemd 服务 pvenetcommit 将在网络服务应用该配置之前激活暂存的 interfaces.new 
文件。这样，在节点重启时，新的网络配置将被应用。

3.3.2 命名约定
我们目前使用以下命名约定来表示设备名称：
    以太网设备：en*，systemd 网络接口名称。这种命名方案自 Proxmox VE 5.0 版本以来用于新的 Proxmox VE 安装。
    以太网设备：eth[N]，其中 0 ≤ N（eth0, eth1, ...）这种命名方案用于在 5.0 版本之前安装的 Proxmox VE 主机。升级到 5.0 时，名称保持不变。
    桥接器名称：vmbr[N]，其中 0 ≤ N ≤ 4094（vmbr0 - vmbr4094）
    Bonds：bond[N]，其中 0 ≤ N（bond0, bond1, ...）
    VLAN：简单地将 VLAN 编号添加到设备名称，用句点分隔（eno1.50，bond1.30）
这使得调试网络问题更加容易，因为设备名称暗示了设备类型。
Systemd 网络接口名称
Systemd 对以太网网络设备使用两个字符前缀 en。接下来的字符取决于设备驱动程序以及首先匹配的方案。
    o<index>[n<phys_port_name>|d<dev_port>] — 板载设备
    s<slot>[f<function>][n<phys_port_name>|d<dev_port>] — 通过热插拔 ID 的设备
    [P<domain>]p<bus>s<slot>[f<function>][n<phys_port_name>|d<dev_port>] — 按总线 ID 的设备
    x<MAC> — 通过 MAC 地址的设备
最常见的模式是：
    eno1 — 是第一个板载 NIC
    enp3s0f1 — 是位于 pcibus 3 插槽 0 的 NIC，并使用 NIC 功能 1。
有关更多信息，请参阅可预测的网络接口名称。

3.3.3 选择网络配置
根据您当前的网络组织和资源，您可以选择桥接、路由或伪装网络设置。
位于私有 LAN 中的 Proxmox VE 服务器，使用外部网关访问互联网
在这种情况下，桥接模型是最有意义的，这也是新 Proxmox VE 安装的默认模式。您的每个客户端系统都将拥有一个连接到 Proxmox VE 
桥接器的虚拟接口。这在效果上类似于将客户端网络卡直接连接到 LAN 上的新交换机，Proxmox VE 主机充当交换机的角色。
托管在托管提供商处的 Proxmox VE 服务器，为客户端提供公共 IP 范围
对于此设置，您可以根据提供商允许的情况使用桥接或路由模型。
托管在托管提供商处的 Proxmox VE 服务器，具有单个公共 IP 地址
在这种情况下，为您的客户端系统获得外部网络访问的唯一方法是使用伪装。对于传入客户端的网络访问，您需要配置端口转发。
为了进一步提高灵活性，您可以配置 VLAN（IEEE 802.1q）和网络绑定，也称为“链路聚合”。这样就可以构建复杂而灵活的虚拟网络。

3.3.4 使用桥接的默认配置
桥接就像是用软件实现的物理网络交换机。所有虚拟客户端都可以共享单个桥接，或者您可以创建多个桥接来分隔网络域。每个主机最多可以有 4094 个桥接。
安装程序创建一个名为 vmbr0 的单个桥接，该桥接连接到第一个以太网卡。在 /etc/network/interfaces 中的相应配置可能如下所示：
auto lo
iface lo inet loopback
iface eno1 inet manual
auto vmbr0
iface vmbr0 inet static
address 192.168.10.2/24
gateway 192.168.10.1
bridge-ports eno1
bridge-stp off
bridge-fd 0
虚拟机的行为就好像它们直接连接到物理网络一样。反过来，网络会将每个虚拟机视为拥有自己的 MAC，尽管只有一根网络电缆将所有这些虚拟机连接到网络。

3.3.5 路由配置
大多数托管提供商不支持上述设置。出于安全原因，当他们在单个接口上检测到多个 MAC 地址时，他们会禁用网络。
提示
某些提供商允许您通过其管理界面注册额外的 MAC。这可以避免问题，但是配置起来可能很麻烦，因为您需要为每个虚拟机注册一个 MAC。
您可以通过通过单个接口“路由”所有流量来避免此问题。这确保所有网络数据包使用相同的 MAC 地址。
一个常见的场景是，您有一个公共 IP（本例中假设为 198.51.100.5），以及一个为 VM 提供的额外 IP 块（203.0.113.16/28）。对于这种情况，我们建议使用以下设置：
auto lo
iface lo inet loopback
auto eno0
iface eno0 inet static
address 198.51.100.5/29
gateway 198.51.100.1
post-up echo 1 > /proc/sys/net/ipv4/ip_forward
post-up echo 1 > /proc/sys/net/ipv4/conf/eno0/proxy_arp
auto vmbr0
iface vmbr0 inet static
address 203.0.113.17/28
bridge-ports none
bridge-stp off
bridge-fd 0

3.3.6 使用 iptables 进行伪装 (NAT)
伪装允许只有私有 IP 地址的客户端通过使用主机 IP 地址访问网络进行出站流量。
每个出站数据包都被 iptables 重写，使其看起来像是来自主机，并相应地重写响应以将其路由到原始发送者。
auto lo
iface lo inet loopback
auto eno1
真实 IP 地址
iface eno1 inet static
address 198.51.100.5/24
gateway 198.51.100.1
auto vmbr0
私有子网
iface vmbr0 inet static
address 10.10.10.1/24
bridge-ports none
bridge-stp off
bridge-fd 0
post-up echo 1 > /proc/sys/net/ipv4/ip_forward
post-up iptables -t nat -A POSTROUTING -s '10.10.10.0/24' -o eno1 -j MASQUERADE
post-down iptables -t nat -D POSTROUTING -s '10.10.10.0/24' -o eno1 -j MASQUERADE
注意
在某些启用防火墙的伪装设置中，可能需要连接跟踪区域进行出站连接。否则，防火墙可能会阻止出站连接，因为它们将优先使用 VM 桥的 POSTROUTING（而不是伪装）。
在 /etc/network/interfaces 中添加以下行可以解决此问题：
post-up iptables -t raw -I PREROUTING -i fwbr+ -j CT --zone 1
post-down iptables -t raw -D PREROUTING -i fwbr+ -j CT --zone 1
有关此内容的更多信息，请参阅以下链接：
Netfilter 数据包流
在 netdev-list 上引入连接跟踪区域的补丁
使用 raw 表中的 TRACE 进行良好解释的博客文章

3.3.7 Linux Bond
绑定（也称为 NIC 组队或链路聚合）是一种将多个 NIC 绑定到单个网络设备的技术。通过它可以实现不同的目标，如使网络容错，提高性能或两者兼而有之。
像光纤通道这样的高速硬件及其相关的交换硬件可能相当昂贵。通过进行链路聚合，两个 NIC 可以作为一个逻辑接口，从而实现双倍速度。这是一种原生 Linux 
内核功能，得到了大多数交换机的支持。如果您的节点有多个以太网端口，您可以通过将网络电缆连接到不同的交换机来分布故障点，
而绑定的连接将在出现网络故障时故障切换到另一根电缆。
聚合链接可以减少实时迁移延迟，提高 Proxmox VE 集群节点之间的数据复制速度。
绑定有 7 种模式：
• 循环轮询（balance-rr）：从第一个可用网络接口（NIC）从属设备顺序传输网络数据包到最后一个。此模式提供负载均衡和容错能力。
• 主备模式（active-backup）：绑定中只有一个 NIC 从设备处于活动状态。只有在活动从设备故障时，另一个从设备才会变为活动状态。单个逻辑绑定接口的 MAC 地址仅在一个 
NIC（端口）上对外可见，以避免网络交换机中的失真。此模式提供容错能力。
• 异或（balance-xor）：根据 [(源 MAC 地址 XOR 目标 MAC 地址) 模 NIC 从设备计数] 传输网络数据包。此模式为每个目标 MAC 地址选择相同的 NIC 
从设备。此模式提供负载均衡和容错能力。
• 广播（broadcast）：在所有从属网络接口上传输网络数据包。此模式提供容错能力。
• IEEE 802.3ad 动态链路聚合（802.3ad）(LACP)：创建共享相同速度和双工设置的聚合组。根据 802.3ad 规范使用活动聚合组中的所有从属网络接口。
• 自适应传输负载平衡（balance-tlb）：不需要任何特殊网络交换机支持的 Linux 
绑定驱动程序模式。根据每个网络接口从设备的当前负载（相对于速度计算）分发传出网络数据包流量。传入流量由当前指定的从属网络接口接收。
如果接收从设备出现故障，另一个从设备会接管已故障接收从设备的 MAC 地址。
• 自适应负载平衡（balance-alb）：包括 balance-tlb 以及针对 IPV4 流量的接收负载平衡（rlb），无需任何特殊网络交换机支持。通过 ARP 
协商实现接收负载平衡。绑定驱动程序拦截本地系统发送的 ARP 回复，并将源硬件地址覆盖为单个逻辑绑定接口中的一个 NIC 从设备的唯一硬件地址，
从而使不同的网络对等方使用不同的 MAC 地址进行网络数据包传输。
如果您的交换机支持 LACP（IEEE 802.3ad）协议，那么我们建议使用相应的绑定模式（802.3ad）。否则，您通常应使用 active-backup 模式。
如果您打算在绑定接口上运行集群网络，那么您必须在绑定接口上使用 active-passive 模式，其他模式不受支持。
以下绑定配置可用作分布式/共享存储网络。优点是您可以获得更高的速度，而且网络将具有容错能力。
这个示例展示了如何使用固定 IP 地址配置网络绑定
auto lo
iface lo inet loopback
iface eno1 inet manual
iface eno2 inet manual
iface eno3 inet manual
auto bond0
iface bond0 inet static
bond-slaves eno1 eno2
address 192.168.1.2/24
bond-miimon 100
bond-mode 802.3ad
bond-xmit-hash-policy layer2+3
auto vmbr0
iface vmbr0 inet static
address 10.10.10.2/24
gateway 10.10.10.1
bridge-ports eno3
bridge-stp off
bridge-fd 0
将 bond 直接用作桥接端口是另一种可能性。这可以用于提高虚拟机网络的容错能力。
以下是使用 bond 作为桥接端口的示例：
auto lo
iface lo inet loopback
iface eno1 inet manual
iface eno2 inet manual
auto bond0
iface bond0 inet manual
bond-slaves eno1 eno2
bond-miimon 100
bond-mode 802.3ad
bond-xmit-hash-policy layer2+3
auto vmbr0
iface vmbr0 inet static
address 10.10.10.2/24
gateway 10.10.10.1
bridge-ports bond0
bridge-stp off
bridge-fd 0

3.3.8 VLAN 802.1Q
虚拟局域网（VLAN）是一个在二层网络中划分和隔离的广播域。因此，在一个物理网络中可以有多个网络（4096个），
每个网络彼此独立。每个VLAN网络都由一个通常称为标签的数字来识别。网络数据包会被打上标签以识别它们属于哪个虚拟网络。
针对客户机网络的VLAN
Proxmox VE支持开箱即用的设置。当您创建虚拟机时，可以指定VLAN标签。VLAN标签是客户机网络配置的一部分。根据桥接配置，网络层支持实现VLAN的不同模式：
    Linux桥上的VLAN感知：在这种情况下，每个客户机的虚拟网络卡会分配一个VLAN标签，该标签由Linux桥透明支持。Trunk模式也是可能的，但这需要在客户机中进行配置。
    Linux桥上的“传统”VLAN：与VLAN感知方法相反，此方法不透明，为每个VLAN创建一个带有关联桥接的VLAN设备。也就是说，例如在VLAN 
    5上创建一个客户机，会创建两个接口eno1.5和vmbr0v5，这些接口将一直保留，直到重新启动发生。
    Open vSwitch VLAN：此模式使用OVS VLAN功能。
    客户机配置的VLAN：VLAN在客户机内部分配。在这种情况下，设置完全在客户机内部完成，无法从外部影响。好处是您可以在单个虚拟网络接口上使用多个VLAN。
主机上的VLAN
为了允许主机与隔离网络进行通信，可以将VLAN标签应用于任何网络设备（NIC，Bond，Bridge）。通常，您应该在与物理NIC之间抽象层次最少的接口上配置VLAN。
例如，在默认配置中，您希望将主机管理地址放在单独的VLAN上。
示例：使用传统Linux桥在VLAN 5上为Proxmox VE管理IP。
auto lo
iface lo inet loopback
iface eno1 inet manual
iface eno1.5 inet manual
auto vmbr0v5
iface vmbr0v5 inet static
address 10.10.10.2/24
gateway 10.10.10.1
bridge-ports eno1.5
bridge-stp off
bridge-fd 0
auto vmbr0
iface vmbr0 inet manual
bridge-ports eno1
bridge-stp off
bridge-fd 0
示例：使用VLAN感知Linux桥在VLAN 5上为Proxmox VE管理IP。
auto lo
iface lo inet loopback
iface eno1 inet manual
auto vmbr0.5
iface vmbr0.5 inet static
address 10.10.10.2/24
gateway 10.10.10.1
auto vmbr0
iface vmbr0 inet manual
bridge-ports eno1
bridge-stp off
bridge-fd 0
bridge-vlan-aware yes
bridge-vids 2-4094
接下来的示例是相同的设置，但使用bond使该网络具有故障保护功能。
示例：使用传统Linux桥将VLAN 5与bond0一起用于Proxmox VE管理IP。
auto lo
iface lo inet loopback
iface eno1 inet manual
iface eno2 inet manual
auto bond0
iface bond0 inet manual
bond-slaves eno1 eno2
bond-miimon 100
bond-mode 802.3ad
bond-xmit-hash-policy layer2+3
iface bond0.5 inet manual
auto vmbr0v5
iface vmbr0v5 inet static
address 10.10.10.2/24
gateway 10.10.10.1
bridge-ports bond0.5
bridge-stp off
bridge-fd 0
auto vmbr0
iface vmbr0 inet manual
bridge-ports bond0
bridge-stp off
bridge-fd 0

3.3.9 在节点上禁用IPv6
无论是否部署了IPv6，Proxmox VE都可以在所有环境中正常工作。我们建议保留所有提供的默认设置。
如果您仍需要在节点上禁用对IPv6的支持，请通过创建适当的sysctl.conf（5）片段文件并设置适当的sysctl来实现，
例如添加/etc/sysctl.d/disable-ipv6.conf文件，并包含以下内容：
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
此方法优于在内核命令行上禁用加载IPv6模块。

3.3.10 在桥上禁用MAC学习
默认情况下，桥上启用了MAC学习，以确保虚拟客户端及其网络的流畅体验。
但在某些环境中，这可能是不需要的。从Proxmox VE 7.3开始，
您可以通过在“/etc/network/interfaces”中的桥上设置“bridge-disable-mac-learning 1”配置来禁用桥上的MAC学习，例如：
...
auto vmbr0
iface vmbr0 inet static
address 10.10.10.2/24
gateway 10.10.10.1
bridge-ports ens18
bridge-stp off
bridge-fd 0
bridge-disable-mac-learning 1
启用后，Proxmox VE将手动将虚拟机和容器的配置过的MAC地址添加到桥的转发数据库中，以确保客户端仍然可以使用网络——但前提是它们使用的是实际的MAC地址。

3.4 时间同步
Proxmox VE集群堆栈本身严重依赖于所有节点具有精确同步的时间。某些其他组件（如Ceph）也无法在所有节点的本地时间未同步的情况下正常工作。
节点之间的时间同步可以使用“网络时间协议”（NTP）实现。从Proxmox VE 7开始，chrony被用作默认的NTP守护程序，而Proxmox VE 
6使用systemd-timesyncd。两者都预先配置为使用一组公共服务器。
重要提示
如果将系统升级到Proxmox VE 7，建议您手动安装chrony，ntp或openntpd。

3.4.1 使用自定义NTP服务器
在某些情况下，可能需要使用非默认的NTP服务器。例如，如果您的Proxmox VE节点由于限制性的防火墙规则而无法访问公共互联网，您需要设置本地NTP服务器并告诉NTP守护程序使用它们。
对于使用chrony的系统：
在/etc/chrony/chrony.conf中指定chrony应使用的服务器：
server ntp1.example.com iburst
server ntp2.example.com iburst
server ntp3.example.com iburst
重新启动chrony：
systemctl restart chronyd
检查日志以确认正在使用新配置的NTP服务器：
# journalctl --since -1h -u chrony
...
Aug 26 13:00:09 node1 systemd[1]: Started chrony, an NTP client/server.
Aug 26 13:00:15 node1 chronyd[4873]: Selected source 10.0.0.1 (ntp1.example  -
.com)
Aug 26 13:00:15 node1 chronyd[4873]: System clock TAI offset set to 37  -
seconds
...
对于使用systemd-timesyncd的系统：
在/etc/systemd/timesyncd.conf中指定systemd-timesyncd应使用的服务器：
[Time]
NTP=ntp1.example.com ntp2.example.com ntp3.example.com ntp4.example.com
然后，重启同步服务（systemctl restart systemd-timesyncd），并通过检查日志来验证您的新配置的NTP服务器是否正在使用。
(journalctl --since -1h
-u systemd-timesyncd):
...
Oct 07 14:58:36 node1 systemd[1]: Stopping Network Time Synchronization...
Oct 07 14:58:36 node1 systemd[1]: Starting Network Time Synchronization...
Oct 07 14:58:36 node1 systemd[1]: Started Network Time Synchronization.
Oct 07 14:58:36 node1 systemd-timesyncd[13514]: Using NTP server  -
10.0.0.1:123 (ntp1.example.com).
Oct 07 14:58:36 node1 systemd-timesyncd[13514]: interval/delta/delay/jitter  -
/drift 64s/-0.002s/0.020s/0.000s/-31ppm
...

3.5 外部指标服务器
在Proxmox VE中，您可以定义外部指标服务器，这些服务器将定期接收有关您的主机、虚拟客户机和存储的各种统计信息。
目前支持的有：
• Graphite（请参阅 https://graphiteapp.org ）
• InfluxDB（请参阅 https://www.influxdata.com/time-series-platform/influxdb/ ）
外部指标服务器定义保存在 /etc/pve/status.cfg 中，可以通过Web界面进行编辑。

3.5.1 Graphite服务器配置
默认端口设置为2003，而默认的Graphite路径为proxmox。
默认情况下，Proxmox VE通过UDP发送数据，因此Graphite服务器必须配置为接受此类数据。
在此处，可以为不使用标准1500 MTU的环境配置最大传输单元（MTU）。
您还可以将插件配置为使用TCP。为了不阻塞重要的pvestatd统计收集守护程序，需要一个超时来应对网络问题。

3.5.2 Influxdb插件配置。Proxmox VE通过UDP发送数据，因此必须为Influxdb服务器进行配置。如果需要，也可以在此处配置MTU。
以下是Influxdb的示例配置（在您的Influxdb服务器上）：
[[udp]]
enabled = true
bind-address = "0.0.0.0:8089"
database = "proxmox"
batch-size = 1000
batch-timeout = "1s"
根据此配置，您的服务器将在所有IP地址上的8089端口上监听，并将数据写入proxmox数据库。
或者，该插件可以配置为使用InfluxDB 2.x的http(s) API。InfluxDB 1.8.x确实包含了这个v2 API的向前兼容的API端点。
要使用它，将influxdbproto设置为http或https（取决于您的配置）。
默认情况下，Proxmox VE使用proxmox组织和proxmox存储桶/数据库（它们可以分别使用配置组织和存储桶进行设置）。
由于InfluxDB的v2 API只能通过身份验证使用，因此您必须生成一个可以写入正确存储桶的令牌并设置它。
在1.8.x的v2兼容API中，您可以使用user:password作为令牌（如果需要），并可以省略组织，因为在InfluxDB 1.x中没有意义。
您还可以使用超时设置设置HTTP超时（默认为1s），以及使用max-body-size设置设置最大批量大小（默认为25000000字节）（这对应于同名的InfluxDB设置）。

3.6 磁盘健康监控
尽管建议使用强大且冗余的存储，但监视本地磁盘的健康状况可能非常有帮助。
从Proxmox VE 4.3开始，安装并需要使用smartmontools 1包。这是一组用于监控和控制本地硬盘的S.M.A.R.T.系统的工具。
您可以通过执行以下命令来获取磁盘的状态：
smartctl -a /dev/sdX
其中 /dev/sdX 是您本地磁盘的路径之一。
如果输出显示：
SMART support is: Disabled
您可以使用以下命令启用它：
smartctl -s on /dev/sdX
有关如何使用smartctl的更多信息，请参阅man smartctl。
默认情况下，smartmontools守护程序smartd处于活动状态并已启用，并且每30分钟扫描位于 /dev/sdX 和 /dev/hdX 的磁盘以查找错误和警告，如果检测到问题，则向root发送一封电子邮件。
有关如何配置smartd的更多信息，请参阅man smartd 和 man smartd.conf。
如果您将硬盘与硬件RAID控制器一起使用，那么很可能有一些工具可以监控RAID阵列中的磁盘以及阵列本身。有关此信息，请参阅您的RAID控制器的供应商。

3.7 逻辑卷管理器（LVM）
大多数人将Proxmox VE直接安装在本地磁盘上。Proxmox 
VE安装CD为本地磁盘管理提供了几个选项，当前的默认设置使用LVM。安装程序让您选择一个磁盘进行此类设置，并将该磁盘用作卷组（VG）pve的物理卷。
以下输出来自使用小型8GB磁盘的测试安装：
安装程序在此VG内分配了三个逻辑卷（LV）：
root
格式化为ext4，包含操作系统。
1smartmontools主页https://www.smartmontools.org
swap
交换分区
data
此卷使用LVM-thin，并用于存储VM映像。LVM-thin更适合执行此任务，因为它为快照和克隆提供了高效的支持。
对于Proxmox VE 4.1及更早版本，安装程序会创建一个名为“data”的标准逻辑卷，挂载在 /var/lib/vz。
从4.2版本开始，“data”逻辑卷是一个LVM-thin池，用于存储基于块的客户端映像，/var/lib/vz只是根文件系统上的一个目录。

3.7.1 硬件
我们强烈建议在这种设置中使用具有BBU的硬件RAID控制器。这将提高性能，提供冗余，并使磁盘更换更容易（热插拔）。
LVM本身不需要任何特殊的硬件，内存要求非常低。

3.7.2 引导加载程序
默认情况下，我们安装了两个引导加载程序。第一个分区包含标准的GRUB引导加载程序。第二个分区是EFI系统分区（ESP），使其可以在EFI系统上引导。

3.7.3 创建卷组
假设我们有一个空磁盘 /dev/sdb，我们想在其上创建一个名为“vmdata”的卷组。
警告
请注意，以下命令将销毁 /dev/sdb 上的所有现有数据。
首先创建一个分区。
sgdisk -N 1 /dev/sdb
在没有确认和250K元数据大小的情况下创建物理卷（PV）。
pvcreate --metadatasize 250k -y -ff /dev/sdb1
在 /dev/sdb1 上创建名为“vmdata”的卷组
vgcreate vmdata /dev/sdb1

3.7.4 为 /var/lib/vz 创建额外的LV
通过创建一个新的细分LV可以轻松完成。
lvcreate -n <Name> -V <Size[M,G,T]> <VG>/<LVThin_pool>
一个真实世界的例子：
lvcreate -n vz -V 10G pve/data
现在必须在LV上创建一个文件系统。
mkfs.ext4 /dev/pve/vz
最后，这必须被挂载。
警告
确保 /var/lib/vz 是空的。在默认安装中，它不是空的。
要使其始终可访问，在 /etc/fstab 中添加以下行。
echo ’/dev/pve/vz /var/lib/vz ext4 defaults 0 2’ >> /etc/fstab

3.7.5 调整thin pool的大小
使用以下命令调整LV和元数据池的大小：
lvresize --size +<size[\M,G,T]> --poolmetadatasize +<size[\M,G]> < -
VG>/<LVThin_pool>
注意
在扩展数据池时，元数据池也必须扩展。

3.7.6 创建LVM-thin池
一个thin池必须在卷组的基础上创建。如何创建卷组，请参见LVM部分。
lvcreate -L 80G -T -n vmstore vmdata

3.8 ZFS on Linux
ZFS 是 Sun Microsystems 设计的一种组合文件系统和逻辑卷管理器。从 Proxmox VE 3.4 开始，作为可选文件系统以及根文件系统的附加选择，
引入了 ZFS 文件系统的本机 Linux 内核端口。无需手动编译 ZFS 模块 - 所有软件包都已包含。
通过使用 ZFS，可以通过低预算硬件实现最大的企业功能，但也可以通过利用 SSD 缓存甚至仅使用 SSD 设置实现高性能系统。ZFS 可以通过适度的 CPU 
和内存负载以及简便的管理来替换成本高的硬件 RAID 卡。
ZFS的一般优势
• 使用 Proxmox VE GUI 和 CLI 进行简单的配置和管理。
• 可靠
• 防止数据损坏
• 文件系统级数据压缩
• 快照
• 写时复制克隆
• 多种 RAID 级别：RAID0、RAID1、RAID10、RAIDZ-1、RAIDZ-2、RAIDZ-3、dRAID、dRAID2、dRAID3
• 可以使用 SSD 进行缓存
• 自我修复
• 持续完整性检查
• 为大容量存储而设计
• 异步网络复制
• 开源
• 加密
• . . .

3.8.1 硬件
ZFS 严重依赖内存，因此至少需要 8GB 内存才能启动。在实践中，根据您的硬件/预算，尽可能多地使用内存。为了防止数据损坏，我们建议使用高质量的 ECC RAM。
如果您使用专用缓存和/或日志磁盘，则应使用企业级 SSD。这可以显著提高整体性能。
重要提示
不要在具有自己的缓存管理的硬件 RAID 控制器上使用 ZFS。ZFS 需要直接与磁盘通信。HBA 适配器或类似于以 “IT” 模式刷新的 LSI 控制器更合适。
如果您正在尝试在 VM 中安装 Proxmox VE（嵌套虚拟化），请不要对该 VM 使用 virtio 磁盘，因为 ZFS 不支持它们。请改用 IDE 或 SCSI（也适用于 virtio SCSI 控制器类型）。

3.8.2 作为根文件系统的安装
当您使用 Proxmox VE 安装程序安装时，可以选择将 ZFS 用作根文件系统。您需要在安装时选择 RAID 类型：
RAID0 也称为“条带化”。这种卷的容量是所有磁盘容量之和。但 RAID0 不增加任何冗余，因此单个驱动器的故障会导致卷无法使用。
RAID1 也称为“镜像”。数据被完全相同地写入所有磁盘。此模式至少需要2个相同大小的磁盘。产生的容量等于单个磁盘的容量。
RAID10 是 RAID0 和 RAID1 的组合。至少需要 4 个磁盘。
RAIDZ-1 是 RAID-5 的变种，单奇偶校验。至少需要 3 个磁盘。
RAIDZ-2 是 RAID-5 的变种，双奇偶校验。至少需要 4 个磁盘。
RAIDZ-3 是 RAID-5 的变种，三重奇偶校验。至少需要 5 个磁盘。
安装程序会自动对磁盘进行分区，创建一个名为 rpool 的 ZFS 池，并在 ZFS 子卷 rpool/ROOT/pve-1 上安装根文件系统。
创建另一个名为 rpool/data 的子卷来存储 VM 映像。为了与 Proxmox VE 工具一起使用，安装程序在 /etc/pve/storage.cfg 中创建以下配置条目：
zfspool: local-zfs
pool rpool/data
sparse
content images,rootdir
安装后，您可以使用 zpool 命令查看您的 ZFS 池状态：
# zpool status
pool: rpool
state: ONLINE
scan: none requested
config:
NAME STATE READ WRITE CKSUM
rpool ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
sda2 ONLINE 0 0 0
sdb2 ONLINE 0 0 0
mirror-1 ONLINE 0 0 0
sdc ONLINE 0 0 0
sdd ONLINE 0 0 0
errors: No known data errors
zfs 命令用于配置和管理您的 ZFS 文件系统。以下命令在安装后列出所有文件系统：

3.8.3 ZFS RAID 级别考虑因素
在选择 ZFS 池的布局时，有一些因素需要考虑。ZFS 池的基本构建模块是虚拟设备，或称为 vdev。
池中的所有 vdev 都被平等使用，数据在它们之间分条存储（RAID0）。有关 vdevs 的更多详细信息，请查阅 zpool(8) 手册页。
性能
每种 vdev 类型具有不同的性能特性。感兴趣的两个参数是 IOPS（每秒输入/输出操作数）和可以写入或读取数据的带宽。
在写入数据时，镜像 vdev（RAID1）在这两个参数方面的表现大致类似于单个磁盘。在读取数据时，性能将随镜像中磁盘数量线性增长。
一个常见的情况是拥有 4 个磁盘。将其设置为 2 个镜像 vdev（RAID10）时，池在 IOPS 和带宽方面的写特性将类似于两个单独的磁盘。对于读操作，它将类似于 4 个单独的磁盘。
任何冗余级别的 RAIDZ 大约在 IOPS 方面的表现类似于单个磁盘，但带宽很大。具体有多少带宽取决于 RAIDZ vdev 的大小和冗余级别。
对于运行 VM，IOPS 在大多数情况下是更重要的指标。
大小、空间使用和冗余
虽然由镜像 vdev 组成的池将具有最佳性能特性，但可用空间将为磁盘可用空间的 50%。如果镜像 vdev 由多于 2 个磁盘组成，例如在 3 路镜像中，可用空间会更少。
为了保持池的正常运行，每个镜像至少需要一个健康磁盘。
RAIDZ 类型 vdev 的可用空间大约为 N-P，其中 N 为磁盘数，P 为 RAIDZ 级别。RAIDZ 级别表示在不丢失数据的情况下可以失败的任意磁盘数量。
一个特殊情况是一个具有 RAIDZ2 的 4 磁盘池。在这种情况下，通常最好使用 2 个镜像 vdev 以获得更好的性能，因为可用空间将相同。
在使用任何 RAIDZ 级别时，另一个重要因素是 ZVOL 数据集（用于 VM 磁盘）的行为。对于每个数据块，池需要至少由池的 ashift 值定义的最小块大小的奇偶校验数据。
具有 12 的 ashift 的池的块大小为 4k。ZVOL 的默认块大小为 8k。因此，在 RAIDZ2 中，每写入一个 8k 块，将导致写入两个额外的 4k 奇偶校验块，
8k + 4k + 4k = 16k。当然，这是一种简化方法，实际情况会因元数据、压缩等因素的考虑而略有不同。
可以在检查 ZVOL 的以下属性时观察到这种行为：
• volsize
• refreservation（如果池没有使用精简配置）
• used（如果池使用了精简配置且没有快照存在）
zfs get volsize,refreservation,used <pool>/vm-<vmid>-disk-X
volsize 是磁盘在 VM 中显示的大小，而 refreservation 显示了包括预期奇偶校验数据所需空间在内的池上保留的空间。如果池进行了精简配置，
refreservation 将设置为 0。观察行为的另一种方法是比较 VM 内的已使用磁盘空间和已使用属性。请注意，快照会使数值产生偏差。
有一些方法可以解决空间使用增加的问题：
• 增加 volblocksize 以提高数据与奇偶校验的比例
• 使用镜像 vdev 代替 RAIDZ
• 使用 ashift=9（块大小为 512 字节）
只有在创建 ZVOL 时才能设置 volblocksize 属性。默认值可以在存储配置中更改。这样做时，需要相应地调整客户端，并根据用例，将写放大问题从 ZFS 层移到客户端。
在创建池时使用 ashift=9 可能会导致性能不佳，具体取决于底层磁盘，且之后无法更改。
镜像 vdev（RAID1、RAID10）对于 VM 负载具有有利的行为。除非您的环境具有特定需求和特征，在这种情况下，RAIDZ 的性能特性是可以接受的，否则请使用它们。

3.8.4 ZFS dRAID
在 ZFS dRAID（分散式 RAID）中，热备份驱动器参与 RAID。当一个驱动器发生故障时，它们的备用容量会被保留并用于重建。这根据配置，与 RAIDZ 
相比，在驱动器故障时提供更快的重建。您可以在官方 OpenZFS 文档中找到更多信息。2
注意
dRAID 适用于一个 dRAID 中的 10-15 个磁盘以上。在大多数用例中，较少数量的磁盘应该使用 RAIDZ 设置。
注意
GUI 需要比最小数量多一个磁盘（即 dRAID1 需要 3 个）。它还期望添加一个备用磁盘。
2OpenZFS dRAID https://openzfs.github.io/openzfs-docs/Basic%20Concepts/dRAID%20Howto.html
• dRAID1 或 dRAID：至少需要 2 个磁盘，在数据丢失前一个可以发生故障
• dRAID2：至少需要 3 个磁盘，在数据丢失前两个可以发生故障
• dRAID3：至少需要 4 个磁盘，在数据丢失前三个可以发生故障
可以在手册页面上找到更多信息：
man zpoolconcepts
备用和数据
备用数量告诉系统在磁盘发生故障时应准备好多少个磁盘。默认值为 0 备用。没有备用磁盘，重建速度不会有任何优势。
数据定义了一个冗余组中的设备数量。默认值为 8。除非磁盘 - 奇偶校验 - 备用等于小于 8 的数字，否则将使用较小的数字。通常，较少的数据设备会导致更高的 
IOPS、更好的压缩比和更快的重银，但定义较少的数据设备会减少池的可用存储容量。

3.8.5 引导加载程序
Proxmox VE 使用 proxmox-boot-tool（第 3.12.2 节）来管理引导加载程序配置。请参阅关于 Proxmox VE 主机引导加载程序的章节（第 3.12 节）了解详细信息。

3.8.6 ZFS 管理
本节为您提供一些常见任务的使用示例。ZFS 本身非常强大，提供了许多选项。管理 ZFS 的主要命令是 zfs 和 zpool。这两个命令都带有很好的手册页面，可以通过以下方式阅读：
man zpool
man zfs
创建一个新的 zpool
要创建一个新的池，至少需要一个磁盘。ashift 应该与底层磁盘的扇区大小（2 的 ashift 次方）相同或更大。
zpool create -f -o ashift=12 <pool> <device>
提示
池名称必须遵循以下规则：
• 以字母开头（a-z 或 A-Z）
• 只包含字母数字、-、_、.、: 或空格字符
• 不能以 mirror、raidz、draid 或 spare 开头
• 不能是 log
要激活压缩（请参阅 ZFS 中的压缩部分）：
zfs set compression=lz4 <pool>
使用 RAID-0 创建一个新池
最少需要 1 个磁盘
zpool create -f -o ashift=12 <pool> <device1> <device2>
创建一个新的带有 RAID-1 的池
最少需要 2 个磁盘
zpool create -f -o ashift=12 <pool> mirror <device1> <device2>
创建一个新的带有 RAID-10 的池
最少需要 4 个磁盘
zpool create -f -o ashift=12 <pool> mirror <device1> <device2> mirror <device3> <device4>
创建一个新的带有 RAIDZ-1 的池
最少需要 3 个磁盘
zpool create -f -o ashift=12 <pool> raidz1 <device1> <device2> <device3>
创建一个新的带有 RAIDZ-2 的池
最少需要 4 个磁盘
zpool create -f -o ashift=12 <pool> raidz2 <device1> <device2> <device3> <device4>
创建一个带有缓存（L2ARC）的新池
可以使用专用缓存驱动器分区来提高性能（使用 SSD）。
作为 <device>，可以使用更多设备，如"创建一个新的带有 RAID* 的池"所示。
zpool create -f -o ashift=12 <pool> <device> cache <cache_device>
创建一个带有日志（ZIL）的新池
可以使用专用缓存驱动器分区来提高性能（使用 SSD）。
作为 <device>，可以使用更多设备，如"创建一个新的带有 RAID* 的池"所示。
zpool create -f -o ashift=12 <pool> <device> log <log_device>
将缓存和日志添加到现有池
如果您的池没有缓存和日志，请先使用 parted 或 gdisk 在 SSD 上创建两个分区。
重要提示
始终使用 GPT 分区表。
日志设备的最大大小应该是物理内存大小的一半左右，所以通常相当小。SSD 的其余部分可以用作缓存。
zpool add -f <pool> log <device-part1> cache <device-part2>
更换失败的设备
zpool replace -f <pool> <old device> <new device>
更换失败的可引导设备
根据 Proxmox VE 的安装方式，它要么使用 systemd-boot 或通过 proxmox-boot-3 的 grub 或纯粹的 grub 作为引导加载程序
（请参阅主机引导加载程序第 3.12 节）。您可以通过运行以下命令进行检查：
proxmox-boot-tool status
复制分区表、重新分配 GUID 和替换 ZFS 分区的第一步相同。
要使系统能从新磁盘启动，需要执行不同的步骤，这取决于使用的引导加载程序。
sgdisk <healthy bootable device> -R <new device>
sgdisk -G <new device>
zpool replace -f <pool> <old zfs partition> <new zfs partition>
注意
使用 zpool status -v 命令来监视新磁盘的重银行过程进度。
使用 proxmox-boot-tool：
proxmox-boot-tool format <new disk’s ESP>
proxmox-boot-tool init <new disk’s ESP>
注意
ESP 是 EFI 系统分区的缩写，在 Proxmox VE 5.4 版本安装程序设置的可引导磁盘上设置为分区＃2。有关详细信息，请参阅用作同步 ESP 的新分区的设置。
使用普通的 grub：
grub-install <new disk>
注意
普通的 grub 仅用于安装了 Proxmox VE 6.3 或更早版本的系统，这些系统尚未手动迁移到使用 proxmox-boot-tool。

3.8.7 配置电子邮件通知
ZFS 附带了一个事件守护程序 ZED，用于监控 ZFS 内核模块生成的事件。守护程序还可以在发生诸如池错误之类的 ZFS 事件时发送电子邮件。
较新的 ZFS 软件包在单独的 zfs-zed 软件包中提供守护程序，这在 Proxmox VE 中应该默认已经安装。
您可以使用您喜欢的编辑器通过文件 /etc/zfs/zed.d/zed.rc 配置守护程序。电子邮件通知所需的设置是 ZED_EMAIL_ADDR，默认设置为 root。
ZED_EMAIL_ADDR="root"
请注意，Proxmox VE 将邮件转发给 root 用户配置的电子邮件地址。

3.8.8 限制 ZFS 内存使用
默认情况下，ZFS 将主机内存的 50% 用于自适应替换缓存（ARC）。为 ARC 分配足够的内存对于 IO 性能至关重要，因此请谨慎减少。
作为一个经验法则，分配至少 2 GiB 基本 + 1 GiB/TiB-存储。例如，如果您有一个可用存储空间为 8 TiB 的池，那么您应该为 ARC 使用 10 GiB 的内存。
您可以通过直接写入 zfs_arc_max 模块参数来更改当前引导的 ARC 使用限制（重新启动会再次重置此更改）：
echo "$[10 * 102410241024]" >/sys/module/zfs/parameters/zfs_arc_max
要永久更改 ARC 限制，请将以下行添加到 /etc/modprobe.d/zfs.conf：
options zfs zfs_arc_max=8589934592
此示例设置将使用限制为 8 GiB（8 * 2^30）。
重要提示
如果您期望的 zfs_arc_max 值小于或等于 zfs_arc_min（默认为系统内存的 1/32），则除非您还将 zfs_arc_min 设置为最多 zfs_arc_max - 1，否则将忽略 zfs_arc_max。
echo "$[8 * 102410241024 - 1]" >/sys/module/zfs/parameters/zfs_arc_min
echo "$[8 * 102410241024]" >/sys/module/zfs/parameters/zfs_arc_max
此示例设置（临时）将具有超过 256 GiB 总内存的系统的使用限制为 8 GiB（8 * 2^30），其中仅设置 zfs_arc_max 将无法正常工作。
重要提示
如果您的根文件系统是 ZFS，则每次此值更改时，您必须更新 initramfs：
update-initramfs -u -k all
您必须重新启动以激活这些更改。

3.8.9 ZFS 上的 SWAP
在 zvol 上创建的交换空间可能会引发一些问题，例如阻止服务器或产生高 IO 负载，通常在启动到外部存储的备份时看到。
我们强烈建议使用足够的内存，以便您通常不会遇到内存不足的情况。如果您需要或想要添加交换空间，最好在物理磁盘上创建一个分区并将其用作交换设备。
您可以在安装程序的高级选项中为此目的预留一些空闲空间。
此外，您可以降低“swappiness”值。服务器的一个好值是 10：
sysctl -w vm.swappiness=10
要使 swappiness 持久化，请用您选择的编辑器打开 /etc/sysctl.conf 并添加以下行：
vm.swappiness = 10
Table 3.1: Linux kernel swappiness parameter values
Value Strategy
vm.swappiness = 0 The kernel will swap only to avoid an out of memory condition
vm.swappiness = 1 Minimum amount of swapping without disabling it entirely.
vm.swappiness = 10 This value is sometimes recommended to improve performance
when sufficient memory exists in a system.
vm.swappiness = 60 The default value.
vm.swappiness = 100 The kernel will swap aggressively.

3.8.10 加密 ZFS 数据集
警告
Proxmox VE 中的原生 ZFS 加密处于实验阶段。已知的限制和问题包括使用加密数据集的复制 a 以及在使用快照或 ZVOL 时出现的校验和错误。b
a https://bugzilla.proxmox.com/show_bug.cgi?id=2350
b https://github.com/openzfs/zfs/issues/11688
ZFS on Linux 版本 0.8.0 引入了对数据集的本地加密的支持。从之前的 ZFS on Linux 版本升级后，可以为每个存储池启用加密功能：
zpool get feature@encryption tank
NAME PROPERTY VALUE SOURCE
tank feature@encryption disabled local
zpool set feature@encryption=enabled
zpool get feature@encryption tank
NAME PROPERTY VALUE SOURCE
tank feature@encryption enabled local
警告
目前 Grub 不支持从使用加密数据集的存储池引导，自动解锁加密数据集在引导时的支持也有限。没有加密支持的旧版 ZFS 将无法解密存储的数据。
注意
建议在引导后手动解锁存储数据集，或编写一个自定义单元，将引导时解锁所需的密钥材料传递给 zfs load-key。
警告
在启用生产数据的加密之前，请建立并测试备份程序。如果关联的密钥材料/密码短语/密钥文件丢失，将无法再访问加密数据。
在创建数据集/zvols 时需要设置加密，并默认继承给子数据集。例如，要创建加密数据集 tank/encrypted_data 并将其配置为 Proxmox VE 的存储，请运行以下命令：
zfs create -o encryption=on -o keyformat=passphrase tank/encrypted_data
输入密码短语：
重新输入密码短语：
pvesm add zfspool encrypted_zfs -pool tank/encrypted_data
在此存储上创建的所有客户端卷/磁盘将使用父数据集的共享密钥材料进行加密。
要实际使用存储，需要加载关联的密钥材料并挂载数据集。这可以通过以下一步完成：
zfs mount -l tank/encrypted_data
输入 'tank/encrypted_data' 的密码短语：
通过设置 keylocation 和 keyformat 属性，还可以在创建时或在现有数据集上使用 zfs change-key 代替提示输入密码短语，使用（随机）密钥文件：
dd if=/dev/urandom of=/path/to/keyfile bs=32 count=1
zfs change-key -o keyformat=raw -o keylocation=file:///path/to/keyfile tank/encrypted_data
警告
使用密钥文件时，需要特别注意保护密钥文件免受未经授权访问或意外丢失。没有密钥文件，就无法访问明文数据！
在加密数据集下创建的客户端卷将相应地设置其 encryptionroot 属性。只需为每个 encryptionroot 加载一次密钥材料，即可提供给其下的所有加密数据集。
请参阅 encryptionroot、encryption、keylocation、keyformat 和 keystatus 属性，以及 zfs load-key、zfs unload-key 和 zfs change-key 命令以及 man zfs 
中的加密部分，以获取更多详细信息和高级用法。

3.8.11 ZFS 中的压缩
当在数据集上启用压缩时，ZFS 会在写入之前尝试压缩所有新块，并在读取时解压缩它们。已经存在的数据不会事后被压缩。
您可以使用以下命令启用压缩：
zfs set compression=<algorithm> <dataset>
我们建议使用 lz4 算法，因为它对 CPU 开销非常小。其他算法，如 lzjb 和 gzip-N（其中 N 是从 1（最快）到 9（最佳压缩比）的整数），也是可用的。
根据算法和数据的可压缩性，启用压缩甚至可以提高 I/O 性能。
您可以随时使用以下命令禁用压缩：
zfs set compression=off <dataset>
同样，此更改仅影响新块。

3.8.12 ZFS 特殊设备
自 0.8.0 版本起，ZFS 支持特殊设备。池中的特殊设备用于存储元数据、重复数据删除表以及可选的小文件块。
特殊设备可以提高由许多元数据更改组成的缓慢旋转硬盘池的速度。例如，涉及创建、更新或删除大量文件的工作负载将受益于特殊设备的存在。ZFS 
数据集还可以配置为在特殊设备上存储整个小文件，从而进一步提高性能。为特殊设备使用快速的 SSD。
重要提示
特殊设备的冗余应与池的冗余相匹配，因为特殊设备是整个池的故障点。
警告
向池中添加特殊设备无法撤销！
使用 RAID-1 和特殊设备创建池：
zpool create -f -o ashift=12 <pool> mirror <device1> <device2> special mirror <device3> <device4>
将 RAID-1 的特殊设备添加到现有池中：
zpool add <pool> special mirror <device1> <device2>
ZFS 数据集暴露了 special_small_blocks=<size> 属性。size 可以是 0，以禁用在特殊设备上存储小文件块，
或者在 512B 到 1M 之间的 2 的幂。设置属性后，小于 size 的新文件块将在特殊设备上分配。
重要提示
如果 special_small_blocks 的值大于或等于数据集的 recordsize（默认 128K），则所有数据都将写入特殊设备，因此要小心！
在池上设置 special_small_blocks 属性将更改该属性的默认值，以便所有子 ZFS 数据集（例如，池中的所有容器将选择小文件块）。
在整个池范围内选择所有小于 4K 块的文件：
zfs set special_small_blocks=4K <pool>
针对单个数据集选择小文件块：
zfs set special_small_blocks=4K <pool>/<filesystem>
针对单个数据集取消选择小文件块：
zfs set special_small_blocks=0 <pool>/<filesystem>

3.8.13 ZFS 池功能
ZFS 中磁盘格式的更改仅在主版本更改之间进行，并通过功能进行指定。所有功能以及通用机制都在 zpool-features（手册页）中有详细记录。
由于启用新功能可能会使池不能被旧版本的 ZFS 导入，因此管理员需要通过在池上运行 zpool upgrade（参见 zpool-upgrade（8）手册页）来主动进行此操作。
除非您需要使用新功能，否则启用它们没有好处。
实际上，启用新功能有一些缺点：
• 如果在 rpool 上启用了新功能，使用 grub 引导的具有 ZFS 根文件系统的系统将无法启动，因为 grub 中的 ZFS 实现不兼容。
• 使用仍附带旧 ZFS 模块的旧内核引导的系统将无法导入任何升级后的池。
• 使用旧的 Proxmox VE ISO 启动以修复无法启动的系统同样不起作用。
重要提示
如果您的系统仍然使用 grub 引导，请不要升级您的 rpool，因为这将使您的系统无法启动。这包括在 Proxmox VE 5.4 之前安装的系统以及使用传统 BIOS 
引导的系统（参见如何确定引导加载程序第 3.12.3 节）。
为 ZFS 池启用新功能：
zpool upgrade <pool>

3.9 BTRFS
警告
BTRFS 集成目前在 Proxmox VE 中处于技术预览阶段。
BTRFS 是一种现代的写时复制文件系统，由 Linux 内核原生支持，实现了诸如快照、内置 RAID 和通过数据和元数据的校验和进行自我修复等功能。
从 Proxmox VE 7.0 开始，BTRFS 作为可选项引入根文件系统。
BTRFS 的一般优势
• 主系统设置与传统基于 ext4 的设置几乎相同
• 快照
• 文件系统级数据压缩
• 写时复制克隆
• RAID0、RAID1 和 RAID10
• 防止数据损坏
• 自我修复
• 由 Linux 内核原生支持
• ...
注意事项
• RAID 5/6 级别处于实验阶段，存在风险

3.9.1 作为根文件系统安装
当您使用 Proxmox VE 安装程序安装时，可以为根文件系统选择 BTRFS。您需要在安装时选择 RAID 类型：
RAID0 也称为“条带化”。这种卷的容量是所有磁盘容量的总和。但 RAID0 不增加任何冗余，因此单个驱动器的故障会使卷无法使用。
RAID1 也称为“镜像”。数据被相同地写入所有磁盘。这种模式需要至少 2 个相同大小的磁盘。生成的容量等于单个磁盘的容量。
RAID10 是 RAID0 和 RAID1 的组合。需要至少 4 个磁盘。
安装程序会自动分区磁盘，并在 /var/lib/pve/local-btrfs 下创建一个额外的子卷。为了使用 Proxmox VE 工具，安装程序会在以下配置条目中创建以下内容：
/etc/pve/storage.cfg:
dir: local
path /var/lib/vz
content iso,vztmpl,backup
disable
Proxmox VE Administration Guide 61 / 534
btrfs: local-btrfs
path /var/lib/pve/local-btrfs
content iso,vztmpl,backup,images,rootdir
这明确地禁用了默认的本地存储，以支持额外子卷上的 btrfs 特定存储条目。
btrfs 命令用于配置和管理 btrfs 文件系统。安装后，以下命令列出了所有附加子卷：
btrfs subvolume list /
ID 256 gen 6 top level 5 path var/lib/pve/local-btrfs

3.9.2 BTRFS 管理
本节为您提供了一些常见任务的使用示例。
创建 BTRFS 文件系统
要创建 BTRFS 文件系统，请使用 mkfs.btrfs。-d 和 -m 参数分别用于设置元数据和数据的配置文件。通过可选的 -L 参数，可以设置标签。
通常，支持以下模式：single、raid0、raid1、raid10。
在单个磁盘 /dev/sdb 上创建带有标签 My-Storage 的 BTRFS 文件系统：
mkfs.btrfs -m single -d single -L My-Storage /dev/sdb
或者在两个分区 /dev/sdb1 和 /dev/sdc1 上创建 RAID1：
mkfs.btrfs -m raid1 -d raid1 -L My-Storage /dev/sdb1 /dev/sdc1
挂载 BTRFS 文件系统
然后可以手动挂载新的文件系统，例如：
mkdir /my-storage
mount /dev/sdb /my-storage
BTRFS 也可以像任何其他挂载点一样添加到 /etc/fstab 中，在启动时自动挂载。
建议避免使用块设备路径，而是使用 mkfs.btrfs 命令打印的 UUID 值，尤其是在 BTRFS 设置中有多个磁盘的情况下。
例如：
文件 /etc/fstab
# ... other mount points left out for brevity
# using the UUID from the mkfs.btrfs output is highly recommended
UUID=e2c0c3ff-2114-4f54-b767-3a203e49f6f3 /my-storage btrfs defaults 0 0
提示
如果您不再有 UUID 可用，您可以使用 blkid 工具列出所有块设备的属性。
然后，您可以通过执行以下命令触发第一次挂载：
mount /my-storage
在下次重启后，系统在启动时会自动完成此操作。
将 BTRFS 文件系统添加到 Proxmox VE
您可以通过 Web 界面或使用 CLI 将现有的 BTRFS 文件系统添加到 Proxmox VE，例如：
pvesm add btrfs my-storage --path /my-storage
创建子卷
创建子卷会将其链接到 btrfs 文件系统中的一个路径，该路径将作为常规目录显示。
btrfs subvolume create /some/path
之后，/some/path 将像常规目录一样工作。
删除子卷
与通过 rmdir 删除的目录相反，子卷不需要为空才能通过 btrfs 命令删除。
btrfs subvolume delete /some/path
创建子卷的快照
BTRFS 实际上并未区分快照和普通子卷，因此，获取快照也可以看作是创建子卷的任意副本。
按照惯例，Proxmox VE 在创建客户端磁盘或子卷的快照时会使用只读标志，但稍后可以更改此标志。
btrfs subvolume snapshot -r /some/path /a/new/path
这将在 /a/new/path 创建一个只读的 /some/path 子卷的“克隆”。对 /some/path 的任何未来修改都会在修改之前复制修改过的数据。
如果省略只读（-r）选项，则两个子卷都将可写。
启用压缩
默认情况下，BTRFS 不会压缩数据。要启用压缩，可以添加 compress 挂载选项。请注意，已写入的数据在事后不会被压缩。
默认情况下，rootfs 将在 /etc/fstab 中列出如下：
UUID=<uuid of your root file system> / btrfs defaults 0 1
您可以简单地将 compress=zstd、compress=lzo 或 compress=zlib 追加到上述默认值中，如下所示：
UUID=<uuid of your root file system> / btrfs defaults,compress=zstd 0 1
这个更改将在重启后生效。
检查空间使用情况
对于某些 btrfs 设置，经典的 df 工具可能输出令人困惑的值。为了更好地估算，请使用 btrfs 文件系统使用情况 /PATH 命令，例如：
btrfs fi usage /my-storage

3.10 Proxmox 节点管理
Proxmox VE 节点管理工具（pvenode）允许您控制节点特定的设置和资源。
目前，pvenode 允许您设置节点的描述、在节点的客户机上运行各种批量操作、查看节点的任务历史记录以及管理节点的 SSL 证书，
这些证书用于通过 pveproxy 进行 API 和 web GUI。

3.10.1 网络唤醒
Wake-on-LAN（WoL）允许您通过发送一个魔术数据包在网络中打开一个休眠的计算机。至少有一个 NIC 必须支持这个功能，并且相应的选项需要在计算机的固件（BIOS/
UEFI）配置中启用。选项名称可以从 Enable Wake-on-Lan 变为 Power On By PCIE Device；如果您不确定，请查阅主板厂商的手册。
ethtool 可用于通过运行以下命令检查 <interface> 的 WoL 配置：
ethtool <interface> | grep Wake-on
pvenode 允许您通过 WoL 唤醒集群中的休眠成员，使用命令：
pvenode wakeonlan <node>
这将在 UDP 端口 9 上广播包含从 wakeonlan 属性获得的 <node> 的 MAC 地址的 WoL 魔术包。可以使用以下命令设置节点特定的 wakeonlan 属性：
pvenode config set -wakeonlan XX:XX:XX:XX:XX:XX

3.10.2 任务历史记录
在排查服务器问题时（例如，失败的备份任务），通常查看以前运行的任务日志会很有帮助。使用 Proxmox VE，您可以通过 pvenode 任务命令访问节点的任务历史记录。
您可以使用 list 子命令获取节点已完成任务的过滤列表。例如，要获取与 VM 100 相关的已结束并出现错误的任务列表，命令如下：
pvenode task list --errors --vmid 100
然后，可以使用其 UPID 打印任务日志：
pvenode task log UPID:pve1:00010D94:001CA6EA:6124E1B9:vzdump:100:root@pam:

3.10.3 批量客户机电源管理
如果您有很多 VM/容器，可以使用 pvenode 的 startall 和 stopall 子命令进行批量操作来启动和停止客户机。
默认情况下，pvenode startall 只会启动设置为自动启动的 VM/容器（请参见 虚拟机的自动启动和关闭 第 10.2.16 节），
但是，您可以使用 --force 标志覆盖此行为。这两个命令还有一个 --vms 选项，该选项将停止/启动的客户机限制为指定的 VMID。
例如，要启动 VM 100、101 和 102，无论它们是否设置了 onboot，您可以使用：
pvenode startall --vms 100,101,102 --force
要停止这些客户机（以及可能正在运行的其他客户机），请使用以下命令：
pvenode stopall
注意
stopall 命令首先尝试执行干净的关机，然后等待，直到所有客户机成功关机或可覆盖的超时（默认为 3 分钟）到期。一旦发生这种情况并且 force-stop 参数未明确设置为 
0（false），所有仍在运行的虚拟客户机将被强制停止。

3.10.4 第一个客户机启动延迟
如果您的 VM/容器依赖于启动缓慢的外部资源（例如，NFS 服务器），您还可以为每个节点设置一个延迟，
该延迟表示 Proxmox VE 启动和配置为自动启动的第一个 VM/容器启动之间的时间（请参见 虚拟机的自动启动和关闭 第 10.2.16 节）。
您可以通过设置以下内容来实现这一点（其中 10 代表以秒为单位的延迟）：
pvenode config set --startall-onboot-delay 10

3.10.5 批量客户机迁移
在升级情况下需要将所有客户机从一个节点迁移到另一个节点时，pvenode 还提供了用于批量迁移的 migrateall 
子命令。默认情况下，此命令将系统上的每个客户机迁移到目标节点。但是，它可以设置为仅迁移一组客户机。
例如，要将 VM 100、101 和 102 迁移到节点 pve2，并启用本地磁盘的实时迁移，您可以运行：
pvenode migrateall pve2 --vms 100,101,102 --with-local-disks

3.11 证书管理

3.11.1 集群内通信的证书
默认情况下，每个 Proxmox VE 集群都会创建自己的（自签名的）证书颁发机构（CA）并为每个节点生成一个由上述 CA 签名的证书。
这些证书用于与集群的 pveproxy 服务以及使用 SPICE 时的 Shell/Console 功能进行加密通信。
CA 证书和密钥存储在 Proxmox 集群文件系统（pmxcfs）第 6 章中。

3.11.2 API 和 Web GUI 的证书
REST API 和 web GUI 由运行在每个节点上的 pveproxy 服务提供。
您可以为 pveproxy 使用以下选项的证书：
    默认情况下，使用位于 /etc/pve/nodes/NODENAME/pve-ssl.pem 的节点特定证书。此证书由集群 CA 签名，因此浏览器和操作系统不会自动信任。
    使用外部提供的证书（例如，由商业 CA 签名）。
    使用 ACME（Let's Encrypt）获取具有自动更新的可信证书，这也集成在 Proxmox VE API 和 web 接口中。
    对于选项 2 和 3，将使用文件 /etc/pve/local/pveproxy-ssl.pem（以及 /etc/pve/local/pveproxy-，需要没有密码）。
    注意
    请记住，/etc/pve/local 是对 /etc/pve/nodes/NODENAME 的节点特定符号链接。
    证书使用 Proxmox VE 节点管理命令进行管理（请参阅 pvenode(1) 手册页）。
    警告
    请勿替换或手动修改在 /etc/pve/local/pve-ssl.pem 和 /etc/pve/local/pve-ssl.key 中的自动生成的节点证书文件，
    或者在 /etc/pve/pve-root-ca.pem 和 /etc/pve/priv/pve-root-ca.key 中的集群
    CA 文件。

3.11.3 上传自定义证书
如果您已经有一个要用于 Proxmox VE 节点的证书，您可以通过 Web 界面简单地上传该证书。请注意，如果提供了证书密钥文件，它不能受密码保护。

3.11.4 通过 Let's Encrypt（ACME）获得可信证书
Proxmox VE 包括一个自动证书管理环境（ACME）协议的实现，
允许 Proxmox VE 管理员使用像 Let's Encrypt 这样的 ACME 提供商轻松设置现代操作系统和网络浏览器默认接受和信任的 TLS 证书。
目前，实现的两个 ACME 端点是 Let's Encrypt（LE）生产环境和其测试环境。
我们的 ACME 客户端支持使用内置 Web 服务器验证 http-01 挑战和使用支持所有 acme.sh DNS API 端点的 DNS 插件验证 dns-01 挑战。
ACME 账户 您需要为每个集群注册一个与您想要使用的端点的 ACME 账户。用于该帐户的电子邮件地址将用作与 ACME 端点的续订通知或类似通知的联系点。
您可以在 Web 界面 Datacenter -> ACME 或使用 pvenode 命令行工具上注册和停用 ACME 帐户。
pvenode acme account register account-name mail@example.com
提示
由于速率限制，您应该将 LE 测试环境用于实验，或者如果您首次使用 ACME。
ACME 插件
ACME 插件的任务是提供自动验证，证明您和您操作下的 Proxmox VE 集群是域名的真正所有者。这是自动证书管理的基本构建块。
ACME 协议规定了不同类型的挑战，例如 http-01，其中 Web 
服务器提供一个带有特定内容的文件，以证明它控制某个域。有时这是不可能的，无论是因为技术限制还是因为记录的地址无法从公共互联网访问。
在这些情况下可以使用 dns-01 挑战。该挑战通过在域的区域中创建特定的 DNS 记录来实现。Proxmox VE 支持这两种挑战类型，
您可以在 Datacenter -> ACME 下的 Web 界面中配置插件，或使用 pvenode acme plugin add 命令。
ACME 插件配置存储在 /etc/pve/priv/acme/plugins.cfg 中。一个插件可用于集群中的所有节点。
节点域名
每个域名都是特定于节点的。您可以在 Node -> Certificates 下添加新的或管理现有的域名条目，或使用 pvenode config 命令。
配置节点所需的域名后，并确保选择了所需的 ACME 帐户，您可以通过 Web 界面订购新证书。成功后，界面将在 10 秒后重新加载。
续订将自动发生，详见第 3.11.7 节。

3.11.5 ACME HTTP 挑战插件
通过内置的 Web 服务器在端口 80 上生成，始终隐式配置用于验证 http-01 挑战的独立插件。
注意
独立的名称意味着它可以在没有任何第三方服务的情况下提供验证。因此，此插件也适用于集群节点。
使用 Let's Encrypt 的 ACME 进行证书管理需要满足一些先决条件。
• 您必须接受 Let's Encrypt 的 ToS 才能注册帐户。
• 节点的端口 80 需要从互联网上可访问。
• 端口 80 上不能有其他监听器。
• 请求的（子）域名需要解析为节点的公共 IP。

3.11.6 ACME DNS API 挑战插件
在无法或不希望通过 http-01 方法进行外部访问验证的系统上，可以使用 dns-01 验证方法。这种验证方法需要一个允许通过 API 提供 TXT 记录的 DNS 服务器。
配置 ACME DNS API 进行验证
Proxmox VE 重用了为 acme.sh 4 项目开发的 DNS 插件，请参阅其文档以获取有关特定 API 配置的详细信息。
使用 Web 界面（Datacenter -> ACME）配置具有 DNS API 的新插件是最简单的方法。选择 DNS 作为挑战类型。
然后，您可以选择 API 提供商，输入用于通过其 API 访问帐户的凭据数据。
4acme.sh https://github.com/acmesh-official/acme.sh
提示
有关获取提供商 API 凭据的更详细信息，请参阅 acme.sh 如何使用 DNS API wiki。
由于有许多 DNS 提供商和 API 端点，Proxmox VE 会自动为某些提供商生成凭据表单。对于其他提供商，您将看到一个更大的文本区域，只需将所有凭据 KEY=VALUE 对复制到其中。
通过 CNAME 别名进行 DNS 验证
如果您的主要/实际 DNS 不支持通过 API 进行配置，可以使用特殊的别名模式在不同的域/DNS 服务器上处理验证。
手动为 _acme-challenge.domain1.example 设置一个永久 CNAME 记录，指向 _acme-challenge.domain2.example，然后将 Proxmox VE 节点配置文件中的别名属性设置为 
domain2.example，以允许 domain2.example 的 DNS 服务器验证 domain1.example 的所有挑战。
插件组合
如果您的节点通过具有不同要求/ DNS 配置能力的多个域名可访问，则可以组合 http-01 和 dns-01 验证。
还可以通过为每个域指定不同的插件实例来混合来自多个提供商或实例的 DNS API。
提示
如果可能的话，应避免通过多个域名访问相同的服务，因为这会增加复杂性。

3.11.7 ACME 证书的自动更新
如果成功地为节点配置了 ACME 提供的证书（通过 pvenode 或 GUI），则 pve-daily-update.service 将自动更新证书。
目前，如果证书已经过期，或者将在接下来的 30 天内过期，将尝试更新。

3.11.8 使用 pvenode 的 Let's Encrypt 证书示例
root@proxmox:~# pvenode acme account register default mail@example.invalid
Directory endpoints:
0) Let’s Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory)
1) Let’s Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/  -
directory)
2) Custom
Enter selection: 1
Terms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November  -
-15-2017.pdf
Do you agree to the above terms? [y|N]y
...
Task OK
root@proxmox:~# pvenode config set --acme domains=example.invalid
root@proxmox:~# pvenode acme cert order
Loading ACME account details
Placing ACME order
...
Status is ’valid’!
All domains validated!
...
Downloading certificate
Setting pveproxy certificate and key
Restarting pveproxy
Task OK
示例：设置 OVH API 以验证域名
注意
无论使用哪个插件，帐户注册步骤都是相同的，在这里不重复。
注意
根据 OVH API 文档，需要从 OVH 获取 OVH_AK 和 OVH_AS
首先，您需要获取所有信息，以便您和 Proxmox VE 可以访问 API。
root@proxmox:~# cat /path/to/api-token
OVH_AK=XXXXXXXXXXXXXXXX
OVH_AS=YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
root@proxmox:~# source /path/to/api-token
root@proxmox:~# curl -XPOST -H"X-Ovh-Application: $OVH_AK" -H "Content-type  -
: application/json" \
https://eu.api.ovh.com/1.0/auth/credential -d ’{
"accessRules": [
{"method": "GET","path": "/auth/time"},
{"method": "GET","path": "/domain"},
{"method": "GET","path": "/domain/zone/*"},
{"method": "GET","path": "/domain/zone/*/record"},
{"method": "POST","path": "/domain/zone/*/record"},
{"method": "POST","path": "/domain/zone/*/refresh"},
{"method": "PUT","path": "/domain/zone/*/record/"},
{"method": "DELETE","path": "/domain/zone/*/record/*"}
]
}’
{"consumerKey":"ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ","state":"  -
pendingValidation","validationUrl":"https://eu.api.ovh.com/auth/?  -
credentialToken=  -
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"}
(open validation URL and follow instructions to link Application Key with  -
account/Consumer Key)
root@proxmox:~# echo "OVH_CK=ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ" >> /path/to/  -
api-token

现在，您可以设置 ACME 插件：
root@proxmox:~# pvenode acme plugin add dns example_plugin --api ovh --data  -
/path/to/api_token
root@proxmox:~# pvenode acme plugin config example_plugin
&#x250c;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x252c;&#x2500;&#&#x2502; key &#x2502; value &#x2502;
&#x255e;&#x2550;&#x2550;&#x2550;&#x2550;&#x2550;&#x2550;&#x2550;&#x2550;&#x256a;&#x2550;&#&#x2502; api &#x2502; ovh &#x2502;
&#x251c;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x253c;&#x2500;&#&#x2502; data &#x2502; OVH_AK=XXXXXXXXXXXXXXXX &#x2502;
&#x2502; &#x2502; OVH_AS=YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY &#x2502;
&#x2502; &#x2502; OVH_CK=ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ &#x2502;
&#x251c;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x253c;&#x2500;&#&#x2502; digest &#x2502; 
867fcf556363ca1bea866863093fcab83edf47a1 &#x2502;
&#x251c;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x253c;&#x2500;&#&#x2502; plugin &#x2502; example_plugin &#x2502;
&#x251c;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x253c;&#x2500;&#&#x2502; type &#x2502; dns &#x2502;
&#x2514;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2534;&#x2500;&#
最后，您可以配置要获取证书的域名并为其下达证书订单：
root@proxmox:~# pvenode config set -acmedomain0 example.proxmox.com,plugin=  -
example_plugin
root@proxmox:~# pvenode acme cert order
Loading ACME account details
Placing ACME order
Order URL: https://acme-staging-v02.api.letsencrypt.org/acme/order  -
/11111111/22222222
Getting authorization details from ’https://acme-staging-v02.api.  -
letsencrypt.org/acme/authz-v3/33333333’
The validation for example.proxmox.com is pending!
[Wed Apr 22 09:25:30 CEST 2020] Using OVH endpoint: ovh-eu
[Wed Apr 22 09:25:30 CEST 2020] Checking authentication
[Wed Apr 22 09:25:30 CEST 2020] Consumer key is ok.
[Wed Apr 22 09:25:31 CEST 2020] Adding record
[Wed Apr 22 09:25:32 CEST 2020] Added, sleep 10 seconds.
Add TXT record: _acme-challenge.example.proxmox.com
Triggering validation
Sleeping for 5 seconds
Status is ’valid’!
[Wed Apr 22 09:25:48 CEST 2020] Using OVH endpoint: ovh-eu
[Wed Apr 22 09:25:48 CEST 2020] Checking authentication
[Wed Apr 22 09:25:48 CEST 2020] Consumer key is ok.
Remove TXT record: _acme-challenge.example.proxmox.com
All domains validated!
Creating CSR
Checking order status
Order is ready, finalizing order
valid!
Downloading certificate
Setting pveproxy certificate and key
Restarting pveproxy
Task OK
示例：从暂存切换到常规 ACME 目录
更改帐户的 ACME 目录是不受支持的，但由于 Proxmox VE 支持多个帐户，因此您可以使用生产（受信任）的 ACME 目录作为端点创建一个新帐户。
您还可以停用暂存帐户并重新创建。
示例：使用 pvenode 将默认 ACME 帐户从暂存更改为目录
root@proxmox:~# pvenode acme account deactivate default
Renaming account file from ’/etc/pve/priv/acme/default’ to ’/etc/pve/priv/  -
acme/_deactivated_default_4’
Task OK
root@proxmox:~# pvenode acme account register default example@proxmox.com
Directory endpoints:
0) Let’s Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory)
1) Let’s Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/  -
directory)
2) Custom
Enter selection: 0
Terms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November  -
-15-2017.pdf
Do you agree to the above terms? [y|N]y
...
Task OK

3.12 主机引导加载程序
根据安装程序中选择的磁盘设置，Proxmox VE 目前使用两个引导加载程序之一。对于使用 ZFS 作为根文件系统的 EFI 系统，
使用 systemd-boot。所有其他部署使用标准的 grub 引导加载程序（这通常也适用于安装在 Debian 之上的系统）。

3.12.1 安装程序使用的分区方案
Proxmox VE 安装程序在所有选定的安装磁盘上创建 3 个分区。
创建的分区包括：
    1 MB 的 BIOS 引导分区（gdisk 类型 EF02）
    512 MB 的 EFI 系统分区（ESP，gdisk 类型 EF00）
    第三个分区跨越设置的 hdsize 参数或用于所选存储类型的剩余空间
    使用 ZFS 作为根文件系统的系统，通过存储在 512 MB 的 EFI 系统分区上的内核和 initrd 映像进行引导。对于传统的 BIOS 系统，使用 grub，对于 EFI 系统，使用 
    systemd-boot。两者都已安装并配置为指向 ESP。
    在所有使用 grub 5 引导的系统上，BIOS 模式下的 grub（--target i386-pc）都被安装到所有选定磁盘的 BIOS 引导分区。

3.12.2 使用 proxmox-boot-tool 同步 ESP 的内容
proxmox-boot-tool 是一个用于保持 EFI 系统分区配置正确且同步的实用工具。它将某些内核版本复制到所有 ESP，
并配置相应的引导加载程序从 vfat 格式的 ESP 引导。在使用 ZFS 作为根文件系统的上下文中，这意味着您可以在根池上使用所有可选功能，
而不是在 grub 或创建单独的小型引导池6中的 ZFS 实现中也存在的子集。
在具有冗余的设置中，安装程序使用 ESP 为所有磁盘分区。这确保即使第一个引导设备失败或 BIOS 只能从特定磁盘引导时，系统仍能引导。
在常规操作期间，ESP 不会保持挂载。这有助于防止在系统崩溃时 vfat 格式的 ESP 发生文件系统损坏，并消除了在主引导设备失败时手动调整 /etc/fstab 的需要。
proxmox-boot-tool 处理以下任务：
    格式化并设置新分区
    将新的内核映像和 initrd 映像复制并配置到所有列出的 ESP
    在内核升级和其他维护任务上同步配置
    管理同步的内核版本列表
    配置引导加载程序以引导特定内核版本（固定）
您可以通过运行以下命令查看当前配置的 ESP 及其状态：
proxmox-boot-tool status
5这包括所有安装在 ext4 或 xfs 上的根目录以及在非 EFI 系统上安装在 ZFS 上的根目录
6使用 grub 引导根目录上的 ZFS https://github.com/zfsonlinux/zfs/wiki/Debian-Stretch-Root-on-ZFS
设置新分区以用作同步的 ESP
为了格式化并初始化作为同步 ESP 的分区，例如，在替换 rpool 中的失败 vdev 后，或者在转换预先存在同步机制的现有系统时，
可以使用 pve-kernel-helpers 的 proxmox-boot-tool。
警告
format 命令将格式化 <partition>，请确保传入正确的设备/分区！
例如，要将空分区 /dev/sda2 格式化为 ESP，请运行以下命令：
proxmox-boot-tool format /dev/sda2
要设置位于 /dev/sda2 的现有、未挂载的 ESP，以便包含在 Proxmox VE 的内核更新同步机制中，请使用以下命令：
proxmox-boot-tool init /dev/sda2
之后，/etc/kernel/proxmox-boot-uuids 应该包含一个新行，其中包含新添加分区的 UUID。init 命令也会自动触发刷新所有配置的 ESP。
在所有 ESP 上更新配置
要复制和配置所有可引导的内核并保持 /etc/kernel/proxmox-boot-uuids 中列出的所有 ESP 的同步，你只需要运行：
proxmox-boot-tool refresh
（这等价于在 ext4 或 xfs 上的 root 系统上运行 update-grub）。
如果您对内核命令行进行了更改，或者想要同步所有内核和 initrds，这是必要的。
注意
update-initramfs 和 apt（在必要时）会自动触发刷新。
proxmox-boot-tool 考虑的内核版本
默认配置了以下内核版本：
    当前正在运行的内核
    在包更新时新安装的版本
    已安装的两个最新内核
    如果适用，最后第二个内核系列（例如 5.0，5.3）的最新版本
    任何手动选择的内核
手动保持内核可引导
如果您希望将某个内核和 initrd 映像添加到可引导内核列表，请使用 proxmox-boot-tool kernel add。
例如，运行以下命令将 ABI 版本为 5.0.15-1-pve 的内核添加到要保持安装并同步到所有 ESP 的内核列表中：
proxmox-boot-tool kernel add 5.0.15-1-pve
proxmox-boot-tool kernel list 将列出当前选择用于引导的所有内核版本：
proxmox-boot-tool kernel list
手动选择的内核：
5.0.15-1-pve
自动选择的内核：
5.0.12-1-pve
4.15.18-18-pve
运行 proxmox-boot-tool kernel remove 来从手动选择的内核列表中删除一个内核，例如：
proxmox-boot-tool kernel remove 5.0.15-1-pve
注意
在从上述手动内核添加或移除后，需要运行 proxmox-boot-tool refresh 来更新所有 EFI 系统分区（ESP）。

3.12.3 确定使用哪个引导程序
确定使用哪种引导加载程序的最简单和最可靠的方法是观察 Proxmox VE 节点的启动过程。您会看到 grub 的蓝色框或者简单的黑白 systemd-boot。
从正在运行的系统中确定引导加载程序可能不是 100% 准确。最安全的方式是运行以下命令：
efibootmgr -v
如果它返回 EFI 变量不受支持的消息，那么 grub 在 BIOS/Legacy 模式下被使用。
如果输出包含类似于以下的行，那么 grub 在 UEFI 模式下被使用。
Boot0005* proxmox [...] File(\EFI\proxmox\grubx64.efi)
如果输出包含类似于以下的行，那么 systemd-boot 被使用。
Boot0006* Linux Boot Manager [...] File(\EFI\systemd\systemd-bootx64.efi -
)
通过运行：
proxmox-boot-tool status
您可以找出 proxmox-boot-tool 是否被配置，这是判断系统如何启动的一个很好的指标。

3.12.4 Grub
grub 多年来一直是启动 Linux 系统的事实标准，并有很好的文档支持7。
7Grub 手册 https://www.gnu.org/software/grub/manual/grub/grub.html
配置
对 grub 配置的更改是通过默认文件 /etc/default/grub 或者配置片段 /etc/default/grub.d 完成的。更改配置后，运行以下命令以重新生成配置文件：
8
update-grub

3.12.5 Systemd-boot
systemd-boot 是一个轻量级的 EFI 引导加载程序。它直接从安装它的 EFI 服务分区（ESP）读取内核和 initrd 映像。直接从 ESP 
加载内核的主要优点是它不需要重新实现访问存储的驱动程序。在 Proxmox VE 中，proxmox-boot-tool（参见第 3.12.2 节）用于保持 ESPs 上的配置同步。
配置
systemd-boot 通过 EFI 系统分区（ESP）的根目录中的文件 loader/loader.conf 进行配置。详情请参阅 loader.conf(5) 手册页。
每个引导加载程序条目都放在目录 loader/entries/ 的单独文件中。
一个 example entry.conf 如下所示（/ 指的是 ESP 的根目录）：
title Proxmox
version 5.0.15-1-pve
options root=ZFS=rpool/ROOT/pve-1 boot=zfs
linux /EFI/proxmox/5.0.15-1-pve/vmlinuz-5.0.15-1-pve
initrd /EFI/proxmox/5.0.15-1-pve/initrd.img-5.0.15-1-pve

3.12.6 编辑内核命令行
你可以在以下位置修改内核命令行，取决于使用的引导加载程序：
Grub
内核命令行需要放在文件 /etc/default/grub 中的变量 GRUB_CMDLINE_LINUX_DEFAULT 中。运行 update-grub 将其内容附加到 /boot/grub/grub 中的所有 linux 条目。
Systemd-boot
内核命令行需要放在 /etc/kernel/cmdline 中的一行中。要应用更改，请运行 proxmox-boot-tool refresh，该命令将其设置为 loader/entries/proxmox-*.conf 
中所有配置文件的选项行。
8使用 proxmox-boot-tool 的系统将在 update-grub 时调用 proxmox-boot-tool refresh。

3.12.7 覆盖下一次启动的内核版本
要选择当前未默认的内核，你可以：
• 使用启动过程开始时显示的引导加载程序菜单
• 使用 proxmox-boot-tool 将系统固定到一个内核版本，可以是一次性的或永久的（直到解除固定）。
这应该帮助你解决新内核版本与硬件之间的不兼容问题。
注意
这样的固定应尽快移除，以便最新内核的所有当前安全补丁也能应用到系统上。
例如：要永久选择版本 5.15.30-1-pve 作为启动，你需要运行：
proxmox-boot-tool kernel pin 5.15.30-1-pve
提示
固定功能适用于所有 Proxmox VE 系统，不仅仅是那些使用 proxmox-boot-tool 同步 ESP 内容的系统，如果你的系统不使用 proxmox-boot-tool 进行同步，
你也可以跳过最后的 proxmox-boot-tool refresh 调用。
你也可以设置一个内核版本只在下次系统启动时启动。例如，这对于测试更新后的内核是否解决了一个问题非常有用，这个问题最初导致你固定一个版本：
proxmox-boot-tool kernel pin 5.15.30-1-pve --next-boot
要删除任何固定版本配置，请使用 unpin 子命令：
proxmox-boot-tool kernel unpin
虽然 unpin 也有一个 --next-boot 选项，但它是用来清除使用 --next-boot 设置的固定版本。由于在启动时已经自动发生，手动调用它的用处不大。
在设置或清除固定版本后，你还需要通过运行 refresh 子命令来同步 ESP 上的内容和配置。
提示
如果你以交互方式调用工具，将提示你自动执行 proxmox-boot-tool 管理的系统。
proxmox-boot-tool refresh

3.13 内核同页合并 (KSM)
内核同页合并 (Kernel Samepage Merging，KSM) 是 Linux 内核提供的一种可选内存重复数据删除功能，它在 Proxmox VE 中默认启用。KSM 
通过扫描一系列物理内存页的内容进行工作，识别映射到它们的虚拟页面。如果找到相同的页面，相应的虚拟页面会被重新映射，使它们都指向同一物理页面，旧的页面被释放。
虚拟页面被标记为 "写时复制"，这样对它们的任何写入都将写入新的内存区域，保留共享的物理页面不变。

3.13.1 KSM的影响
KSM 可以在虚拟化环境中优化内存使用，因为运行相似操作系统或工作负载的多个虚拟机可能会共享许多常见的内存页面。
然而，尽管 KSM 可以减少内存使用，但它也带来一些安全风险，因为它可能会将虚拟机暴露给旁路攻击。研究表明，通过利用 KSM 
的某些特性，可以通过在同一主机上的第二个虚拟机推断出关于正在运行的虚拟机的信息。
因此，如果你正在使用 Proxmox VE 提供托管服务，你应该考虑禁用 KSM，以便为你的用户提供额外的安全性。此外，你应该检查你国家的法规，因为禁用 KSM 可能是法律要求。

3.13.2 禁用 KSM
要查看 KSM 是否激活，你可以检查以下输出：
systemctl status ksmtuned
如果已启用，可以立即禁用：
systemctl disable --now ksmtuned
最后，要解除所有当前合并的页面，运行：
echo 2 > /sys/kernel/mm/ksm/run













